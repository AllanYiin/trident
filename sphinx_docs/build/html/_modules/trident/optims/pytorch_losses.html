<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>trident.optims.pytorch_losses &#8212; trident 0.7.5 documentation</title>

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/material-icons.css" />
    <link rel="stylesheet" href="../../../_static/notosanscjkjp.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/roboto.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/material-design-lite-1.3.0/material.deep_orange-indigo.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../_static/sphinx_materialdesign_theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <!-- Title -->
        <span class="mdl-layout-title">
            <a class="brand" href="../../../index.html">
                <img class="logo" src="../../../_static/trident_logo.png" alt="trident"/>
            </a>
        </span>
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="../../index.html">Module code</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active">trident.optims.pytorch_losses</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../../../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
            <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          <a  class="mdl-navigation__link" href="../../../index.html">
                  <i class="material-icons navigation-link-icon">home</i>
                  Home
              </a>
          
              <a  class="mdl-navigation__link" href="http://example.com">
                  <i class="material-icons navigation-link-icon">launch</i>
                  ExternalLink
              </a>
          
              <a  class="mdl-navigation__link" href="http://example.com">
                  
                  NoIconLink
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/AllanYiin/trident">
                  <i class="material-icons navigation-link-icon">link</i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../../../index.html">
              <img class="logo" src="../../../_static/trident_logo.png" alt="trident"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
        <!-- Local TOC -->
        <nav class="mdl-navigation"></nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">
<header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../../../index.html">
              <img class="logo" src="../../../_static/trident_logo.png" alt="trident"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
        <!-- Local TOC -->
        <nav class="mdl-navigation"></nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content">
        
  <h1>Source code for trident.optims.pytorch_losses</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>

<span class="kn">import</span> <span class="nn">builtins</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">numbers</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">typing</span> <span class="kn">import</span> <span class="n">Callable</span><span class="p">,</span> <span class="n">Any</span>

<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">from</span> <span class="nn">torch.nn</span> <span class="kn">import</span> <span class="n">init</span>
<span class="kn">from</span> <span class="nn">torch.nn.modules.loss</span> <span class="kn">import</span> <span class="n">_Loss</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>

<span class="kn">from</span> <span class="nn">trident</span> <span class="kn">import</span> <span class="n">context</span>
<span class="kn">from</span> <span class="nn">trident.backend</span> <span class="kn">import</span> <span class="n">dtype</span> <span class="k">as</span> <span class="n">Dtype</span>
<span class="kn">from</span> <span class="nn">trident.backend.common</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">trident.backend.model</span> <span class="kn">import</span> <span class="n">ModelBase</span>
<span class="kn">from</span> <span class="nn">trident.backend.pytorch_backend</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">trident.backend.pytorch_ops</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">trident.backend.tensorspec</span> <span class="kn">import</span> <span class="n">ObjectType</span><span class="p">,</span> <span class="n">object_type_inference</span>
<span class="kn">from</span> <span class="nn">trident.data.dataset</span> <span class="kn">import</span> <span class="o">*</span>
<span class="kn">from</span> <span class="nn">trident.layers.pytorch_activations</span> <span class="kn">import</span> <span class="n">sigmoid</span>
<span class="kn">from</span> <span class="nn">trident.layers.pytorch_layers</span> <span class="kn">import</span> <span class="n">Dense</span>
<span class="kn">from</span> <span class="nn">trident.optims.losses</span> <span class="kn">import</span> <span class="n">Loss</span><span class="p">,</span> <span class="n">_check_logsoftmax_logit</span><span class="p">,</span> <span class="n">_check_logit</span><span class="p">,</span><span class="n">_check_softmax</span>

<span class="n">_device</span> <span class="o">=</span> <span class="n">get_device</span><span class="p">()</span>
<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;_ClassificationLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;MSELoss&#39;</span><span class="p">,</span> <span class="s1">&#39;CrossEntropyLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;NLLLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;BCELoss&#39;</span><span class="p">,</span> <span class="s1">&#39;F1ScoreLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;L1Loss&#39;</span><span class="p">,</span>
           <span class="s1">&#39;SmoothL1Loss&#39;</span><span class="p">,</span> <span class="s1">&#39;L2Loss&#39;</span><span class="p">,</span> <span class="s1">&#39;CosineSimilarityLoss&#39;</span><span class="p">,</span>
           <span class="s1">&#39;ExponentialLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;ItakuraSaitoLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;MS_SSIMLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;DiceLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;ActiveContourLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;WingLoss&#39;</span><span class="p">,</span>
           <span class="s1">&#39;AdaptiveWingLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;KLDivergenceLoss&#39;</span><span class="p">,</span>
           <span class="s1">&#39;IoULoss&#39;</span><span class="p">,</span> <span class="s1">&#39;FocalLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;SoftIoULoss&#39;</span><span class="p">,</span> <span class="s1">&#39;CenterLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;TripletLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;TripletMarginLoss&#39;</span><span class="p">,</span>
           <span class="s1">&#39;LovaszSoftmax&#39;</span><span class="p">,</span> <span class="s1">&#39;PerceptionLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;EdgeLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;TransformInvariantLoss&#39;</span><span class="p">,</span> <span class="s1">&#39;get_loss&#39;</span><span class="p">]</span>

<span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="p">()</span>
<span class="n">_float_dtype</span> <span class="o">=</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">float16</span> <span class="k">if</span> <span class="n">ctx</span><span class="o">.</span><span class="n">amp_available</span> <span class="o">==</span> <span class="kc">True</span> <span class="ow">and</span> <span class="n">ctx</span><span class="o">.</span><span class="n">is_autocast_enabled</span> <span class="o">==</span> <span class="kc">True</span> <span class="ow">and</span> <span class="n">get_device</span><span class="p">()</span> <span class="o">==</span> <span class="s1">&#39;cuda&#39;</span> <span class="k">else</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">float32</span>


<span class="k">def</span> <span class="nf">_calculate_loss_unimplemented</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="k">raise</span> <span class="ne">NotImplementedError</span>



<span class="k">def</span> <span class="nf">_nlp_vercabs_unique_value_process</span><span class="p">(</span><span class="n">uniques</span><span class="p">,</span><span class="n">counts</span><span class="p">):</span>
    <span class="n">uniques_array</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">uniques</span><span class="p">)</span>
    <span class="n">counts_array</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
    <span class="n">counts_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">builtins</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">counts_array</span><span class="p">[</span><span class="n">uniques</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">)],</span> <span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">uniques</span> <span class="k">else</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span>
                             <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">uniques_array</span><span class="o">.</span><span class="n">max</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">uniques_array</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">uniques_array</span><span class="o">.</span><span class="n">max</span><span class="p">())</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span>


    <span class="n">order_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">counts_array</span><span class="p">)</span>
    <span class="n">sorted_ratio</span> <span class="o">=</span> <span class="n">counts_array</span><span class="p">[</span><span class="n">order_index</span><span class="p">]</span> <span class="o">/</span> <span class="n">counts_array</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">soreted_uniques</span> <span class="o">=</span> <span class="n">uniques_array</span><span class="p">[</span><span class="n">order_index</span><span class="p">]</span>
    <span class="n">sorted_ratio_cumsum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">sorted_ratio</span><span class="p">)</span>
    <span class="n">threshold_value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">n</span> <span class="k">for</span>  <span class="n">n</span> <span class="ow">in</span>  <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sorted_ratio_cumsum</span><span class="p">))</span> <span class="k">if</span> <span class="p">(</span><span class="n">sorted_ratio_cumsum</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">&gt;</span><span class="mf">0.5</span> <span class="ow">and</span> <span class="n">sorted_ratio_cumsum</span><span class="p">[</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">&lt;</span><span class="mf">0.5</span><span class="p">)</span> <span class="ow">or</span> <span class="p">(</span><span class="n">sorted_ratio_cumsum</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">&gt;</span><span class="mf">0.995</span> <span class="ow">and</span> <span class="n">sorted_ratio_cumsum</span><span class="p">[</span><span class="n">n</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">&lt;</span><span class="mf">0.995</span><span class="p">)])</span>
    <span class="n">threshold_uniques</span><span class="o">=</span><span class="n">soreted_uniques</span><span class="p">[</span><span class="n">threshold_value</span><span class="p">]</span>
    <span class="n">threshold_counts</span><span class="o">=</span><span class="n">counts_array</span><span class="p">[</span><span class="n">order_index</span><span class="p">][</span><span class="n">threshold_value</span><span class="p">]</span>
    <span class="n">threshold_ratio</span> <span class="o">=</span> <span class="n">sorted_ratio</span><span class="p">[</span><span class="n">threshold_value</span><span class="p">]</span>
    <span class="n">threshold_cumsum</span> <span class="o">=</span> <span class="n">sorted_ratio_cumsum</span><span class="p">[</span><span class="n">threshold_value</span><span class="p">]</span>

    <span class="n">reweights1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">counts_array</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">counts_array</span><span class="p">))</span>
    <span class="n">reweights0</span> <span class="o">=</span> <span class="n">counts_array</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">counts_array</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">counts_array</span><span class="p">))</span>
    <span class="n">reweights</span><span class="o">=</span><span class="n">reweights1</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="n">reweights</span><span class="p">[</span><span class="n">counts_array</span> <span class="o">&gt;=</span><span class="n">threshold_counts</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span> <span class="o">=</span> <span class="n">reweights0</span><span class="p">[</span><span class="n">counts_array</span> <span class="o">&gt;=</span><span class="n">threshold_counts</span><span class="p">[</span><span class="mi">0</span><span class="p">]]</span>
    <span class="n">reweights</span><span class="p">[</span><span class="n">counts_array</span> <span class="o">&lt;=</span> <span class="mi">10</span><span class="p">]</span> <span class="o">=</span><span class="mi">1</span>
    <span class="n">reweights</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">=</span><span class="mf">0.01</span>
    <span class="c1">#çš„</span>

    <span class="k">return</span> <span class="n">reweights</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">),</span> <span class="n">OrderedDict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">uniques_array</span><span class="p">,</span> <span class="n">counts_array</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">_class_unique_value_process</span><span class="p">(</span><span class="n">uniques</span><span class="p">,</span><span class="n">counts</span><span class="p">):</span>
    <span class="n">uniques_array</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">uniques</span><span class="p">)</span>
    <span class="n">counts_array</span> <span class="o">=</span> <span class="n">to_numpy</span><span class="p">(</span><span class="n">counts</span><span class="p">)</span>
    <span class="n">counts_array</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">builtins</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">counts_array</span><span class="p">[</span><span class="n">uniques</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">i</span><span class="p">)],</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">uniques</span> <span class="k">else</span> <span class="mi">1</span>  <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">uniques_array</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
    <span class="n">uniques_array</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">uniques_array</span><span class="o">.</span><span class="n">max</span><span class="p">())</span><span class="o">+</span><span class="mi">1</span><span class="p">)))</span>
    <span class="n">reweights</span><span class="o">=</span><span class="n">counts_array</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">/</span><span class="p">(</span><span class="n">counts_array</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">counts_array</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">reweights</span><span class="p">,</span><span class="n">OrderedDict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">uniques_array</span><span class="p">,</span> <span class="n">counts_array</span><span class="p">))</span>




<span class="k">class</span> <span class="nc">_ClassificationLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate loss for  complex classification task.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): the position where the classes are.</span>
<span class="sd">            sample_weight (Tensor): means the weights of  classes , it shoud be a 1D tensor and length the same as</span>
<span class="sd">            number of classes.</span>
<span class="sd">            from_logits (bool): whether the output tensor is normalized as a probability (total equal to 1)</span>
<span class="sd">            ignore_index (int or list of int):</span>
<span class="sd">            cutoff (None or decimal): the cutoff point of probability for classification, should be None of a number</span>
<span class="sd">            less than 1</span>
<span class="sd">            is_target_onehot (bool): Is the target tensor in onehot format?</span>
<span class="sd">            label_smooth (bool): Should use label smoothing?</span>
<span class="sd">            reduction (string): the method to aggrgate loss. None means no need to aggregate, &#39;mean&#39; means average loss,</span>
<span class="sd">                &#39;sum&#39; means the summation of losses,&#39;batch_mean&#39; means average loss cross the batch axis then</span>
<span class="sd">                summation them.</span>

<span class="sd">        Attributes:</span>
<span class="sd">            need_target_onehot (bool): If True, means the before loss calculation , need to transform target as one-hot format, ex. label-smooth, default is False.</span>
<span class="sd">            is_multiselection (bool): If True, means the classification model is multi-selection, so cannot use  any softmax process, use sigmoid and binary_crosss_entropy</span>
<span class="sd">            insteaded.</span>
<span class="sd">            is_target_onehot (bool):  If True, means we have confirmed (not just declare) the target is transformed as  one-hot format</span>
<span class="sd">            reduction(str): The aggregation function for loss, available options are &#39;sum&#39;, &#39;mean &#39;and &#39;batch_mean&#39;, default is &#39;mean&#39;</span>
<span class="sd">            axis (None or int): The axis we according to for loss calculation. Default is 1.</span>
<span class="sd">            from_logits (bool):If True, means  the sum of all probability will equal 1.</span>
<span class="sd">            is_logsoftmax (bool):If True, means model  use SoftMax as last layer or use any equivalent calculation.</span>
<span class="sd">            sample_weight(1D tensor):The loss weight for all classes.</span>
<span class="sd">            ignore_index(int , list, tuple): The classes we want to ignore in the loss calculation.</span>
<span class="sd">            cutoff(float): Means the decision boundary in this classification model, default=0.5.</span>
<span class="sd">            num_classes(int):number of  all the classes.</span>
<span class="sd">            label_smooth (bool):If True, mean we will apply label-smoothing in loss calculation.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span>
                                                  <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                                                  <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">valid_target_object_type</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_multiselection</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span> <span class="o">=</span> <span class="n">from_logits</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">=</span> <span class="p">[</span><span class="n">ignore_index</span><span class="p">]</span> <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ignore_index</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="k">else</span> <span class="n">ignore_index</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">auto_balance</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="n">ctx</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="n">magenta_color</span><span class="p">(</span><span class="s1">&#39;auto_balance cannot be used with sample_weight together  at the same time&#39;</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="o">=</span> <span class="n">auto_balance</span>

        <span class="k">if</span> <span class="n">cutoff</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="ow">not</span> <span class="mi">0</span> <span class="o">&lt;</span> <span class="n">cutoff</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;cutoff should between 0 and 1&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cutoff</span> <span class="o">=</span> <span class="n">cutoff</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_smooth</span> <span class="o">=</span> <span class="n">label_smooth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="c1"># initilize weight</span>

    <span class="k">def</span> <span class="nf">_calculate_label_statistics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="n">ctx</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">_context</span><span class="p">()</span>
        <span class="n">inferred_target_object_type</span> <span class="o">=</span> <span class="n">object_type_inference</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="p">,</span> <span class="s1">&#39;data_providers&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">data_providers</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
                <span class="n">dp</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">ctx</span><span class="o">.</span><span class="n">_thread_local_info</span><span class="o">.</span><span class="n">data_providers</span><span class="o">.</span><span class="n">values</span><span class="p">())[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
                <span class="n">ds</span> <span class="o">=</span> <span class="kc">None</span>
                <span class="k">if</span> <span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">pass</span>
                <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="n">ZipDataset</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">dp_ds</span> <span class="ow">in</span> <span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">items</span><span class="p">:</span>
                        <span class="k">if</span> <span class="n">dp_ds</span><span class="o">.</span><span class="n">symbol</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">signature</span><span class="o">.</span><span class="n">inputs</span><span class="p">:</span>
                            <span class="n">ds</span> <span class="o">=</span> <span class="n">dp_ds</span>
                        <span class="c1"># maybe duplicate</span>
                        <span class="k">elif</span> <span class="n">dp_ds</span><span class="o">.</span><span class="n">object_type</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">valid_target_object_type</span> <span class="ow">and</span> <span class="n">dp_ds</span><span class="o">.</span><span class="n">object_type</span> <span class="o">==</span> <span class="n">inferred_target_object_type</span><span class="p">:</span>
                            <span class="n">ds</span> <span class="o">=</span> <span class="n">dp_ds</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">ds</span> <span class="o">=</span> <span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span>
                <span class="k">if</span> <span class="n">ds</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">pass</span>
                <span class="k">elif</span> <span class="n">ds</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="s1">&#39;_label_statistics&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="n">ds</span><span class="o">.</span><span class="n">_label_statistics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span><span class="n">TextSequenceDataset</span><span class="p">):</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="p">,</span><span class="n">_</span><span class="o">=</span> <span class="n">_class_unique_value_process</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">_label_statistics</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span> <span class="nb">list</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">_label_statistics</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span>  <span class="n">_nlp_vercabs_unique_value_process</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">_label_statistics</span><span class="o">.</span><span class="n">keys</span><span class="p">()),</span>     <span class="nb">list</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">_label_statistics</span><span class="o">.</span><span class="n">values</span><span class="p">()))</span>

                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">LabelDataset</span><span class="p">)</span> <span class="ow">or</span> <span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">object_type</span> <span class="ow">in</span> <span class="p">[</span>
                        <span class="n">ObjectType</span><span class="o">.</span><span class="n">classification_label</span><span class="p">]:</span>
                        <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Start retrive label class distribution for auto-balance in loss function.&#39;</span><span class="p">)</span>
                        <span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">items</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">items</span><span class="p">)))]),</span>
                            <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                        <span class="n">ctx</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
                        <span class="n">reweights</span><span class="p">,</span><span class="n">label_statistics</span><span class="o">=</span><span class="n">_class_unique_value_process</span><span class="p">(</span><span class="n">unique</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span> <span class="n">counts</span><span class="o">.</span><span class="n">tolist</span><span class="p">())</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="o">=</span> <span class="n">reweights</span>
                        <span class="n">ds</span><span class="o">.</span><span class="n">_label_statistics</span><span class="o">=</span> <span class="n">label_statistics</span>

                        <span class="k">del</span> <span class="n">unique</span>
                        <span class="k">del</span> <span class="n">counts</span>

                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">BboxDataset</span><span class="p">)</span> <span class="ow">or</span> <span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">object_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">ObjectType</span><span class="o">.</span><span class="n">absolute_bbox</span><span class="p">,</span>
                                                                                           <span class="n">ObjectType</span><span class="o">.</span><span class="n">relative_bbox</span><span class="p">]:</span>
                        <span class="n">ctx</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;Start retrive label class distribution for auto-balance in loss function.&#39;</span><span class="p">)</span>
                        <span class="n">unique</span><span class="p">,</span> <span class="n">counts</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span><span class="p">[</span><span class="n">i</span><span class="p">][:,</span> <span class="mi">4</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">items</span><span class="p">)))],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                            <span class="n">dtype</span><span class="o">=</span><span class="n">Dtype</span><span class="o">.</span><span class="n">long</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">),</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
                        <span class="n">ctx</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
                        <span class="n">reweights</span><span class="p">,</span> <span class="n">label_statistics</span> <span class="o">=</span> <span class="n">_class_unique_value_process</span><span class="p">(</span><span class="n">unique</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="o">=</span> <span class="n">reweights</span>
                        <span class="n">ds</span><span class="o">.</span><span class="n">_label_statistics</span> <span class="o">=</span> <span class="n">label_statistics</span>
                        <span class="k">del</span> <span class="n">unique</span>
                        <span class="k">del</span> <span class="n">counts</span>

                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">MaskDataset</span><span class="p">)</span> <span class="ow">or</span> <span class="n">dp</span><span class="o">.</span><span class="n">traindata</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">object_type</span> <span class="ow">in</span> <span class="p">[</span><span class="n">ObjectType</span><span class="o">.</span><span class="n">label_mask</span><span class="p">,</span>
                                                                                           <span class="n">ObjectType</span><span class="o">.</span><span class="n">color_mask</span><span class="p">,</span>
                                                                                           <span class="n">ObjectType</span><span class="o">.</span><span class="n">binary_mask</span><span class="p">]:</span>
                        <span class="n">ctx</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;Start retrive label class distribution for auto-balance in loss function.&#39;</span><span class="p">)</span>
                        <span class="n">sample_base</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">ds</span><span class="p">)))</span>
                        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_base</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">1000</span><span class="p">:</span>
                            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">sample_base</span><span class="p">)</span>
                            <span class="n">sample_base</span><span class="o">=</span><span class="n">sample_base</span><span class="p">[:</span><span class="mi">1000</span><span class="p">]</span>
                        <span class="n">overall_unique</span><span class="o">=</span><span class="n">OrderedDict</span><span class="p">()</span>
                        <span class="n">unique_results</span> <span class="o">=</span><span class="p">[</span><span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">return_counts</span><span class="o">=</span><span class="kc">True</span><span class="p">)))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">sample_base</span><span class="p">)]</span>
                        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="n">unique_results</span><span class="p">:</span>
                            <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">d</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                                <span class="k">if</span> <span class="n">k</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">overall_unique</span><span class="p">:</span>
                                    <span class="n">overall_unique</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">=</span><span class="n">v</span>
                                <span class="k">else</span><span class="p">:</span>
                                    <span class="n">overall_unique</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">+=</span><span class="n">v</span>

                        <span class="n">unique</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">sorted</span><span class="p">(</span><span class="n">overall_unique</span><span class="o">.</span><span class="n">key_list</span><span class="p">))</span>
                        <span class="n">counts</span><span class="o">=</span><span class="p">[</span><span class="n">overall_unique</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">unique</span><span class="p">]</span>
                        <span class="c1"># unique = to_list(to_numpy(unique))</span>
                        <span class="c1"># counts = to_numpy(counts)</span>

                        <span class="n">ctx</span><span class="o">.</span><span class="n">print</span><span class="p">(</span><span class="s1">&#39;&#39;</span><span class="p">)</span>
                        <span class="n">reweights</span><span class="p">,</span> <span class="n">label_statistics</span> <span class="o">=</span> <span class="n">_class_unique_value_process</span><span class="p">(</span><span class="n">unique</span><span class="p">,</span> <span class="n">counts</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="o">=</span> <span class="n">reweights</span>
                        <span class="n">ds</span><span class="o">.</span><span class="n">_label_statistics</span> <span class="o">=</span> <span class="n">label_statistics</span>
                        <span class="k">del</span> <span class="n">unique</span>
                        <span class="k">del</span> <span class="n">counts</span>

                    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">ds</span><span class="p">,</span> <span class="n">TextSequenceDataset</span><span class="p">):</span>
                        <span class="n">keys</span> <span class="o">=</span> <span class="p">[</span><span class="n">ds</span><span class="o">.</span><span class="n">text2index</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="n">ds</span><span class="o">.</span><span class="n">vocabs_frequency</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span> <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">ds</span><span class="o">.</span><span class="n">vocabs</span><span class="p">]</span>
                        <span class="n">reweights</span><span class="p">,</span> <span class="n">label_statistics</span> <span class="o">=</span> <span class="n">_nlp_vercabs_unique_value_process</span><span class="p">(</span><span class="n">keys</span><span class="p">,</span> <span class="n">ds</span><span class="o">.</span><span class="n">vocabs_frequency</span><span class="o">.</span><span class="n">value_list</span><span class="p">)</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="o">=</span> <span class="n">reweights</span>
                        <span class="n">ds</span><span class="o">.</span><span class="n">_label_statistics</span> <span class="o">=</span> <span class="n">label_statistics</span>


    <span class="k">def</span> <span class="nf">flatten_check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="s2">&quot;Check that `out` and `targ` have the same number of elements and flatten them.&quot;</span>
        <span class="n">ndim_output</span> <span class="o">=</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">ndim_target</span> <span class="o">=</span> <span class="n">ndim</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># N,C,H,W =&gt; N,C,H*W</span>
                <span class="k">if</span> <span class="n">ndim_target</span> <span class="o">==</span> <span class="n">ndim_output</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">long</span><span class="p">:</span>
                    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">ndim_target</span> <span class="o">==</span> <span class="n">ndim_output</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">long</span><span class="p">:</span>
                    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
            <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">==</span> <span class="o">-</span><span class="mi">1</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">ndim_target</span> <span class="o">==</span> <span class="n">ndim_output</span> <span class="o">-</span> <span class="mi">1</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">long</span><span class="p">:</span>
                    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">ndim_target</span> <span class="o">==</span> <span class="n">ndim_output</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">long</span><span class="p">:</span>
                    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>
                    <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span>
        <span class="k">elif</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">2</span><span class="p">:</span>

            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span>
        <span class="k">else</span><span class="p">:</span>

            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span>
            <span class="c1"># raise ValueError(&#39;output and target have diffent elements.&#39;)</span>

    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">())</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="p">)</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;weight should be 1-D tensor and length equal to numbers of filters&#39;</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">())</span>
        <span class="c1"># ignore_index</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">)):</span>
                <span class="k">for</span> <span class="n">idx</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">:</span>
                    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">)</span> <span class="ow">and</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="n">idx</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_smooth</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_calculate_label_statistics</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_built</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># print(2, &#39;output&#39;, output.shape, &#39;abnormal:&#39;, any_abnormal_number(output))</span>
        <span class="c1"># check num_clases</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">to_tensor</span><span class="p">(</span><span class="mf">0.0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">reduce_max</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">_check_logit</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">elif</span> <span class="n">_check_logsoftmax_logit</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="c1"># output = clip(output,min=-12 ,max=-1e-7)</span>

            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span> <span class="o">=</span> <span class="kc">False</span>

            <span class="k">if</span> <span class="p">(</span><span class="n">ndim</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">==</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="ow">and</span> <span class="s1">&#39;float&#39;</span> <span class="ow">in</span> <span class="nb">str</span><span class="p">(</span>
                    <span class="n">target</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">):</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">pass</span>
            <span class="c1"># output = clipoutput, min=-12, max=-1e-7)</span>

        <span class="c1"># need target onehot but currently not</span>
        <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">str2dtype</span><span class="p">(</span><span class="s1">&#39;long&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">str2dtype</span><span class="p">(</span><span class="s1">&#39;int64&#39;</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="k">elif</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">str2dtype</span><span class="p">(</span><span class="s1">&#39;long&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="o">&lt;=</span> <span class="mi">1</span> <span class="ow">and</span> <span class="nb">abs</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">1e-4</span><span class="p">):</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">long</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">==</span> <span class="kc">True</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">==</span> <span class="kc">False</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">):</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">make_onehot</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">())</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_smooth</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">target</span> <span class="o">*</span> <span class="n">random_uniform_like</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>



        <span class="c1"># setting cutoff</span>
        <span class="c1"># if self.cutoff is not None:</span>
        <span class="c1">#     mask = (output &gt; self.cutoff).to(output.dtype)</span>
        <span class="c1">#     output = output * mask</span>
        <span class="c1"># print(8, &#39;output&#39;, output.shape, &#39;abnormal:&#39;, any_abnormal_number(output))</span>

        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span>

    <span class="n">calculate_loss</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="n">_calculate_loss_unimplemented</span>

    <span class="c1"># def calculate_loss(self, output, target, **kwargs):</span>
    <span class="c1">#     &quot;&quot;&quot; Calculate the unaggregate loss.</span>
    <span class="c1">#     The loss function calculation logic should define here., please don&#39;t aggregate the loss in this phase.</span>
    <span class="c1">#</span>
    <span class="c1">#     Args:</span>
    <span class="c1">#         output (tf.Tensor):</span>
    <span class="c1">#         target (tf.Tensor):</span>
    <span class="c1">#     &quot;&quot;&quot;</span>
    <span class="c1">#     ##dont do aggregation</span>
    <span class="c1">#     raise NotImplementedError</span>

    <span class="k">def</span> <span class="nf">_handel_abnormal</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">any_abnormal_number</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
            <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
                <span class="s1">&#39;</span><span class="si">{0}</span><span class="s1"> has abnormal value,trident automatically replace these abnormal value to zero.</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">name</span><span class="p">))</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">is_abnormal_number</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">loss</span><span class="p">),</span> <span class="n">loss</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Args:</span>
<span class="sd">                output (tf.Tensor):</span>
<span class="sd">                target (tf.Tensor):</span>

<span class="sd">            Returns:</span>
<span class="sd">                calculated loss</span>

<span class="sd">            &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten_check</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_loss</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">(),</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_ohem</span><span class="p">:</span>
                <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_do_ohem</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">any_abnormal_number</span><span class="p">(</span><span class="n">loss</span><span class="p">):</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;output shape:&#39;</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="s1">&#39;target shape:&#39;</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handel_abnormal</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_reduction</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">loss</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="n">PrintException</span><span class="p">()</span>
            <span class="k">raise</span> <span class="n">e</span>

    <span class="k">def</span> <span class="nf">_do_ohem</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_ohem</span> <span class="ow">and</span> <span class="n">ndim</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">n_min</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ohem_thresh</span><span class="p">)</span>

            <span class="c1"># if loss[self.n_min] &gt; self.ohem_thresh:</span>
            <span class="c1">#     loss = loss[loss &gt; self.ohem_thresh]</span>
            <span class="c1"># else:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">[:</span><span class="n">n_min</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span>


<span class="k">class</span> <span class="nc">_PairwiseLoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate loss for  complex classification task.&quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span>
                 <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): the position where the classes are.</span>
<span class="sd">            sample_weight (Tensor): means the weights of  classes , it shoud be a 1D tensor and length the same as</span>
<span class="sd">            number of classes.</span>
<span class="sd">            from_logits (bool): whether the output tensor is normalized as a probability (total equal to 1)</span>
<span class="sd">            ignore_index (int or list of int):</span>
<span class="sd">            cutoff (None or decimal): the cutoff point of probability for classification, should be None of a number</span>
<span class="sd">            less than 1</span>
<span class="sd">            is_target_onehot (bool): Is the target tensor in onehot format?</span>
<span class="sd">            label_smooth (bool): Should use label smoothing?</span>
<span class="sd">            reduction (string): the method to aggrgate loss. None means no need to aggregate, &#39;mean&#39; means average loss,</span>
<span class="sd">                &#39;sum&#39; means the summation of losses,&#39;batch_mean&#39; means average loss cross the batch axis then</span>
<span class="sd">                summation them.</span>

<span class="sd">        Attributes:</span>
<span class="sd">            need_target_onehot (bool): If True, means the before loss calculation , need to transform target as one-hot format, ex. label-smooth, default is False.</span>
<span class="sd">            is_multiselection (bool): If True, means the classification model is multi-selection, so cannot use  any softmax process, use sigmoid and binary_crosss_entropy</span>
<span class="sd">            insteaded.</span>
<span class="sd">            is_target_onehot (bool):  If True, means we have confirmed (not just declare) the target is transformed as  one-hot format</span>
<span class="sd">            reduction(str): The aggregation function for loss, available options are &#39;sum&#39;, &#39;mean &#39;and &#39;batch_mean&#39;, default is &#39;mean&#39;</span>
<span class="sd">            axis (None or int): The axis we according to for loss calculation. Default is 1.</span>
<span class="sd">            from_logits (bool):If True, means  the sum of all probability will equal 1.</span>
<span class="sd">            is_logsoftmax (bool):If True, means model  use SoftMax as last layer or use any equivalent calculation.</span>
<span class="sd">            sample_weight(1D tensor):The loss weight for all classes.</span>
<span class="sd">            ignore_index(int , list, tuple): The classes we want to ignore in the loss calculation.</span>
<span class="sd">            cutoff(float): Means the decision boundary in this classification model, default=0.5.</span>
<span class="sd">            num_classes(int):number of  all the classes.</span>
<span class="sd">            label_smooth (bool):If True, mean we will apply label-smoothing in loss calculation.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span>
                                            <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span>
                                            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span>


        <span class="c1"># initilize weight</span>

    <span class="k">def</span> <span class="nf">flatten_check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="s2">&quot;Check that `out` and `targ` have the same number of elements and flatten them.&quot;</span>
        <span class="k">if</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">!=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Output and Target should have same shape in pairwise loss. Output:</span><span class="si">{0}</span><span class="s1"> Target: </span><span class="si">{1}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">target</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="n">ndim</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># N,C,H,W =&gt; N,C,H*W</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
                <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span>


    <span class="k">def</span> <span class="nf">preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># check num_clases</span>
        <span class="c1"># output, target = self.flatten_check(output, target)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="p">)</span> <span class="o">!=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((</span><span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">ones_like</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">*</span> <span class="n">target</span>

        <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span>
        <span class="k">elif</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">int64</span> <span class="ow">and</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="n">ndim</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">+</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">num_class</span> <span class="o">=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">make_onehot</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">num_class</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>


        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span>


    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; Calculate the unaggregate loss.</span>
<span class="sd">        The loss function calculation logic should define here., please don&#39;t aggregate the loss in this phase.</span>

<span class="sd">        Args:</span>
<span class="sd">            output (tf.Tensor):</span>
<span class="sd">            target (tf.Tensor):</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">##dont do aggregation</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">            Args:</span>
<span class="sd">                output (tf.Tensor):</span>
<span class="sd">                target (tf.Tensor):</span>

<span class="sd">            Returns:</span>
<span class="sd">                calculated loss</span>

<span class="sd">            &quot;&quot;&quot;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">built</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>

            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">calculate_loss</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">preprocess</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">))</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_ohem</span><span class="p">:</span>
                <span class="n">loss</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">_do_ohem</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_handel_abnormal</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_reduction</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">loss</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="n">PrintException</span><span class="p">()</span>
            <span class="k">raise</span> <span class="n">e</span>

    <span class="k">def</span> <span class="nf">_do_ohem</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">Tensor</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">enable_ohem</span> <span class="ow">and</span> <span class="n">ndim</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">loss</span><span class="o">=</span><span class="n">loss</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">loss</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="n">n_min</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">ohem_thresh</span><span class="p">)</span>

            <span class="c1"># if loss[self.n_min] &gt; self.ohem_thresh:</span>
            <span class="c1">#     loss = loss[loss &gt; self.ohem_thresh]</span>
            <span class="c1"># else:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span><span class="p">[:</span><span class="n">n_min</span><span class="p">]</span>
            <span class="k">return</span> <span class="n">loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span>

<div class="viewcode-block" id="CrossEntropyLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.CrossEntropyLoss">[docs]</a><span class="k">class</span> <span class="nc">CrossEntropyLoss</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.</span>

<span class="sd">    It is useful when training a classification problem with `C` classes.</span>
<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D `Tensor`</span>
<span class="sd">    assigning weight to each of the classes.</span>
<span class="sd">    This is particularly useful when you have an unbalanced training set.</span>

<span class="sd">    The `input` is expected to contain raw, unnormalized scores for each class.</span>

<span class="sd">    `input` has to be a Tensor of size either :math:`(minibatch, C)` or</span>
<span class="sd">    :math:`(minibatch, C, d_1, d_2, ..., d_K)`</span>
<span class="sd">    with :math:`K \geq 1` for the `K`-dimensional case (described later).</span>

<span class="sd">    This criterion expects a class index in the range :math:`[0, C-1]` as the</span>
<span class="sd">    `target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`</span>
<span class="sd">    is specified, this criterion also accepts this class index (this index may not</span>
<span class="sd">    necessarily be in the class range).</span>

<span class="sd">    The loss can be described as:</span>

<span class="sd">    math::</span>
<span class="sd">        \text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)</span>
<span class="sd">                       = -x[class] + \log\left(\sum_j \exp(x[j])\right)</span>

<span class="sd">    or in the case of the :attr:`weight` argument being specified:</span>

<span class="sd">    math::</span>
<span class="sd">        \text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)</span>

<span class="sd">    The losses are averaged across observations for each minibatch.</span>

<span class="sd">    Can also be used for higher dimension inputs, such as 2D images, by providing</span>
<span class="sd">    an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`,</span>
<span class="sd">    where :math:`K` is the number of dimensions, and a target of appropriate shape</span>
<span class="sd">    (see below).</span>


<span class="sd">    Args:</span>
<span class="sd">        axis (int): the position where the classes are.</span>
<span class="sd">        sample_weight (Tensor): means the weights of  classes , it shoud be a 1D tensor and length the same as</span>
<span class="sd">        number of classes.</span>
<span class="sd">        from_logits (bool): whether the output tensor is normalized as a probability (total equal to 1)</span>
<span class="sd">        ignore_index (int or list of int):</span>
<span class="sd">        cutoff (None or decimal): the cutoff point of probability for classification, should be None of a number</span>
<span class="sd">        less than 1ã„¡</span>
<span class="sd">        is_target_onehot (bool): Is the target tensor in onehot format?</span>
<span class="sd">        label_smooth (bool): Should use label smoothing?</span>
<span class="sd">        reduction (string): the method to aggrgate loss. None means no need to aggregate, &#39;mean&#39; means average loss,</span>
<span class="sd">            &#39;sum&#39; means the summation of losses,&#39;batch_mean&#39; means average loss cross the batch axis then</span>
<span class="sd">            summation them.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; output=to_tensor([[0.1, 0.7 , 0.2],[0.3 , 0.6 , 0.1],[0.9 , 0.05 , 0.05],[0.3 , 0.4 , 0.3]]).float()</span>
<span class="sd">    &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">    torch.Size([4, 3])</span>
<span class="sd">    &gt;&gt;&gt; target=to_tensor([1,0,1,2]).long()</span>
<span class="sd">    &gt;&gt;&gt; print(target.shape)</span>
<span class="sd">    torch.Size([4])</span>
<span class="sd">    &gt;&gt;&gt; CrossEntropyLoss(reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(1.1305)</span>
<span class="sd">    &gt;&gt;&gt; CrossEntropyLoss(reduction=&#39;sum&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(4.5221)</span>
<span class="sd">    &gt;&gt;&gt; CrossEntropyLoss(label_smooth=True,reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(1.0786)</span>
<span class="sd">    &gt;&gt;&gt; CrossEntropyLoss(sample_weight=to_tensor([1.0,1.0,0.5]).float(),reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.9889)</span>
<span class="sd">    &gt;&gt;&gt; CrossEntropyLoss(ignore_index=2,reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.8259)</span>




<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;CrossEntropyLoss&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            axis (int): the axis where the class label is.</span>
<span class="sd">            sample_weight ():</span>
<span class="sd">            from_logits ():</span>
<span class="sd">            ignore_index ():</span>
<span class="sd">            cutoff ():</span>
<span class="sd">            label_smooth ():</span>
<span class="sd">            reduction (string):</span>
<span class="sd">            name (stringf):</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                         <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                         <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="CrossEntropyLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.CrossEntropyLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        Examples:</span>
<span class="sd">        &gt;&gt;&gt; output =  to_tensor([[0.3, 0.3, 0.2], [0, 1, 0.5], [1, 1, 0]],dtype=torch.float32)</span>
<span class="sd">        &gt;&gt;&gt; target = to_tensor([[0, 1, 0], [0, 1, 0], [1, 0, 0]],dtype=torch.float32)</span>
<span class="sd">        &gt;&gt;&gt; CrossEntropyLoss(reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">        tensor(0.8695)</span>
<span class="sd">        &gt;&gt;&gt; CrossEntropyLoss(reduction=&#39;mean&#39;)(output,argmax(target,-1)).cpu()</span>
<span class="sd">        tensor(0.8695)</span>
<span class="sd">        &gt;&gt;&gt; CrossEntropyLoss(reduction=&#39;mean&#39;)(log_softmax(output),target).cpu()</span>
<span class="sd">        tensor(0.8695)</span>
<span class="sd">        &gt;&gt;&gt; CrossEntropyLoss(reduction=&#39;mean&#39;)(log_softmax(output),argmax(target,-1)).cpu()</span>
<span class="sd">        tensor(0.8695)</span>
<span class="sd">        &gt;&gt;&gt; CrossEntropyLoss(reduction=&#39;mean&#39;, sample_weight=to_tensor([1,2,3]))(output,target).cpu()</span>
<span class="sd">        tensor(1.4518)</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">sample_weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">long</span><span class="p">:</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>

            <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="p">):</span>
                    <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">)</span>


            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">unbalance_weight</span>

            <span class="n">target_shape</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span>

            <span class="c1"># gather_unbalance_weight=unbalance_weight.gather(1,target.unsqueeze(-1))</span>
            <span class="c1"># gather_unbalance_weight=torch.index_select(sample_weight, -1, target.reshape((-1))).reshape(target_shape).unsqueeze(1).detach()</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="p">:</span>
                <span class="k">if</span> <span class="n">_check_softmax</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
                    <span class="n">output</span><span class="o">=</span><span class="n">log</span><span class="p">(</span><span class="n">output</span><span class="o">+</span><span class="mf">1e-12</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">)</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="p">):</span>
                    <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">)</span>
                    <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">unbalance_weight</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="p">:</span>
                <span class="k">if</span> <span class="n">_check_softmax</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="mf">1e-12</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">output</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>


            <span class="n">new_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="n">new_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span>

            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">new_shape</span><span class="p">)</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
            <span class="k">return</span> <span class="o">-</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">output</span> <span class="o">*</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="NLLLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.NLLLoss">[docs]</a><span class="k">class</span> <span class="nc">NLLLoss</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;The negative log likelihood loss. It is useful to train a classification</span>
<span class="sd">    problem with `C` classes.</span>

<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D Tensor assigning</span>
<span class="sd">    weight to each of the classes. This is particularly useful when you have an</span>
<span class="sd">    unbalanced training set.</span>

<span class="sd">    The `input` given through a forward call is expected to contain</span>
<span class="sd">    log-probabilities of each class. `input` has to be a Tensor of size either</span>
<span class="sd">    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ã„¡., d_K)`</span>
<span class="sd">    with :math:`K \geq 1` for the `K`-dimensional case (described later).</span>

<span class="sd">    Obtaining log-probabilities in a neural network is easily achieved by</span>
<span class="sd">    adding a  `LogSoftmax`  layer in the last layer of your network.</span>
<span class="sd">    You may use `CrossEntropyLoss` instead, if you prefer not to add an extra</span>
<span class="sd">    layer.</span>

<span class="sd">    The `target` that this loss expects should be a class index in the range :math:`[0, C-1]`</span>
<span class="sd">    where `C = number of classes`; if `ignore_index` is specified, this loss also accepts</span>
<span class="sd">    this class index (this index may not necessarily be in the class range).</span>

<span class="sd">    The unreduced (i.e. with :attr:`reduction` set to ``&#39;none&#39;``) loss can be described as:</span>

<span class="sd">    math::</span>
<span class="sd">        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad</span>
<span class="sd">        l_n = - w_{y_n} x_{n,y_n}, \quad</span>
<span class="sd">        w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},</span>

<span class="sd">    where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight, and</span>
<span class="sd">    :math:`N` is the batch size. If :attr:`reduction` is not ``&#39;none&#39;``</span>
<span class="sd">    (default ``&#39;mean&#39;``), then</span>

<span class="sd">    ã„¡ math::</span>
<span class="sd">        \ell(x, y) = \begin{cases}</span>
<span class="sd">            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &amp;</span>
<span class="sd">            \text{if reduction} = \text{&#39;mean&#39;;}\\</span>
<span class="sd">            \sum_{n=1}^N l_n,  &amp;</span>
<span class="sd">            \text{if reduction} = \text{&#39;sum&#39;.}</span>
<span class="sd">        \end{cases}</span>

<span class="sd">    Can also be used for higher dimension inputs, such as 2D images, by providing</span>
<span class="sd">    an input of size :math:`(minibatch, C, d_1, d_2, ã„¡., d_K)` with :math:`K \geq 1`,</span>
<span class="sd">    where :math:`K` is the number of dimensions, and a target of appropriate shape</span>
<span class="sd">    (see below). In the case of images, it computes NLL loss per-pixel.</span>

<span class="sd">    Args:</span>
<span class="sd">        weight (Tensor, optional): a manual rescaling weight given to each</span>
<span class="sd">            class. If given, it has to be a Tensor of size `C`. Otherwise, it is</span>
<span class="sd">            treated as if having all ones.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        ignore_index (int, optional): Specifies a target value that is ignored</span>
<span class="sd">            and does not contribute to the input gradient. When</span>
<span class="sd">            :attr:`size_average` is ``True``, the loss is averaged over</span>
<span class="sd">            non-ignored targets.</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, C)` where `C = number of classes`, or</span>
<span class="sd">          :math:`(N, C, d_1, d_2, ã„¡., d_K)` with :math:`K \geq 1`</span>
<span class="sd">          in the case of `K`-dimensional loss.</span>
<span class="sd">        - Target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`, or</span>
<span class="sd">          :math:`(N, d_1, d_2, ã„¡., d_K)` with :math:`K \geq 1` in the case of</span>
<span class="sd">          K-dimensional loss.</span>
<span class="sd">        - Output: scalar.</span>
<span class="sd">          If :attr:`reduction` is ``&#39;none&#39;``, then the same size as the target: :math:`(N)`, or</span>
<span class="sd">          :math:`(N, d_1, d_2, ã„¡., d_K)` with :math:`K \geq 1` in the case</span>
<span class="sd">          of K-dimensional loss.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; output=to_tensor([[0.1, 0.7 , 0.2],[0.3 , 0.6 , 0.1],[0.9 , 0.05 , 0.05],[0.3 , 0.4 , 0.3]]).float()</span>
<span class="sd">    &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">    torch.Size([4, 3])</span>
<span class="sd">    &gt;&gt;&gt; target=to_tensor([1,0,1,2]).long()</span>
<span class="sd">    &gt;&gt;&gt; print(target.shape)</span>
<span class="sd">    torch.Size([4])</span>
<span class="sd">    &gt;&gt;&gt; NLLLoss(reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(-0.3375)</span>
<span class="sd">    &gt;&gt;&gt; NLLLoss(reduction=&#39;sum&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(-1.3500)</span>
<span class="sd">    &gt;&gt;&gt; NLLLoss(label_smooth=True,reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(1.1034)</span>
<span class="sd">    &gt;&gt;&gt; NLLLoss(sample_weight=to_tensor([1.0,1.0,0.5]).float(),reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(-0.2625)</span>
<span class="sd">    &gt;&gt;&gt; NLLLoss(ignore_index=2,reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(-0.2625)</span>
<span class="sd">    &gt;&gt;&gt; output2 = torch.tensor([[-0.1, 0.2, -0.3, 0.4],[0.5, -0.6, 0.7, -0.8],[-0.9, 0.1, -0.11, 0.12]])</span>
<span class="sd">    &gt;&gt;&gt; target2= torch.tensor([1,2,3]).long()</span>
<span class="sd">    &gt;&gt;&gt; NLLLoss(reduction=&#39;mean&#39;)(output2,target2).cpu()</span>
<span class="sd">    tensor(1.0847)</span>
<span class="sd">    &gt;&gt;&gt; NLLLoss(reduction=&#39;mean&#39;)(log_softmax(output2),target2).cpu()</span>
<span class="sd">    tensor(1.0847)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;NllLoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                         <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                         <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="NLLLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.NLLLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">reshape_shape</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">reshape_shape</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">(</span><span class="n">reshape_shape</span><span class="p">))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="ow">and</span> <span class="n">ndim</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">==</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">reduce_sum</span><span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">output</span> <span class="o">*</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="F1ScoreLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.F1ScoreLoss">[docs]</a><span class="k">class</span> <span class="nc">F1ScoreLoss</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    This operation computes the f-measure between the output and target. If beta is set as one,</span>
<span class="sd">    it&#39;s called the f1-scorce or dice similarity coefficient. f1-scorce is monotonic in jaccard distance.</span>

<span class="sd">    f-measure = (1 + beta ** 2) * precision * recall / (beta ** 2 * precision + recall)</span>

<span class="sd">    This loss function is frequently used in semantic segmentation of images. Works with imbalanced classes, for</span>
<span class="sd">    balanced classes you should prefer cross_entropy instead.</span>
<span class="sd">    This operation works with both binary and multiclass classification.</span>

<span class="sd">    Args:</span>
<span class="sd">        beta: greater than one weights recall higher than precision, less than one for the opposite.</span>
<span class="sd">        Commonly chosen values are 0.5, 1 or 2.</span>
<span class="sd">        axis (int): the position where the classes are.</span>
<span class="sd">        sample_weight (Tensor): means the weights of  classes , it shoud be a 1D tensor and length the same as</span>
<span class="sd">        number of classes.</span>
<span class="sd">        from_logits (bool): whether the output tensor is normalized as a probability (total equal to 1)</span>
<span class="sd">        ignore_index (int or list of int):</span>
<span class="sd">        cutoff (None or decimal): the cutoff point of probability for classification, should be None of a number</span>
<span class="sd">        less than 1</span>
<span class="sd">        label_smooth (bool): Should use label smoothing?</span>
<span class="sd">        reduction (string): the method to aggrgate loss. None means no need to aggregate, &#39;mean&#39; means average loss,</span>
<span class="sd">            &#39;sum&#39; means the summation of losses,&#39;batch_mean&#39; means average loss cross the batch axis then</span>
<span class="sd">            summation them.</span>

<span class="sd">    Returns:</span>
<span class="sd">        :class:`~cntk.ops.functions.Function`</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; output=to_tensor([[0.1, 0.7 , 0.2],[0.3 , 0.6 , 0.1],[0.9 , 0.05 , 0.05],[0.3 , 0.4 , 0.3]]).float()</span>
<span class="sd">    &gt;&gt;&gt; print(output.shape)</span>
<span class="sd">    torch.Size([4, 3])</span>
<span class="sd">    &gt;&gt;&gt; target=to_tensor([1,0,1,2]).long()</span>
<span class="sd">    &gt;&gt;&gt; print(target.shape)</span>
<span class="sd">    torch.Size([4])</span>
<span class="sd">    &gt;&gt;&gt; F1ScoreLoss(reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.6670)</span>
<span class="sd">    &gt;&gt;&gt; F1ScoreLoss(reduction=&#39;sum&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(2.6680)</span>
<span class="sd">    &gt;&gt;&gt; F1ScoreLoss(label_smooth=True,reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.6740)</span>
<span class="sd">    &gt;&gt;&gt; F1ScoreLoss(loss_weights=to_tensor([1.0,1.0,0]).float(),reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.6670)</span>
<span class="sd">    &gt;&gt;&gt; F1ScoreLoss(ignore_index=2,reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.6670)</span>



<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                 <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;F1ScoreLoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                         <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                         <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">=</span> <span class="n">beta</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>

<div class="viewcode-block" id="F1ScoreLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.F1ScoreLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">sample_weight_shape</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">n</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_weight_shape</span><span class="p">)):</span>
            <span class="k">if</span> <span class="n">n</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="ow">and</span> <span class="n">sample_weight_shape</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">):</span>
                <span class="k">pass</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">sample_weight_shape</span><span class="p">[</span><span class="n">n</span><span class="p">]</span><span class="o">=</span><span class="mi">1</span>
        <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">sample_weight_shape</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">exp</span><span class="p">(</span><span class="n">output</span><span class="p">),</span> <span class="mf">1e-7</span><span class="p">,</span> <span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-7</span><span class="p">)</span>

        <span class="k">elif</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">int64</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">make_onehot</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="n">tp</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">output</span> <span class="o">*</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="c1"># tn = ((1 - target) * (1 - output))</span>
        <span class="c1"># fp = ((1 - target) * output)</span>
        <span class="c1"># fn = (target * (1 - output))</span>
        <span class="n">precision</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">/</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="n">recall</span> <span class="o">=</span> <span class="n">tp</span> <span class="o">/</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">precision</span> <span class="o">*</span> <span class="n">recall</span> <span class="o">/</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">beta</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">precision</span> <span class="o">+</span> <span class="n">recall</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="FocalLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.FocalLoss">[docs]</a><span class="k">class</span> <span class="nc">FocalLoss</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Compute binary focal loss between target and output logits.</span>
<span class="sd">    See :class:`~pytorch_toolbelt.losses.FocalLoss` for details.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): the position where the classes are.</span>
<span class="sd">        sample_weight (Tensor): means the weights of  classes , it shoud be a 1D tensor and length the same as</span>
<span class="sd">        number of classes.</span>
<span class="sd">        from_logits (bool): whether the output tensor is normalized as a probability (total equal to 1)</span>
<span class="sd">        ignore_index (int or list of int):</span>
<span class="sd">        cutoff (None or decimal): the cutoff point of probability for classification, should be None of a number</span>
<span class="sd">        less than 1</span>
<span class="sd">        is_target_onehot (bool): Is the target tensor in onehot format?</span>
<span class="sd">        label_smooth (bool): Should use label smoothing?</span>
<span class="sd">        reduction (string): the method to aggrgate loss. None means no need to aggregate, &#39;mean&#39; means average loss,</span>
<span class="sd">            &#39;sum&#39; means the summation of losses,&#39;batch_mean&#39; means average loss cross the batch axis then</span>
<span class="sd">            summation them.</span>


<span class="sd">    References::</span>

<span class="sd">        https://github.com/open-mmlab/mmdetection/blob/master/mmdet/core/loss/losses.py</span>

<span class="sd">        normalized (bool): Compute normalized focal loss (https://arxiv.org/pdf/1909.07829.pdf).</span>
<span class="sd">        threshold (float, optional): Compute reduced focal loss (https://arxiv.org/abs/1903.01347).</span>

<span class="sd">     Examples:</span>

<span class="sd">        &gt;&gt;&gt; FocalLoss(reduction=&#39;mean&#39;,axis=-1)(to_tensor([[0.1, 0.7 , 0.2],[0.3 , 0.6 , 0.1],[0.9 , 0.05 , 0.05],[0.3 , 0.4 , 0.3]]).float(),to_tensor([1,0,1,2]).long()).cpu()</span>
<span class="sd">        tensor(1.1305)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span> <span class="n">gamma</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">normalized</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;FocalLoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                         <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                         <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span> <span class="o">=</span> <span class="n">gamma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">threshold</span> <span class="o">=</span> <span class="n">threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normalized</span> <span class="o">=</span> <span class="n">normalized</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="FocalLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.FocalLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>


<span class="sd">        Args:</span>
<span class="sd">            output: Tensor of arbitrary shape</span>
<span class="sd">            target: Tensor of the same shape as input</span>



<span class="sd">        Returns:</span>

<span class="sd">            &quot;&quot;&quot;</span>
        <span class="n">num_classes</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((</span><span class="n">num_classes</span><span class="p">))</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
        <span class="n">alpha</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="ow">and</span> <span class="n">target</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">Dtype</span><span class="o">.</span><span class="n">long</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">argmax</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">_check_softmax</span><span class="p">(</span><span class="n">output</span><span class="p">):</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">log</span><span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="mf">1e-12</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">log_softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">ce_loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">nll_loss</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>
        <span class="n">pt</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">ce_loss</span><span class="p">)</span>

        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">.</span><span class="n">gather</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">focal_loss</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">pt</span><span class="p">)</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">gamma</span><span class="p">)</span> <span class="o">*</span> <span class="n">ce_loss</span>
        <span class="k">return</span> <span class="n">focal_loss</span></div></div>


<div class="viewcode-block" id="BCELoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.BCELoss">[docs]</a><span class="k">class</span> <span class="nc">BCELoss</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;BCELoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                         <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                         <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="BCELoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.BCELoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">]))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="p">):</span>
                <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span>
                    <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">]))),</span>
                                        <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="ow">and</span> <span class="n">reduce_max</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">_check_logit</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span> <span class="ow">or</span> <span class="n">_check_softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">):</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">binary_cross_entropy</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expand_as</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">expand_as</span><span class="p">(</span><span class="n">unbalance_weight</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">unbalance_weight</span> <span class="o">*</span> <span class="n">sample_weight</span>
        <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="DiceLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.DiceLoss">[docs]</a><span class="k">class</span> <span class="nc">DiceLoss</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.</span>

<span class="sd">    It is useful when training a classification problem with `C` classes.</span>
<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D `Tensor`</span>
<span class="sd">    assigning weight to each of the classes.</span>
<span class="sd">    This is particularly useful when you have an unbalanced training set.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): the position where the classes are.</span>
<span class="sd">        sample_weight (Tensor): means the weights of  classes , it shoud be a 1D tensor and length the same as</span>
<span class="sd">        number of classes.</span>
<span class="sd">        from_logits (bool): whether the output tensor is normalized as a probability (total equal to 1)</span>
<span class="sd">        ignore_index (int or list of int):</span>
<span class="sd">        cutoff (None or decimal): the cutoff point of probability for classification, should be None of a number</span>
<span class="sd">        less than 1</span>
<span class="sd">        label_smooth (bool): Should use label smoothing?</span>
<span class="sd">        reduction (string): the method to aggrgate loss. None means no need to aggregate, &#39;mean&#39; means average loss,</span>
<span class="sd">            &#39;sum&#39; means the summation of losses,&#39;batch_mean&#39; means average loss cross the batch axis then</span>
<span class="sd">            summation them.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; output=zeros((1,3,128,128))</span>
<span class="sd">    &gt;&gt;&gt; output[0,1,32:64,48:92]=1</span>
<span class="sd">    &gt;&gt;&gt; output[0,2,12:32,56:64]=1</span>
<span class="sd">    &gt;&gt;&gt; target=zeros((1,128,128)).long()</span>
<span class="sd">    &gt;&gt;&gt; target[0,33:63,50:9]=1</span>
<span class="sd">    &gt;&gt;&gt; target[0,13:35,52:65]=2</span>
<span class="sd">    &gt;&gt;&gt; DiceLoss(reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.8271)</span>
<span class="sd">    &gt;&gt;&gt; DiceLoss(ignore_index=0,reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.9829)</span>



<span class="sd">    Reference:</span>
<span class="sd">        https://arxiv.org/abs/1707.03237</span>


<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                 <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;DiceLoss&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            axis (int): the axis where the class label is.</span>
<span class="sd">            sample_weight ():</span>
<span class="sd">            from_logits ():</span>
<span class="sd">            ignore_index ():</span>
<span class="sd">            cutoff ():</span>
<span class="sd">            label_smooth ():</span>
<span class="sd">            reduction (string):</span>
<span class="sd">            name (stringf):</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                         <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                         <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span> <span class="o">=</span> <span class="n">smooth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_multiselection</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="DiceLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.DiceLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if any_abnormal_number(output):</span>
        <span class="c1">#     print(&#39;{0} calculate starting&#39;.format(self.name))</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="ow">or</span> <span class="n">reduce_max</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mf">1e-7</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">sample_weight</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">())</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">()))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">!=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">make_onehot</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="p">):</span>
                <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">unbalance_weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="c1"># reduce the batch axis and all spatial axes</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">numel</span><span class="p">(</span><span class="n">output</span><span class="p">)</span><span class="o">//</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">intersection</span> <span class="o">=</span> <span class="n">reduce_mean</span><span class="p">((</span><span class="n">target</span> <span class="o">*</span> <span class="n">output</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">den1</span> <span class="o">=</span> <span class="n">reduce_mean</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">den2</span> <span class="o">=</span> <span class="n">reduce_mean</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>


        <span class="n">dice</span> <span class="o">=</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">intersection</span> <span class="o">*</span> <span class="n">sample_weight</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="o">/</span><span class="n">scale</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span>
                    <span class="n">den1</span> <span class="o">*</span> <span class="n">sample_weight</span> <span class="o">+</span> <span class="n">den2</span> <span class="o">*</span> <span class="n">sample_weight</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="o">/</span><span class="n">scale</span><span class="p">)</span>
        <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">dice</span></div></div>

        <span class="c1"># dice = (2.0 * intersection.sum(1) + self.smooth)/(den1.sum(1) + den2.sum(1) + self.smooth)</span>

        <span class="c1"># if output.shape[1]==2:</span>
        <span class="c1">#     dice =  (2.0 * (intersection * sample_weight[:,1:].sum(1) )+ self.smooth) / clip(den1[:,1:].sum(1) + den2[:,1:].sum(1) + self.smooth,min=1e-7)</span>
        <span class="c1"># else:</span>
        <span class="c1">#</span>
        <span class="c1">#     dice =  torch.div((2.0 * (intersection * sample_weight).sum(1) + self.smooth) , clip(den1.sum(1) + den2.sum(1) + self.smooth,min=1e-7))</span>

        <span class="c1"># if any_abnormal_number(den2):</span>
        <span class="c1">#     print(&#39;{0} after dice {1}&#39;.format(self.name,dice))</span>

        <span class="c1"># return 1.0 -dice</span>


<div class="viewcode-block" id="ActiveContourLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.ActiveContourLoss">[docs]</a><span class="k">class</span> <span class="nc">ActiveContourLoss</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.</span>

<span class="sd">    It is useful when training a classification problem with `C` classes.</span>
<span class="sd">    If provided, the optional argument :attr:`weight` should be a 1D `Tensor`</span>
<span class="sd">    assigning weight to each of the classes.</span>
<span class="sd">    This is particularly useful when you have an unbalanced training set.</span>

<span class="sd">    Args:</span>
<span class="sd">        axis (int): the position where the classes are.</span>
<span class="sd">        sample_weight (Tensor): means the weights of  classes , it shoud be a 1D tensor and length the same as</span>
<span class="sd">        number of classes.</span>
<span class="sd">        from_logits (bool): whether the output tensor is normalized as a probability (total equal to 1)</span>
<span class="sd">        ignore_index (int or list of int):</span>
<span class="sd">        cutoff (None or decimal): the cutoff point of probability for classification, should be None of a number</span>
<span class="sd">        less than 1.</span>
<span class="sd">        label_smooth (bool): Should use label smoothing?</span>
<span class="sd">        reduction (string): the method to aggrgate loss. None means no need to aggregate, &#39;mean&#39; means average loss,</span>
<span class="sd">            &#39;sum&#39; means the summation of losses,&#39;batch_mean&#39; means average loss cross the batch axis then</span>
<span class="sd">            summation them.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; output=zeros((1,3,128,128))</span>
<span class="sd">    &gt;&gt;&gt; output[0,1,32:64,48:92]=1</span>
<span class="sd">    &gt;&gt;&gt; output[0,2,12:32,56:64]=1</span>
<span class="sd">    &gt;&gt;&gt; target=zeros((1,128,128)).long()</span>
<span class="sd">    &gt;&gt;&gt; target[0,33:63,50:9]=1</span>
<span class="sd">    &gt;&gt;&gt; target[0,13:35,52:65]=2</span>
<span class="sd">    &gt;&gt;&gt; DiceLoss(reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.8271)</span>
<span class="sd">    &gt;&gt;&gt; DiceLoss(ignore_index=0,reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.9829)</span>



<span class="sd">    Reference:</span>
<span class="sd">        cvpr2019 paper &quot;Learning Active Contour Models for Medical Image Segmentation&quot; by Chen, Xu, et al.</span>
<span class="sd">        https://github.com/xuuuuuuchen/Active-Contour-Loss</span>


<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lambdaP</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">mu</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                 <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ActiveContourLoss&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            axis (int): the axis where the class label is.</span>
<span class="sd">            sample_weight ():</span>
<span class="sd">            from_logits ():</span>
<span class="sd">            ignore_index ():</span>
<span class="sd">            cutoff ():</span>
<span class="sd">            label_smooth ():</span>
<span class="sd">            reduction (string):</span>
<span class="sd">            name (stringf):</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                         <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                         <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lambdaP</span> <span class="o">=</span> <span class="n">lambdaP</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">=</span> <span class="n">mu</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_multiselection</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="ActiveContourLoss.flatten_check"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.ActiveContourLoss.flatten_check">[docs]</a>    <span class="k">def</span> <span class="nf">flatten_check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span></div>

<div class="viewcode-block" id="ActiveContourLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.ActiveContourLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">H</span><span class="p">,</span> <span class="n">W</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="n">cast</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span><span class="p">,</span> <span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="p">):</span>
                <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">)</span>
                <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">unbalance_weight</span> <span class="o">&lt;</span> <span class="mf">1e-7</span><span class="p">,</span> <span class="n">ones</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">),</span>
                                         <span class="nb">pow</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">clip</span><span class="p">(</span><span class="n">unbalance_weight</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">))</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">unbalance_weight</span>

        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expand_dims</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">make_onehot</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>


        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        length term</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">output</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">:,:]</span> <span class="o">-</span> <span class="n">output</span><span class="p">[:,:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">,:]</span>    <span class="c1"># horizontal gradient (B, C, H-1, W)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">output</span><span class="p">[:,:,:,</span><span class="mi">1</span><span class="p">:]</span> <span class="o">-</span> <span class="n">output</span><span class="p">[:,:,:,:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>    <span class="c1"># vertical gradient   (B, C, H,   W-1)</span>

        <span class="n">delta_x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[:,:,</span><span class="mi">1</span><span class="p">:,:</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># (B, C, H-2, W-2)</span>
        <span class="n">delta_y</span> <span class="o">=</span> <span class="n">y</span><span class="p">[:,:,:</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">:]</span><span class="o">**</span><span class="mi">2</span>  <span class="c1"># (B, C, H-2, W-2)</span>
        <span class="n">delta_u</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">delta_x</span> <span class="o">+</span> <span class="n">delta_y</span><span class="p">)</span>

        <span class="n">epsilon</span> <span class="o">=</span> <span class="mf">1e-8</span> <span class="c1"># where is a parameter to avoid square root is zero in practice.</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">delta_u</span> <span class="o">+</span> <span class="n">epsilon</span><span class="p">))</span>   <span class="c1"># eq.(11) in the paper, mean is used instead of sum.</span>


        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        region term</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">C_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">C_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
        <span class="c1"># print(&#39;C_in&#39;,C_in.shape)</span>
        <span class="c1"># print(&#39;C_out&#39;, C_out.shape)</span>
        <span class="c1"># print(&#39;output[:,0,:,:]&#39;, output[:,0,:,:].shape)</span>
        <span class="c1"># print(&#39;target[:, 0, :, :] &#39;, target[:, 0, :, :] .shape)</span>

        <span class="n">region_in</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,:,:]</span> <span class="o">*</span> <span class="p">((</span><span class="n">target</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">C_in</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)))</span>         <span class="c1"># equ.(12) in the paper, mean is used instead of sum.</span>
        <span class="n">region_out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="mi">1</span><span class="o">-</span><span class="n">output</span><span class="p">[:,</span><span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,:,:])</span> <span class="o">*</span> <span class="p">((</span><span class="n">target</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">:</span><span class="mi">1</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span> <span class="o">-</span> <span class="n">C_out</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)))</span>   <span class="c1"># equ.(12) in the paper</span>

        <span class="k">return</span> <span class="n">length</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">lambdaP</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mu</span> <span class="o">*</span> <span class="n">region_in</span> <span class="o">+</span> <span class="n">region_out</span><span class="p">)</span></div></div>


<span class="k">class</span> <span class="nc">GeneralizedDiceLoss</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.</span>

<span class="sd">    Compute the generalised Dice loss defined in:</span>

<span class="sd">        Sudre, C. et al. (2017) Generalised Dice overlaps as a deep learning</span>
<span class="sd">        loss function for highly unbalanced segmentations. DLMIA 2017.</span>

<span class="sd">    Reference:</span>
<span class="sd">        https://github.com/NifTK/NiftyNet/blob/v0.6.0/niftynet/layer/loss_segmentation.py#L279</span>


<span class="sd">    Args:</span>
<span class="sd">        axis (int): the position where the classes are.</span>
<span class="sd">        sample_weight (Tensor): means the weights of  classes , it shoud be a 1D tensor and length the same as</span>
<span class="sd">        number of classes.</span>
<span class="sd">        from_logits (bool): whether the output tensor is normalized as a probability (total equal to 1)</span>
<span class="sd">        ignore_index (int or list of int):</span>
<span class="sd">        cutoff (None or decimal): the cutoff point of probability for classification, should be None of a number</span>
<span class="sd">        less than 1</span>
<span class="sd">        label_smooth (bool): Should use label smoothing?</span>
<span class="sd">        reduction (string): the method to aggrgate loss. None means no need to aggregate, &#39;mean&#39; means average loss,</span>
<span class="sd">            &#39;sum&#39; means the summation of losses,&#39;batch_mean&#39; means average loss cross the batch axis then</span>
<span class="sd">            summation them.</span>

<span class="sd">    Examples:</span>
<span class="sd">    &gt;&gt;&gt; output=zeros((1,3,128,128))</span>
<span class="sd">    &gt;&gt;&gt; output[0,1,32:64,48:92]=1</span>
<span class="sd">    &gt;&gt;&gt; output[0,2,12:32,56:64]=1</span>
<span class="sd">    &gt;&gt;&gt; target=zeros((1,128,128)).long()</span>
<span class="sd">    &gt;&gt;&gt; target[0,33:63,50:9]=1</span>
<span class="sd">    &gt;&gt;&gt; target[0,13:35,52:65]=2</span>
<span class="sd">    &gt;&gt;&gt; DiceLoss(reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.8271)</span>
<span class="sd">    &gt;&gt;&gt; DiceLoss(ignore_index=0,reduction=&#39;mean&#39;)(output,target).cpu()</span>
<span class="sd">    tensor(0.9829)</span>





<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">smooth</span><span class="o">=</span><span class="mf">1.</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span>
                 <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;GeneralizedDiceLoss&#39;</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            axis (int): the axis where the class label is.</span>
<span class="sd">            sample_weight ():</span>
<span class="sd">            from_logits ():</span>
<span class="sd">            ignore_index ():</span>
<span class="sd">            cutoff ():</span>
<span class="sd">            label_smooth ():</span>
<span class="sd">            reduction (string):</span>
<span class="sd">            name (stringf):</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                         <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                         <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span> <span class="o">=</span> <span class="n">smooth</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_multiselection</span> <span class="o">=</span> <span class="kc">False</span>

    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1"># if any_abnormal_number(output):</span>
        <span class="c1">#     print(&#39;{0} calculate starting&#39;.format(self.name))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="ow">and</span> <span class="n">reduce_max</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span> <span class="o">=</span> <span class="kc">True</span>

        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">())</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">()))</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="k">if</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">!=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">target</span><span class="p">):</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">make_onehot</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">from_logits</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">sigmoid</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">reduce_axes</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">ndim</span><span class="p">(</span><span class="n">target</span><span class="p">)))</span>
        <span class="n">axis</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span> <span class="o">&gt;=</span> <span class="mi">0</span> <span class="k">else</span> <span class="n">ndim</span><span class="p">(</span><span class="n">target</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">axis</span>
        <span class="n">reduce_axes</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">reduce_axes</span><span class="o">.</span><span class="n">remove</span><span class="p">(</span><span class="n">axis</span><span class="p">)</span>

        <span class="n">intersection</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">target</span> <span class="o">*</span> <span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">)</span>
        <span class="n">den1</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">)</span>
        <span class="n">den2</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="n">reduce_axes</span><span class="p">)</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expand_as</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">intersection</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">(</span><span class="nb">tuple</span><span class="p">([</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span><span class="p">(</span><span class="n">intersection</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">]))</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="p">):</span>
                <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="o">.</span><span class="n">copy</span><span class="p">(),</span> <span class="nb">tuple</span><span class="p">(</span>
                    <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">ndim</span><span class="p">(</span><span class="n">intersection</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">]))),</span> <span class="nb">min</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">intersection</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">unbalance_weight</span>
        <span class="n">intersection</span> <span class="o">=</span> <span class="n">intersection</span> <span class="o">*</span> <span class="n">sample_weight</span>
        <span class="n">den1</span> <span class="o">=</span> <span class="n">den1</span> <span class="o">*</span> <span class="n">sample_weight</span>
        <span class="n">den2</span> <span class="o">=</span> <span class="n">den2</span> <span class="o">*</span> <span class="n">sample_weight</span>

        <span class="n">dice</span> <span class="o">=</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">intersection</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">den1</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">den2</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">smooth</span><span class="p">)</span>

        <span class="c1"># if output.shape[1]==2:</span>
        <span class="c1">#     dice =  (2.0 * (intersection * sample_weight[:,1:].sum(1) )+ self.smooth) / clip(den1[:,1:].sum(1) + den2[:,1:].sum(1) + self.smooth,min=1e-7)</span>
        <span class="c1"># else:</span>
        <span class="c1">#</span>
        <span class="c1">#     dice =  torch.div((2.0 * (intersection * sample_weight).sum(1) + self.smooth) , clip(den1.sum(1) + den2.sum(1) + self.smooth,min=1e-7))</span>

        <span class="c1"># if any_abnormal_number(den2):</span>
        <span class="c1">#     print(&#39;{0} after dice {1}&#39;.format(self.name,dice))</span>

        <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">dice</span>


<div class="viewcode-block" id="KLDivergenceLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.KLDivergenceLoss">[docs]</a><span class="k">class</span> <span class="nc">KLDivergenceLoss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=-</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;KLDivergenceLoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span>
                         <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                         <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                         <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span> <span class="o">=</span> <span class="kc">False</span>

<div class="viewcode-block" id="KLDivergenceLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.KLDivergenceLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>


        <span class="n">loss</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">kl_div</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="L1Loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.L1Loss">[docs]</a><span class="k">class</span> <span class="nc">L1Loss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;l1_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">     Function that takes the mean element-wise absolute value difference.</span>

<span class="sd">     See :class:`~torch.nn.L1Loss` for details.</span>
<span class="sd">     &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;L1Loss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                                     <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

<div class="viewcode-block" id="L1Loss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.L1Loss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">l1_loss</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="L2Loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.L2Loss">[docs]</a><span class="k">class</span> <span class="nc">L2Loss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;mse_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">        Measures the element-wise mean squared error.</span>

<span class="sd">        See :class:`~torch.nn.MSELoss` for details.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;MSELoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">L2Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                                     <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

<div class="viewcode-block" id="L2Loss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.L2Loss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="p">((</span><span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="SmoothL1Loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.SmoothL1Loss">[docs]</a><span class="k">class</span> <span class="nc">SmoothL1Loss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Function that uses a squared term if the absolute</span>
<span class="sd">    element-wise error falls below 1 and an L1 term otherwise.</span>

<span class="sd">    See :class:`~torch.nn.SmoothL1Loss` for details.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;SmoothL1Loss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SmoothL1Loss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span>
                                           <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">huber_delta</span> <span class="o">=</span> <span class="mf">0.5</span>

<div class="viewcode-block" id="SmoothL1Loss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.SmoothL1Loss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">smooth_l1_loss</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;none&#39;</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="MSELoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.MSELoss">[docs]</a><span class="k">class</span> <span class="nc">MSELoss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;mse_loss(input, target, size_average=None, reduce=None, reduction=&#39;mean&#39;) -&gt; Tensor</span>

<span class="sd">        Measures the element-wise mean squared error.</span>

<span class="sd">        See :class:`~torch.nn.MSELoss` for details.</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;MSELoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MSELoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                                      <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

<div class="viewcode-block" id="MSELoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.MSELoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="WingLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.WingLoss">[docs]</a><span class="k">class</span> <span class="nc">WingLoss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">omega</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;WingLoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">WingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">omega</span> <span class="o">=</span> <span class="n">omega</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>

<div class="viewcode-block" id="WingLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.WingLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">target</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span><span class="p">:</span>
            <span class="n">target</span><span class="o">=</span><span class="n">target</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">reshape</span><span class="p">((</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">delta_y</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
        <span class="n">c</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">omega</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">omega</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">))</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span>
            <span class="n">greater</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">omega</span><span class="p">,</span><span class="n">delta_y</span><span class="p">),</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">omega</span> <span class="o">*</span> <span class="n">log</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">delta_y</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">),</span>
            <span class="n">delta_y</span> <span class="o">-</span> <span class="n">c</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">losses</span></div></div>



<div class="viewcode-block" id="AdaptiveWingLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.AdaptiveWingLoss">[docs]</a><span class="k">class</span> <span class="nc">AdaptiveWingLoss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">omega</span><span class="o">=</span><span class="mi">14</span><span class="p">,</span> <span class="n">theta</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">2.1</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;AdaptiveWingLoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaptiveWingLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">omega</span> <span class="o">=</span> <span class="n">omega</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">=</span> <span class="n">theta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span> <span class="o">=</span> <span class="n">epsilon</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>

<div class="viewcode-block" id="AdaptiveWingLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.AdaptiveWingLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">lossMat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">omega</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">-</span> <span class="n">target</span><span class="p">)))</span> <span class="o">*</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">-</span> <span class="n">target</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span>
        <span class="n">C</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">*</span> <span class="n">A</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">omega</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">theta</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">-</span> <span class="n">target</span><span class="p">))</span>
        <span class="n">case1_ind</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span>
        <span class="n">case2_ind</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">target</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">theta</span>
        <span class="n">lossMat</span><span class="p">[</span><span class="n">case1_ind</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">omega</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
            <span class="mi">1</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">((</span><span class="n">target</span><span class="p">[</span><span class="n">case1_ind</span><span class="p">]</span> <span class="o">-</span> <span class="n">output</span><span class="p">[</span><span class="n">case1_ind</span><span class="p">])</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">epsilon</span><span class="p">)</span> <span class="o">**</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">-</span> <span class="n">target</span><span class="p">[</span><span class="n">case1_ind</span><span class="p">]))</span>
        <span class="n">lossMat</span><span class="p">[</span><span class="n">case2_ind</span><span class="p">]</span> <span class="o">=</span> <span class="n">A</span><span class="p">[</span><span class="n">case2_ind</span><span class="p">]</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">target</span><span class="p">[</span><span class="n">case2_ind</span><span class="p">]</span> <span class="o">-</span> <span class="n">output</span><span class="p">[</span><span class="n">case2_ind</span><span class="p">])</span> <span class="o">-</span> <span class="n">C</span><span class="p">[</span><span class="n">case2_ind</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">lossMat</span></div></div>


<div class="viewcode-block" id="ExponentialLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.ExponentialLoss">[docs]</a><span class="k">class</span> <span class="nc">ExponentialLoss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ExponentialLoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ExponentialLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                                              <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

<div class="viewcode-block" id="ExponentialLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.ExponentialLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="n">error</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">error</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">)</span><span class="o">.</span><span class="n">exp</span><span class="p">()</span>
            <span class="k">return</span> <span class="n">loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;output and target shape should the same in ExponentialLoss. &#39;</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="ItakuraSaitoLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.ItakuraSaitoLoss">[docs]</a><span class="k">class</span> <span class="nc">ItakuraSaitoLoss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ItakuraSaitoLoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">ItakuraSaitoLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                                               <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="n">name</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>

<div class="viewcode-block" id="ItakuraSaitoLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.ItakuraSaitoLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="c1">#  y_true/(y_pred+1e-12) - log(y_true/(y_pred+1e-12)) - 1;</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">output</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">target</span><span class="o">.</span><span class="n">min</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="mi">1</span>
                <span class="n">target</span> <span class="o">=</span> <span class="n">target</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">/</span> <span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span> <span class="o">-</span> <span class="p">((</span><span class="n">target</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">output</span> <span class="o">+</span> <span class="mf">1e-8</span><span class="p">))</span><span class="o">.</span><span class="n">log</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span>

            <span class="k">return</span> <span class="n">loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;output and target shape should the same in ItakuraSaitoLoss. &#39;</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="CosineSimilarityLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.CosineSimilarityLoss">[docs]</a><span class="k">class</span> <span class="nc">CosineSimilarityLoss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;CosineSimilarityLoss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CosineSimilarityLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span><span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span>
                                                   <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

<div class="viewcode-block" id="CosineSimilarityLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.CosineSimilarityLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">cosine_similarity</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div></div>


<span class="k">def</span> <span class="nf">gaussian</span><span class="p">(</span><span class="n">window_size</span><span class="p">,</span> <span class="n">sigma</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create 1-D gauss kernel</span>
<span class="sd">    Args:</span>
<span class="sd">        window_size (int): the size of gauss kernel</span>
<span class="sd">        sigma (float): sigma of normal distribution</span>
<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: 1D kernel (1 x 1 x size)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">coords</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">window_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">coords</span> <span class="o">-=</span> <span class="n">window_size</span> <span class="o">//</span> <span class="mi">2</span>

    <span class="n">gauss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="p">(</span><span class="n">coords</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span>
    <span class="n">gauss</span> <span class="o">/=</span> <span class="n">gauss</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">gauss</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">_gaussian_filter</span><span class="p">(</span><span class="nb">input</span><span class="p">,</span> <span class="n">window_1d</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Blur input with 1-D kernel</span>
<span class="sd">    :param x: batch of tensors to be blured</span>
<span class="sd">    :param window_1d: 1-D gauss kernel</span>
<span class="sd">    :param use_padding: padding image before conv</span>
<span class="sd">    :return: blured tensors</span>
<span class="sd">    &#39;&#39;&#39;</span>
    <span class="k">assert</span> <span class="nb">all</span><span class="p">([</span><span class="n">ws</span> <span class="o">==</span> <span class="mi">1</span> <span class="k">for</span> <span class="n">ws</span> <span class="ow">in</span> <span class="n">window_1d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]]),</span> <span class="n">window_1d</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">window_1d</span> <span class="o">=</span> <span class="n">window_1d</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
        <span class="n">conv</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv2d</span>
    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
        <span class="n">conv</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">conv3d</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

    <span class="n">C</span> <span class="o">=</span> <span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nb">input</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]):</span>
        <span class="k">if</span> <span class="n">s</span> <span class="o">&gt;=</span> <span class="n">window_1d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">conv</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">weight</span><span class="o">=</span><span class="n">window_1d</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">2</span> <span class="o">+</span> <span class="n">i</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">C</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span>
                <span class="sa">f</span><span class="s2">&quot;Skipping Gaussian Smoothing at dimension 2+</span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2"> for input: </span><span class="si">{</span><span class="nb">input</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2"> and win size: </span><span class="si">{</span><span class="n">window_1d</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>


<span class="k">def</span> <span class="nf">ssim</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">window</span><span class="p">,</span> <span class="n">data_range</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
    <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">    Calculate ssim index for X and Y</span>
<span class="sd">    :param X: images</span>
<span class="sd">    :param Y: images</span>
<span class="sd">    :param window: 1-D gauss kernel</span>
<span class="sd">    :param data_range: value range of input images. (usually 1.0 or 255)</span>
<span class="sd">    :param use_padding: padding image before conv</span>
<span class="sd">    :return:</span>
<span class="sd">    &#39;&#39;&#39;</span>

    <span class="n">K1</span> <span class="o">=</span> <span class="mf">0.01</span>
    <span class="n">K2</span> <span class="o">=</span> <span class="mf">0.03</span>
    <span class="n">compensation</span> <span class="o">=</span> <span class="mf">1.0</span>

    <span class="k">if</span> <span class="n">data_range</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">C1</span> <span class="o">=</span> <span class="p">((</span><span class="n">K1</span> <span class="o">*</span> <span class="n">data_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">data_range</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">C2</span> <span class="o">=</span> <span class="p">((</span><span class="n">K2</span> <span class="o">*</span> <span class="n">data_range</span><span class="p">)</span> <span class="o">/</span> <span class="n">data_range</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">C1</span> <span class="o">=</span> <span class="p">(</span><span class="n">K1</span> <span class="o">*</span> <span class="n">data_range</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">C2</span> <span class="o">=</span> <span class="p">(</span><span class="n">K2</span> <span class="o">*</span> <span class="n">data_range</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>

    <span class="n">mu1</span> <span class="o">=</span> <span class="n">_gaussian_filter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>
    <span class="n">mu2</span> <span class="o">=</span> <span class="n">_gaussian_filter</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span>

    <span class="n">mu1_sq</span> <span class="o">=</span> <span class="n">mu1</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">mu2_sq</span> <span class="o">=</span> <span class="n">mu2</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">mu1_mu2</span> <span class="o">=</span> <span class="n">mu1</span> <span class="o">*</span> <span class="n">mu2</span>

    <span class="n">sigma1_sq</span> <span class="o">=</span> <span class="n">compensation</span> <span class="o">*</span> <span class="p">(</span><span class="n">_gaussian_filter</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">X</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu1_sq</span><span class="p">)</span>
    <span class="n">sigma2_sq</span> <span class="o">=</span> <span class="n">compensation</span> <span class="o">*</span> <span class="p">(</span><span class="n">_gaussian_filter</span><span class="p">(</span><span class="n">Y</span> <span class="o">*</span> <span class="n">Y</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu2_sq</span><span class="p">)</span>
    <span class="n">sigma12</span> <span class="o">=</span> <span class="n">compensation</span> <span class="o">*</span> <span class="p">(</span><span class="n">_gaussian_filter</span><span class="p">(</span><span class="n">X</span> <span class="o">*</span> <span class="n">Y</span><span class="p">,</span> <span class="n">window</span><span class="p">)</span> <span class="o">-</span> <span class="n">mu1_mu2</span><span class="p">)</span>

    <span class="n">cs_map</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">sigma12</span> <span class="o">+</span> <span class="n">C2</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">sigma1_sq</span> <span class="o">+</span> <span class="n">sigma2_sq</span> <span class="o">+</span> <span class="n">C2</span><span class="p">)</span>  <span class="c1"># set alpha=beta=gamma=1</span>
    <span class="n">ssim_map</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span> <span class="o">*</span> <span class="n">mu1_mu2</span> <span class="o">+</span> <span class="n">C1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">mu1_sq</span> <span class="o">+</span> <span class="n">mu2_sq</span> <span class="o">+</span> <span class="n">C1</span><span class="p">))</span> <span class="o">*</span> <span class="n">cs_map</span>

    <span class="n">ssim_per_channel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">ssim_map</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">cs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">flatten</span><span class="p">(</span><span class="n">cs_map</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">ssim_per_channel</span><span class="p">,</span> <span class="n">cs</span>


<div class="viewcode-block" id="MS_SSIMLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.MS_SSIMLoss">[docs]</a><span class="k">class</span> <span class="nc">MS_SSIMLoss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">window_size</span><span class="o">=</span><span class="mi">11</span><span class="p">,</span> <span class="n">window_sigma</span><span class="o">=</span><span class="mf">1.5</span><span class="p">,</span> <span class="n">data_range</span><span class="o">=</span><span class="mf">255.</span><span class="p">,</span> <span class="n">channel</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">use_padding</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weights</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">name</span><span class="o">=</span><span class="s1">&#39;ms_ssim&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">MS_SSIMLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span>
                                          <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">window_size</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Window size should be odd.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">=</span> <span class="n">window_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">channel</span> <span class="o">=</span> <span class="n">channel</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">window_sigma</span> <span class="o">=</span> <span class="n">window_sigma</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">data_range</span> <span class="o">=</span> <span class="n">data_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_padding</span> <span class="o">=</span> <span class="n">use_padding</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

        <span class="n">spatial_dims</span> <span class="o">=</span> <span class="mi">2</span><span class="p">,</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">data_range</span> <span class="o">=</span> <span class="n">data_range</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span>

        <span class="k">if</span> <span class="n">weights</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.0448</span><span class="p">,</span> <span class="mf">0.2856</span><span class="p">,</span> <span class="mf">0.3001</span><span class="p">,</span> <span class="mf">0.2363</span><span class="p">,</span> <span class="mf">0.1333</span><span class="p">]</span>
        <span class="n">weights</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">Dtype</span><span class="o">.</span><span class="n">float</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">levels</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">levels</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="p">[:</span><span class="bp">self</span><span class="o">.</span><span class="n">levels</span><span class="p">]</span>
            <span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span> <span class="o">/</span> <span class="n">weights</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">weights</span> <span class="o">=</span> <span class="n">weights</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">())</span>

<div class="viewcode-block" id="MS_SSIMLoss.build"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.MS_SSIMLoss.build">[docs]</a>    <span class="k">def</span> <span class="nf">build</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">win</span> <span class="o">=</span> <span class="n">gaussian</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">window_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">window_sigma</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">win</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">win</span><span class="o">.</span><span class="n">repeat</span><span class="p">([</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span> <span class="o">+</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_built</span> <span class="o">=</span> <span class="kc">True</span></div>

<div class="viewcode-block" id="MS_SSIMLoss.ms_ssim"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.MS_SSIMLoss.ms_ssim">[docs]</a>    <span class="k">def</span> <span class="nf">ms_ssim</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">):</span>
        <span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        interface of ms-ssim</span>
<span class="sd">        :param X: a batch of images, (N,C,H,W)</span>
<span class="sd">        :param Y: a batch of images, (N,C,H,W)</span>
<span class="sd">        :param window: 1-D gauss kernel</span>
<span class="sd">        :param data_range: value range of input images. (usually 1.0 or 255)</span>
<span class="sd">        :param weights: weights for different levels</span>
<span class="sd">        :param use_padding: padding image before conv</span>
<span class="sd">        :param eps: use for avoid grad nan.</span>
<span class="sd">        :return:</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span> <span class="o">==</span> <span class="n">Y</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Input images should have the same dimensions.&quot;</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">d</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>
            <span class="n">Y</span> <span class="o">=</span> <span class="n">Y</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">d</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">X</span><span class="o">.</span><span class="n">type</span><span class="p">()</span> <span class="o">==</span> <span class="n">Y</span><span class="o">.</span><span class="n">type</span><span class="p">():</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Input images should have the same dtype. X:</span><span class="si">{0}</span><span class="s1"> Y:</span><span class="si">{1}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">type</span><span class="p">(),</span> <span class="n">Y</span><span class="o">.</span><span class="n">type</span><span class="p">()))</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">avg_pool</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">avg_pool2d</span>
        <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">avg_pool</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">avg_pool3d</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Input images should be 4-d or 5-d tensors, but got </span><span class="si">{</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

        <span class="n">smaller_side</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span>
        <span class="k">assert</span> <span class="n">smaller_side</span> <span class="o">&gt;</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                <span class="mi">2</span> <span class="o">**</span> <span class="mi">4</span>
        <span class="p">),</span> <span class="s2">&quot;Image size should be larger than </span><span class="si">%d</span><span class="s2"> due to the 4 downsamplings in ms-ssim&quot;</span> <span class="o">%</span> <span class="p">(</span>
                    <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">window_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">4</span><span class="p">))</span>

        <span class="n">mcs</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">levels</span><span class="p">):</span>
            <span class="n">ssim_per_channel</span><span class="p">,</span> <span class="n">cs</span> <span class="o">=</span> <span class="n">ssim</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">win</span><span class="p">,</span> <span class="n">data_range</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">data_range</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">levels</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">mcs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">cs</span><span class="p">))</span>
                <span class="n">padding</span> <span class="o">=</span> <span class="p">[</span><span class="n">s</span> <span class="o">%</span> <span class="mi">2</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">:]]</span>
                <span class="n">X</span> <span class="o">=</span> <span class="n">avg_pool</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>
                <span class="n">Y</span> <span class="o">=</span> <span class="n">avg_pool</span><span class="p">(</span><span class="n">Y</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">)</span>

        <span class="n">ssim_per_channel</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">ssim_per_channel</span><span class="p">)</span>  <span class="c1"># (batch, channel)</span>
        <span class="n">mcs_and_ssim</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">mcs</span> <span class="o">+</span> <span class="p">[</span><span class="n">ssim_per_channel</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (level, batch, channel)</span>
        <span class="n">ms_ssim_val</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">prod</span><span class="p">(</span><span class="n">mcs_and_ssim</span> <span class="o">**</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">ms_ssim_val</span></div>

<div class="viewcode-block" id="MS_SSIMLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.MS_SSIMLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ms_ssim</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="IoULoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.IoULoss">[docs]</a><span class="k">class</span> <span class="nc">IoULoss</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lou_loss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">IoULoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">raxis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span>
                                      <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                                      <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span>
                                      <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span>
                                      <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_multiselection</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="IoULoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.IoULoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">B</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="ow">and</span> <span class="n">reduce_max</span><span class="p">(</span><span class="n">output</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">exp</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">is_target_onehot</span><span class="p">:</span>
            <span class="n">target</span> <span class="o">=</span> <span class="n">make_onehot</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">C</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

        <span class="n">sample_weight</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">())</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">ignore_index_weight</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">get_device</span><span class="p">()))</span>

        <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">ones</span><span class="p">((</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">))</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">auto_balance</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="p">):</span>
                <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">to_tensor</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">label_statistics</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                <span class="n">unbalance_weight</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">unbalance_weight</span> <span class="o">&lt;</span> <span class="mf">1e-7</span><span class="p">,</span> <span class="n">ones</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_float_dtype</span><span class="p">),</span>
                                         <span class="nb">pow</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">clip</span><span class="p">(</span><span class="n">unbalance_weight</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="mf">1e-7</span><span class="p">),</span> <span class="mf">3.5</span><span class="p">))</span>
        <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">unbalance_weight</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="n">tp</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="n">target</span>
        <span class="n">fp</span> <span class="o">=</span> <span class="n">output</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">fn</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">output</span><span class="p">)</span> <span class="o">*</span> <span class="n">target</span>
        <span class="n">intersection</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">union</span> <span class="o">=</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">tp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">fp</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">reduce_sum</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">iou</span> <span class="o">=</span> <span class="p">(</span><span class="n">intersection</span> <span class="o">*</span> <span class="n">sample_weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="n">clip</span><span class="p">((</span><span class="n">union</span> <span class="o">*</span> <span class="n">sample_weight</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="nb">min</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">)</span>
        <span class="k">return</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">iou</span></div></div>


<div class="viewcode-block" id="SoftIoULoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.SoftIoULoss">[docs]</a><span class="k">class</span> <span class="nc">SoftIoULoss</span><span class="p">(</span><span class="n">Loss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">reduced</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;softiou&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">SoftIoULoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span> <span class="o">=</span> <span class="n">n_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduced</span> <span class="o">=</span> <span class="n">reduced</span>

<div class="viewcode-block" id="SoftIoULoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.SoftIoULoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># logit =&gt; N x Classes x H x W</span>
        <span class="c1"># target =&gt; N x H x W</span>

        <span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>

        <span class="n">pred</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">target_onehot</span> <span class="o">=</span> <span class="n">make_onehot</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">)</span>

        <span class="c1"># Numerator Product</span>
        <span class="n">inter</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">*</span> <span class="n">target_onehot</span>
        <span class="c1"># Sum over all pixels N x C x H x W =&gt; N x C</span>
        <span class="n">inter</span> <span class="o">=</span> <span class="n">inter</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="c1"># Denominator</span>
        <span class="n">union</span> <span class="o">=</span> <span class="n">pred</span> <span class="o">+</span> <span class="n">target_onehot</span> <span class="o">-</span> <span class="p">(</span><span class="n">pred</span> <span class="o">*</span> <span class="n">target_onehot</span><span class="p">)</span>
        <span class="c1"># Sum over all pixels N x C x H x W =&gt; N x C</span>
        <span class="n">union</span> <span class="o">=</span> <span class="n">union</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="n">inter</span> <span class="o">/</span> <span class="p">(</span><span class="n">union</span> <span class="o">+</span> <span class="mf">1e-16</span><span class="p">)</span>

        <span class="c1"># Return average loss over classes and batch</span>
        <span class="k">return</span> <span class="o">-</span><span class="n">loss</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span></div></div>


<span class="k">def</span> <span class="nf">_lovasz_grad</span><span class="p">(</span><span class="n">gt_sorted</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes gradient of the Lovasz extension w.r.t sorted errors</span>
<span class="sd">    See Alg. 1 in paper</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">gt_sorted</span><span class="p">)</span>
    <span class="n">gts</span> <span class="o">=</span> <span class="n">gt_sorted</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
    <span class="n">intersection</span> <span class="o">=</span> <span class="n">gts</span> <span class="o">-</span> <span class="n">gt_sorted</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">union</span> <span class="o">=</span> <span class="n">gts</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">gt_sorted</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">jaccard</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">-</span> <span class="n">intersection</span> <span class="o">/</span> <span class="n">union</span>
    <span class="k">if</span> <span class="n">p</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>  <span class="c1"># cover 1-pixel case</span>
        <span class="n">jaccard</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">p</span><span class="p">]</span> <span class="o">=</span> <span class="n">jaccard</span><span class="p">[</span><span class="mi">1</span><span class="p">:</span><span class="n">p</span><span class="p">]</span> <span class="o">-</span> <span class="n">jaccard</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">jaccard</span>


<div class="viewcode-block" id="LovaszSoftmax"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.LovaszSoftmax">[docs]</a><span class="k">class</span> <span class="nc">LovaszSoftmax</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">from_logits</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">label_smooth</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">ohem_thresh</span><span class="o">=</span><span class="mf">0.7</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;lovasz_loss&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LovaszSoftmax</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">raxis</span><span class="o">=</span><span class="n">axis</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span> <span class="n">auto_balance</span><span class="o">=</span><span class="n">auto_balance</span><span class="p">,</span>
                                            <span class="n">from_logits</span><span class="o">=</span><span class="n">from_logits</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="n">ignore_index</span><span class="p">,</span> <span class="n">cutoff</span><span class="o">=</span><span class="n">cutoff</span><span class="p">,</span>
                                            <span class="n">label_smooth</span><span class="o">=</span><span class="n">label_smooth</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">,</span> <span class="n">enable_ohem</span><span class="o">=</span><span class="n">enable_ohem</span><span class="p">,</span>
                                            <span class="n">ohem_thresh</span><span class="o">=</span><span class="n">ohem_thresh</span><span class="p">,</span> <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span>
                                            <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_multiselection</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">is_logsoftmax</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="kc">None</span>

<div class="viewcode-block" id="LovaszSoftmax.flatten_check"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.LovaszSoftmax.flatten_check">[docs]</a>    <span class="k">def</span> <span class="nf">flatten_check</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>
        <span class="k">assert</span> <span class="n">output</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="ow">in</span> <span class="p">[</span><span class="mi">4</span><span class="p">,</span> <span class="mi">5</span><span class="p">]</span>
        <span class="n">output_flatten</span> <span class="o">=</span> <span class="n">output_flattenNone</span>
        <span class="k">if</span> <span class="n">output</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">output_flatten</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">output</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">==</span> <span class="mi">5</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
            <span class="n">output_flatten</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_class</span><span class="p">)</span>
        <span class="n">target_flatten</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">output_flatten</span><span class="p">,</span> <span class="n">target_flatten</span></div>

<div class="viewcode-block" id="LovaszSoftmax.lovasz_softmax_flat"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.LovaszSoftmax.lovasz_softmax_flat">[docs]</a>    <span class="k">def</span> <span class="nf">lovasz_softmax_flat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">):</span>

        <span class="n">losses</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">):</span>
            <span class="n">target_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">target</span> <span class="o">==</span> <span class="n">c</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">input_c</span> <span class="o">=</span> <span class="n">output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">input_c</span> <span class="o">=</span> <span class="n">output</span><span class="p">[:,</span> <span class="n">c</span><span class="p">]</span>
            <span class="n">loss_c</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">target_c</span><span class="p">)</span> <span class="o">-</span> <span class="n">input_c</span><span class="p">)</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
            <span class="n">loss_c_sorted</span><span class="p">,</span> <span class="n">loss_index</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">loss_c</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">target_c_sorted</span> <span class="o">=</span> <span class="n">target_c</span><span class="p">[</span><span class="n">loss_index</span><span class="p">]</span>
            <span class="n">losses</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">loss_c_sorted</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">_lovasz_grad</span><span class="p">(</span><span class="n">target_c_sorted</span><span class="p">))))</span>
        <span class="n">losses</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">losses</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">losses</span></div>

<div class="viewcode-block" id="LovaszSoftmax.flatten_binary_scores"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.LovaszSoftmax.flatten_binary_scores">[docs]</a>    <span class="k">def</span> <span class="nf">flatten_binary_scores</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">ignore</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Flattens predictions in the batch (binary case)</span>
<span class="sd">        Remove labels equal to &#39;ignore&#39;</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">ignore</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">scores</span><span class="p">,</span> <span class="n">labels</span>
        <span class="n">valid</span> <span class="o">=</span> <span class="p">(</span><span class="n">labels</span> <span class="o">!=</span> <span class="n">ignore</span><span class="p">)</span>
        <span class="n">vscores</span> <span class="o">=</span> <span class="n">scores</span><span class="p">[</span><span class="n">valid</span><span class="p">]</span>
        <span class="n">vlabels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">valid</span><span class="p">]</span>
        <span class="k">return</span> <span class="n">vscores</span><span class="p">,</span> <span class="n">vlabels</span></div>

<div class="viewcode-block" id="LovaszSoftmax.lovasz_hinge"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.LovaszSoftmax.lovasz_hinge">[docs]</a>    <span class="k">def</span> <span class="nf">lovasz_hinge</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">per_image</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">ignore</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Binary Lovasz hinge loss</span>
<span class="sd">          logits: [B, H, W] Variable, logits at each pixel (between -\infty and +\infty)</span>
<span class="sd">          labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)</span>
<span class="sd">          per_image: compute the loss per image instead of per batch</span>
<span class="sd">          ignore: void class id</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">per_image</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lovasz_hinge_flat</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">flatten_binary_scores</span><span class="p">(</span><span class="n">log</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">lab</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">ignore</span><span class="p">))</span> <span class="k">for</span>
                    <span class="n">log</span><span class="p">,</span> <span class="n">lab</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lovasz_hinge_flat</span><span class="p">(</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">flatten_binary_scores</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">,</span> <span class="n">ignore</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">loss</span></div>

<div class="viewcode-block" id="LovaszSoftmax.lovasz_hinge_flat"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.LovaszSoftmax.lovasz_hinge_flat">[docs]</a>    <span class="k">def</span> <span class="nf">lovasz_hinge_flat</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">logits</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Binary Lovasz hinge loss</span>
<span class="sd">          logits: [P] Variable, logits at each prediction (between -\infty and +\infty)</span>
<span class="sd">          labels: [P] Tensor, binary ground truth labels (0 or 1)</span>
<span class="sd">          ignore: label to ignore</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># only void pixels, the gradients should be 0</span>
            <span class="k">return</span> <span class="n">logits</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.</span>
        <span class="n">signs</span> <span class="o">=</span> <span class="mf">2.</span> <span class="o">*</span> <span class="n">labels</span><span class="o">.</span><span class="n">float</span><span class="p">()</span> <span class="o">-</span> <span class="mf">1.</span>
        <span class="n">errors</span> <span class="o">=</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">-</span> <span class="n">logits</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">signs</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
        <span class="n">errors_sorted</span><span class="p">,</span> <span class="n">perm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">errors</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">perm</span><span class="o">.</span><span class="n">data</span>
        <span class="n">gt_sorted</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
        <span class="n">grad</span> <span class="o">=</span> <span class="n">_lovasz_grad</span><span class="p">(</span><span class="n">gt_sorted</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">errors_sorted</span><span class="p">),</span> <span class="n">grad</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">loss</span></div>

<div class="viewcode-block" id="LovaszSoftmax.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.LovaszSoftmax.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># print(output.shape, target.shape) # (batch size, class_num, x,y,z), (batch size, 1, x,y,z)</span>
        <span class="c1"># print(output.shape, target.shape)</span>

        <span class="n">losses</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lovasz_softmax_flat</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">&gt;</span> <span class="mi">2</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">lovasz_hinge_flat</span><span class="p">(</span><span class="n">output</span><span class="p">,</span>
                                                                                                              <span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">losses</span></div></div>


<div class="viewcode-block" id="TripletLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.TripletLoss">[docs]</a><span class="k">class</span> <span class="nc">TripletLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Creates a criterion that measures the triplet loss given an input</span>
<span class="sd">    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.</span>
<span class="sd">    This is used for measuring a relative similarity between samples. A triplet</span>
<span class="sd">    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative</span>
<span class="sd">    examples` respectively). The shapes of all input tensors should be</span>
<span class="sd">    :math:`(N, D)`.</span>

<span class="sd">    The distance swap is described in detail in the paper `Learning shallow</span>
<span class="sd">    convolutional feature descriptors with triplet losses`_ by</span>
<span class="sd">    V. Balntas, E. Riba et al.</span>

<span class="sd">    The loss function for each sample in the mini-batch is:</span>

<span class="sd">    math::</span>
<span class="sd">        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}</span>


<span class="sd">    where</span>

<span class="sd">     math::</span>
<span class="sd">        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p</span>

<span class="sd">    Args:</span>
<span class="sd">        margin (float, optional): Default: :math:`1`.</span>
<span class="sd">        p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.</span>
<span class="sd">        swap (bool, optional): The distance swap is described in detail in the paper</span>
<span class="sd">            `Learning shallow convolutional feature descriptors with triplet losses` by</span>
<span class="sd">            V. Balntas, E. Riba et al. Default: ``False``.</span>
<span class="sd">        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,</span>
<span class="sd">            the losses are averaged over each loss element in the batch. Note that for</span>
<span class="sd">            some losses, there are multiple elements per sample. If the field :attr:`size_average`</span>
<span class="sd">            is set to ``False``, the losses are instead summed for each minibatch. Ignored</span>
<span class="sd">            when reduce is ``False``. Default: ``True``</span>
<span class="sd">        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the</span>
<span class="sd">            losses are averaged or summed over observations for each minibatch depending</span>
<span class="sd">            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per</span>
<span class="sd">            batch element instead and ignores :attr:`size_average`. Default: ``True``</span>
<span class="sd">        reduction (string, optional): Specifies the reduction to apply to the output:</span>
<span class="sd">            ``&#39;none&#39;`` | ``&#39;mean&#39;`` | ``&#39;sum&#39;``. ``&#39;none&#39;``: no reduction will be applied,</span>
<span class="sd">            ``&#39;mean&#39;``: the sum of the output will be divided by the number of</span>
<span class="sd">            elements in the output, ``&#39;sum&#39;``: the output will be summed. Note: :attr:`size_average`</span>
<span class="sd">            and :attr:`reduce` are in the process of being deprecated, and in the meantime,</span>
<span class="sd">            specifying either of those two args will override :attr:`reduction`. Default: ``&#39;mean&#39;``</span>

<span class="sd">    Shape:</span>
<span class="sd">        - Input: :math:`(N, D)` where :math:`D` is the vector dimension.</span>
<span class="sd">        - Output: scalar. If :attr:`reduction` is ``&#39;none&#39;``, then :math:`(N)`.</span>

<span class="sd">    &gt;&gt;&gt; triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)</span>
<span class="sd">    &gt;&gt;&gt; anchor = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; positive = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; negative = torch.randn(100, 128, requires_grad=True)</span>
<span class="sd">    &gt;&gt;&gt; output = triplet_loss(anchor, positive, negative)</span>
<span class="sd">    &gt;&gt;&gt; output.backward()</span>

<span class="sd">     _Learning shallow convolutional feature descriptors with triplet losses:</span>
<span class="sd">        http://www.bmva.org/bmvc/2016/papers/paper119/index.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">__constants__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;margin&#39;</span><span class="p">,</span> <span class="s1">&#39;p&#39;</span><span class="p">,</span> <span class="s1">&#39;eps&#39;</span><span class="p">,</span> <span class="s1">&#39;swap&#39;</span><span class="p">,</span> <span class="s1">&#39;reduction&#39;</span><span class="p">]</span>
    <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span>
    <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">2.</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-6</span><span class="p">,</span> <span class="n">swap</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
                 <span class="n">reduction</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s1">&#39;mean&#39;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TripletLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="n">reduction</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">swap</span> <span class="o">=</span> <span class="n">swap</span>

<div class="viewcode-block" id="TripletLoss.forward"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.TripletLoss.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">triplet_margin_loss</span><span class="p">(</span><span class="n">anchor</span><span class="p">,</span> <span class="n">positive</span><span class="p">,</span> <span class="n">negative</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">margin</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">,</span>
                                     <span class="n">eps</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">,</span> <span class="n">swap</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">swap</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)</span></div></div>


<span class="n">TripletMarginLoss</span> <span class="o">=</span> <span class="n">TripletLoss</span>


<span class="k">class</span> <span class="nc">HardTripletLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Hard Triplet Loss</span>
<span class="sd">    (pytorch implementation of https://omoindrot.github.io/triplet-loss)</span>

<span class="sd">    For each anchor, we get the hardest positive and hardest negative to form a triplet.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">margin</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">hardest</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            margin: margin for triplet loss</span>
<span class="sd">            hardest: If true, loss is considered only hardest triplets.</span>
<span class="sd">            squared: If true, output is the pairwise squared euclidean distance matrix.</span>
<span class="sd">                If false, output is the pairwise euclidean distance matrix.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">HardTripletLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">margin</span> <span class="o">=</span> <span class="n">margin</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">hardest</span> <span class="o">=</span> <span class="n">hardest</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">squared</span> <span class="o">=</span> <span class="n">squared</span>

    <span class="k">def</span> <span class="nf">_pairwise_distance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-16</span><span class="p">):</span>
        <span class="c1"># Compute the 2D matrix of distances between all the embeddings.</span>

        <span class="n">cor_mat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        <span class="n">norm_mat</span> <span class="o">=</span> <span class="n">cor_mat</span><span class="o">.</span><span class="n">diag</span><span class="p">()</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">norm_mat</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">cor_mat</span> <span class="o">+</span> <span class="n">norm_mat</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">distances</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">squared</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">distances</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">distances</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">+</span> <span class="n">mask</span> <span class="o">*</span> <span class="n">eps</span>
            <span class="n">distances</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">distances</span><span class="p">)</span>
            <span class="n">distances</span> <span class="o">=</span> <span class="n">distances</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mask</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">distances</span>

    <span class="k">def</span> <span class="nf">_get_anchor_positive_triplet_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="c1"># Return a 2D mask where mask[a, p] is True iff a and p are distinct and have same labeled.</span>

        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="n">indices_not_equal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span> <span class="o">^</span> <span class="mi">1</span>

        <span class="c1"># Check if labels[i] == labels[j]</span>
        <span class="n">labels_equal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">indices_not_equal</span> <span class="o">*</span> <span class="n">labels_equal</span>

        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span> <span class="nf">_get_anchor_negative_triplet_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="c1"># Return a 2D mask where mask[a, n] is True iff a and n have distinct labels.</span>

        <span class="c1"># Check if labels[i] != labels[k]</span>
        <span class="n">labels_equal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">labels_equal</span> <span class="o">^</span> <span class="mi">1</span>

        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span> <span class="nf">_get_triplet_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Return a 3D mask where mask[a, p, n] is True iff the triplet (a, p, n) is valid.</span>

<span class="sd">        A triplet (i, j, k) is valid if:</span>
<span class="sd">            - i, j, k are distinct</span>
<span class="sd">            - labels[i] == labels[j] and labels[i] != labels[k]</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda:0&quot;</span> <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

        <span class="c1"># Check that i, j and k are distinct</span>
        <span class="n">indices_not_same</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">labels</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">byte</span><span class="p">()</span> <span class="o">^</span> <span class="mi">1</span>
        <span class="n">i_not_equal_j</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">indices_not_same</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">i_not_equal_k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">indices_not_same</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">j_not_equal_k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">indices_not_same</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">distinct_indices</span> <span class="o">=</span> <span class="n">i_not_equal_j</span> <span class="o">*</span> <span class="n">i_not_equal_k</span> <span class="o">*</span> <span class="n">j_not_equal_k</span>

        <span class="c1"># Check if labels[i] == labels[j] and labels[i] != labels[k]</span>
        <span class="n">label_equal</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
        <span class="n">i_equal_j</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">label_equal</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
        <span class="n">i_equal_k</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">label_equal</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">valid_labels</span> <span class="o">=</span> <span class="n">i_equal_j</span> <span class="o">*</span> <span class="p">(</span><span class="n">i_equal_k</span> <span class="o">^</span> <span class="mi">1</span><span class="p">)</span>

        <span class="n">mask</span> <span class="o">=</span> <span class="n">distinct_indices</span> <span class="o">*</span> <span class="n">valid_labels</span>  <span class="c1"># Combine the two masks</span>

        <span class="k">return</span> <span class="n">mask</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            labels: labels of the batch, of size (batch_size,)</span>
<span class="sd">            embeddings: tensor of shape (batch_size, embed_dim)</span>

<span class="sd">        Returns:</span>
<span class="sd">            triplet_loss: scalar tensor containing the triplet loss</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">pairwise_dist</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pairwise_distance</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">squared</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">squared</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">hardest</span><span class="p">:</span>
            <span class="c1"># Get the hardest positive pairs</span>
            <span class="n">mask_anchor_positive</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_anchor_positive_triplet_mask</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">valid_positive_dist</span> <span class="o">=</span> <span class="n">pairwise_dist</span> <span class="o">*</span> <span class="n">mask_anchor_positive</span>
            <span class="n">hardest_positive_dist</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">valid_positive_dist</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># Get the hardest negative pairs</span>
            <span class="n">mask_anchor_negative</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_anchor_negative_triplet_mask</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">max_anchor_negative_dist</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">pairwise_dist</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
            <span class="n">anchor_negative_dist</span> <span class="o">=</span> <span class="n">pairwise_dist</span> <span class="o">+</span> <span class="n">max_anchor_negative_dist</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">mask_anchor_negative</span><span class="p">)</span>
            <span class="n">hardest_negative_dist</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">anchor_negative_dist</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

            <span class="c1"># Combine biggest d(a, p) and smallest d(a, n) into final triplet loss</span>
            <span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">hardest_positive_dist</span> <span class="o">-</span> <span class="n">hardest_negative_dist</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">)</span>
            <span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">triplet_loss</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">anc_pos_dist</span> <span class="o">=</span> <span class="n">pairwise_dist</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
            <span class="n">anc_neg_dist</span> <span class="o">=</span> <span class="n">pairwise_dist</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

            <span class="c1"># Compute a 3D tensor of size (batch_size, batch_size, batch_size)</span>
            <span class="c1"># triplet_loss[i, j, k] will contain the triplet loss of anc=i, pos=j, neg=k</span>
            <span class="c1"># Uses broadcasting where the 1st argument has shape (batch_size, batch_size, 1)</span>
            <span class="c1"># and the 2nd (batch_size, 1, batch_size)</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">anc_pos_dist</span> <span class="o">-</span> <span class="n">anc_neg_dist</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">margin</span>

            <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_triplet_mask</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">*</span> <span class="n">mask</span>

            <span class="c1"># Remove negative losses (i.e. the easy triplets)</span>
            <span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">triplet_loss</span><span class="p">)</span>

            <span class="c1"># Count number of hard triplets (where triplet_loss &gt; 0)</span>
            <span class="n">hard_triplets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">gt</span><span class="p">(</span><span class="n">triplet_loss</span><span class="p">,</span> <span class="mf">1e-16</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
            <span class="n">num_hard_triplets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">hard_triplets</span><span class="p">)</span>

            <span class="n">triplet_loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">triplet_loss</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">num_hard_triplets</span> <span class="o">+</span> <span class="mf">1e-16</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">triplet_loss</span>


<div class="viewcode-block" id="CenterLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.CenterLoss">[docs]</a><span class="k">class</span> <span class="nc">CenterLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Center loss.</span>
<span class="sd">    Reference:</span>
<span class="sd">    Wen et al. A Discriminative Feature Learning Approach for Deep Face Recognition. ECCV 2016.</span>

<span class="sd">    Args:</span>
<span class="sd">        num_classes (int): number of classes.</span>
<span class="sd">        feat_dim (int): feature dimension.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">751</span><span class="p">,</span> <span class="n">feat_dim</span><span class="o">=</span><span class="mi">2048</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">,</span> <span class="n">reduced</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">CenterLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">reduced</span> <span class="o">=</span> <span class="n">reduced</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">num_classes</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feat_dim</span> <span class="o">=</span> <span class="n">feat_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">centers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">feat_dim</span><span class="p">))</span>
        <span class="n">init</span><span class="o">.</span><span class="n">kaiming_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">centers</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">5</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_device</span><span class="p">)</span>

<div class="viewcode-block" id="CenterLoss.forward"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.CenterLoss.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Args:</span>
<span class="sd">            features: feature matrix with shape (batch_size, feat_dim).</span>
<span class="sd">            target: ground truth labels with shape (num_classes).</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">batch_size</span> <span class="o">=</span> <span class="n">features</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">centers</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">distmat</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">features</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">centers</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">features</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span>
                                                                                                    <span class="n">batch_size</span><span class="p">)</span><span class="o">.</span><span class="n">t</span><span class="p">()</span>
        <span class="n">distmat</span><span class="o">.</span><span class="n">addmm_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">centers</span><span class="o">.</span><span class="n">t</span><span class="p">())</span>
        <span class="n">classes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_device</span><span class="p">)</span>

        <span class="n">target</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">eq</span><span class="p">(</span><span class="n">classes</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">))</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">batch_size</span><span class="p">):</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">distmat</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">mask</span><span class="p">[</span><span class="n">i</span><span class="p">]]</span>
            <span class="n">value</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="mf">1e+5</span><span class="p">)</span>  <span class="c1"># for numerical stability</span>
            <span class="n">dist</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
        <span class="n">dist</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">dist</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">==</span> <span class="s1">&#39;mean&#39;</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># / self.num_classes</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">dist</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>  <span class="c1"># / self.num_classes</span></div></div>


<span class="k">class</span> <span class="nc">StyleLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">StyleLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_device</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span>
        <span class="n">target</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
        <span class="n">img_size</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
        <span class="n">G</span> <span class="o">=</span> <span class="n">gram_matrix</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">Gt</span> <span class="o">=</span> <span class="n">gram_matrix</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">F</span><span class="o">.</span><span class="n">mse_loss</span><span class="p">(</span><span class="n">G</span><span class="p">,</span> <span class="n">Gt</span><span class="p">)</span><span class="o">.</span><span class="n">div</span><span class="p">((</span><span class="n">img_size</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">img_size</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span> <span class="n">img_size</span><span class="p">[</span><span class="mi">3</span><span class="p">]))</span>


<div class="viewcode-block" id="PerceptionLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.PerceptionLoss">[docs]</a><span class="k">class</span> <span class="nc">PerceptionLoss</span><span class="p">(</span><span class="n">_PairwiseLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">net</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PerceptionLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">net</span><span class="p">,</span> <span class="n">ModelBase</span><span class="p">):</span>
            <span class="n">net</span> <span class="o">=</span> <span class="n">net</span><span class="o">.</span><span class="n">model</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span> <span class="o">=</span> <span class="n">net</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="o">.</span><span class="n">trainable</span> <span class="o">=</span> <span class="kc">False</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">layer_name_mapping</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;block1_relu2&#39;</span><span class="p">,</span> <span class="s1">&#39;block2_relu2&#39;</span><span class="p">,</span> <span class="s1">&#39;block3_relu3&#39;</span><span class="p">,</span> <span class="s1">&#39;block4_relu3&#39;</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">module</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="o">.</span><span class="n">named_modules</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_name_mapping</span><span class="o">.</span><span class="n">values</span><span class="p">()):</span>
                <span class="n">module</span><span class="o">.</span><span class="n">keep_output</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="o">.</span><span class="n">_modules</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
            <span class="n">is_start_delete</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">if</span> <span class="n">k</span> <span class="o">==</span> <span class="s1">&#39;block4_conv3&#39;</span><span class="p">:</span>
                <span class="n">is_start_delete</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="k">if</span> <span class="n">is_start_delete</span> <span class="ow">and</span> <span class="n">k</span> <span class="o">!=</span> <span class="s1">&#39;block4_conv3&#39;</span><span class="p">:</span>
                <span class="k">del</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="o">.</span><span class="n">_modules</span><span class="p">[</span><span class="n">k</span><span class="p">]</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_device</span><span class="p">)</span>

<div class="viewcode-block" id="PerceptionLoss.vgg_preprocess"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.PerceptionLoss.vgg_preprocess">[docs]</a>    <span class="k">def</span> <span class="nf">vgg_preprocess</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">img</span><span class="p">):</span>
        <span class="k">return</span> <span class="p">((</span><span class="n">img</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">to_tensor</span><span class="p">([[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="n">to_tensor</span><span class="p">(</span>
            <span class="p">[[</span><span class="mf">0.229</span><span class="p">,</span> <span class="mf">0.224</span><span class="p">,</span> <span class="mf">0.225</span><span class="p">]])</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span></div>

<div class="viewcode-block" id="PerceptionLoss.calculate_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.PerceptionLoss.calculate_loss">[docs]</a>    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            output ():</span>
<span class="sd">            target ():</span>
<span class="sd">            **kwargs ():</span>

<span class="sd">        Returns:</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">target_features</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">output_features</span> <span class="o">=</span> <span class="n">OrderedDict</span><span class="p">()</span>
        <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vgg_preprocess</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>

        <span class="c1"># _ = self.ref_model(output)</span>
        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_name_mapping</span><span class="p">:</span>
            <span class="n">output_features</span><span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span><span class="o">.</span><span class="n">output</span>

        <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vgg_preprocess</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>

        <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">layer_name_mapping</span><span class="p">:</span>
            <span class="n">target_features</span><span class="p">[</span><span class="n">item</span><span class="p">]</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ref_model</span><span class="p">,</span> <span class="n">item</span><span class="p">)</span><span class="o">.</span><span class="n">output</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

        <span class="n">loss</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">num_filters</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_name_mapping</span><span class="p">)):</span>
            <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">output_features</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span>
            <span class="n">loss</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">+</span> <span class="p">((</span><span class="n">output_features</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">-</span> <span class="n">target_features</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="n">b</span> <span class="o">*</span> <span class="n">c</span> <span class="o">*</span> <span class="n">h</span> <span class="o">*</span> <span class="n">w</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">layer_name_mapping</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="EdgeLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.EdgeLoss">[docs]</a><span class="k">class</span> <span class="nc">EdgeLoss</span><span class="p">(</span><span class="n">_Loss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">reduction</span><span class="o">=</span><span class="s2">&quot;mean&quot;</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">EdgeLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">reduction</span> <span class="o">=</span> <span class="n">reduction</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_device</span><span class="p">)</span>

<div class="viewcode-block" id="EdgeLoss.first_order"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.EdgeLoss.first_order">[docs]</a>    <span class="k">def</span> <span class="nf">first_order</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">):</span>
        <span class="n">h</span><span class="p">,</span> <span class="n">w</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">2</span><span class="p">),</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="n">w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="mi">1</span><span class="p">:,</span> <span class="p">:</span><span class="n">w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
        <span class="k">elif</span> <span class="n">axis</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">return</span> <span class="p">(</span><span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="p">:</span><span class="n">w</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">x</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:</span><span class="n">h</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">:])</span><span class="o">.</span><span class="n">abs</span><span class="p">()</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">None</span></div>

<div class="viewcode-block" id="EdgeLoss.forward"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.EdgeLoss.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="s1">&#39;loss&#39;</span><span class="p">:</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">first_order</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_order</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">+</span> <span class="n">MSELoss</span><span class="p">(</span>
            <span class="n">reduction</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">reduction</span><span class="p">)(</span><span class="bp">self</span><span class="o">.</span><span class="n">first_order</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">first_order</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="mi">3</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="TransformInvariantLoss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.TransformInvariantLoss">[docs]</a><span class="k">class</span> <span class="nc">TransformInvariantLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">loss</span><span class="p">:</span> <span class="n">_Loss</span><span class="p">,</span> <span class="n">embedded_func</span><span class="p">:</span> <span class="n">Layer</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">TransformInvariantLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">loss</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="o">=</span><span class="n">loss</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">=</span> <span class="n">MSELoss</span><span class="p">(</span><span class="n">reduction</span><span class="o">=</span><span class="s1">&#39;mean&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coverage</span> <span class="o">=</span> <span class="mi">110</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rotation_range</span> <span class="o">=</span> <span class="mi">20</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">zoom_range</span> <span class="o">=</span> <span class="mf">0.1</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shift_range</span> <span class="o">=</span> <span class="mf">0.02</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_flip</span> <span class="o">=</span> <span class="mf">0.3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embedded_func</span> <span class="o">=</span> <span class="n">embedded_func</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">_device</span><span class="p">)</span>

<div class="viewcode-block" id="TransformInvariantLoss.forward"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.TransformInvariantLoss.forward">[docs]</a>    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
        <span class="n">angle</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">rotation_range</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">rotation_range</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">180</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">scale</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">zoom_range</span><span class="p">,</span> <span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">zoom_range</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">height</span><span class="p">,</span> <span class="n">width</span> <span class="o">=</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="o">-</span><span class="mi">2</span><span class="p">:]</span>
        <span class="n">center_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([(</span><span class="n">width</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">height</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">])</span><span class="o">.</span><span class="n">expand</span><span class="p">(</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span>
            <span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">mat</span> <span class="o">=</span> <span class="n">get_rotation_matrix2d</span><span class="p">(</span><span class="n">center_tensor</span><span class="p">,</span> <span class="n">angle</span><span class="p">,</span> <span class="n">scale</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
        <span class="n">rotated_target</span> <span class="o">=</span> <span class="n">warp_affine</span><span class="p">(</span><span class="n">target</span><span class="p">,</span> <span class="n">mat</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">2</span><span class="p">:])</span><span class="o">.</span><span class="n">float</span><span class="p">()</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="n">embedded_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedded_func</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">embedded_target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedded_func</span><span class="p">(</span><span class="n">rotated_target</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">embedded_output</span><span class="p">,</span> <span class="n">embedded_target</span><span class="p">)</span></div></div>


<span class="k">class</span> <span class="nc">GPLoss</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">discriminator</span><span class="p">,</span> <span class="n">l</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">GPLoss</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span> <span class="o">=</span> <span class="n">discriminator</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">l</span> <span class="o">=</span> <span class="n">l</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">real_data</span><span class="p">,</span> <span class="n">fake_data</span><span class="p">):</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">real_data</span><span class="o">.</span><span class="n">new_empty</span><span class="p">((</span><span class="n">real_data</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">uniform_</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span><span class="o">.</span><span class="n">expand_as</span><span class="p">(</span><span class="n">real_data</span><span class="p">)</span>
        <span class="n">interpolates</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">real_data</span> <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">fake_data</span><span class="p">)</span>

        <span class="n">interpolates</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">interpolates</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">disc_interpolates</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">discriminator</span><span class="p">(</span><span class="n">interpolates</span><span class="p">)</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">autograd</span><span class="o">.</span><span class="n">grad</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">disc_interpolates</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="n">interpolates</span><span class="p">,</span>
                                        <span class="n">grad_outputs</span><span class="o">=</span><span class="n">real_data</span><span class="o">.</span><span class="n">new_ones</span><span class="p">(</span><span class="n">disc_interpolates</span><span class="o">.</span><span class="n">size</span><span class="p">()),</span> <span class="n">create_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                        <span class="n">retain_graph</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">only_inputs</span><span class="o">=</span><span class="kc">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">gradients</span> <span class="o">=</span> <span class="n">gradients</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">gradients</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">gradients_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">gradients</span> <span class="o">**</span> <span class="mi">2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-12</span><span class="p">)</span>
        <span class="n">gradient_penalty</span> <span class="o">=</span> <span class="p">((</span><span class="n">gradients_norm</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">l</span>


<span class="k">class</span> <span class="nc">AdaCos</span><span class="p">(</span><span class="n">_ClassificationLoss</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_features</span><span class="p">,</span> <span class="n">m</span><span class="o">=</span><span class="mf">0.50</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaCos</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_features</span> <span class="o">=</span> <span class="n">num_features</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">m</span> <span class="o">=</span> <span class="n">m</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">need_target_onehot</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">calculate_loss</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">flatten_check</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>
        <span class="n">shp</span> <span class="o">=</span> <span class="n">int_shape</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
        <span class="n">batch</span> <span class="o">=</span> <span class="n">shp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">=</span> <span class="n">shp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="n">num_filters</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_classes</span><span class="p">,</span> <span class="n">weights_norm</span><span class="o">=</span><span class="s1">&#39;l2&#39;</span><span class="p">,</span> <span class="n">use_bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="n">tensor_to_shape</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">need_exclude_batch_axis</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_singleton</span><span class="o">=</span><span class="kc">False</span><span class="p">))</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">weight</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_uniform_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="o">.</span><span class="n">weight</span><span class="p">)</span>

        <span class="c1"># dot product</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc</span><span class="p">(</span><span class="n">F</span><span class="o">.</span><span class="n">normalize</span><span class="p">(</span><span class="n">output</span><span class="p">))</span>
        <span class="k">if</span> <span class="n">target</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">logits</span>
        <span class="c1"># feature re-scale</span>
        <span class="n">theta</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">acos</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.0</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">,</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="mf">1e-7</span><span class="p">))</span>
        <span class="n">one_hot</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logits</span><span class="p">)</span>
        <span class="n">one_hot</span><span class="o">.</span><span class="n">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">target</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">long</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            <span class="n">B_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">one_hot</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">*</span> <span class="n">logits</span><span class="p">),</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">logits</span><span class="p">))</span>
            <span class="n">B_avg</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">B_avg</span><span class="p">)</span> <span class="o">/</span> <span class="n">output</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
            <span class="c1"># print(B_avg)</span>
            <span class="n">theta_med</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">median</span><span class="p">(</span><span class="n">theta</span><span class="p">[</span><span class="n">one_hot</span> <span class="o">==</span> <span class="mi">1</span><span class="p">])</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">B_avg</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">min</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">4</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">theta_med</span><span class="p">),</span> <span class="n">theta_med</span><span class="p">))</span>
        <span class="n">output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">s</span> <span class="o">*</span> <span class="n">logits</span>

        <span class="k">return</span> <span class="n">output</span>


<div class="viewcode-block" id="get_loss"><a class="viewcode-back" href="../../../trident.optims.html#trident.models.pytorch_ssd.get_loss">[docs]</a><span class="k">def</span> <span class="nf">get_loss</span><span class="p">(</span><span class="n">loss_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">loss_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="n">loss_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;trident.optims.pytorch_losses&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">loss_name</span> <span class="ow">in</span> <span class="n">__all__</span><span class="p">:</span>
        <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">get_class</span><span class="p">(</span><span class="n">loss_name</span><span class="p">,</span> <span class="n">loss_modules</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">get_class</span><span class="p">(</span><span class="n">camel2snake</span><span class="p">(</span><span class="n">loss_name</span><span class="p">),</span> <span class="n">loss_modules</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">loss_fn</span> <span class="o">=</span> <span class="n">get_class</span><span class="p">(</span><span class="n">loss_name</span><span class="p">,</span> <span class="n">loss_modules</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss_fn</span></div>
</pre></div>

        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
  </div>
        <footer class="mdl-mini-footer">
    <div class="mdl-mini-footer__left-section">
      <div class="mdl-logo">trident</div>
      <div>
        
        
      </div>
    </div>

    <div class="mdl-mini-footer__right-section">
        <div>&copy; Copyright 2022, AllanYiin.</div>
      <div>Generated by <a href="http://sphinx.pocoo.org/">Sphinx</a> 5.0.2 using <a href="https://github.com/myyasuda/sphinx_materialdesign_theme">sphinx_materialdesign_theme</a>.</div>
    </div>
</footer>
        </main>
    </div>
  </body>
</html>