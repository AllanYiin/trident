<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    
    <title>trident.optims.pytorch_optimizers &#8212; trident 0.7.5 documentation</title>

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/material-icons.css" />
    <link rel="stylesheet" href="../../../_static/notosanscjkjp.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/roboto.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/material-design-lite-1.3.0/material.deep_orange-indigo.min.css" type="text/css" />
    <link rel="stylesheet" href="../../../_static/sphinx_materialdesign_theme.css" type="text/css" />
    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script src="../../../_static/sphinx_materialdesign_theme.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" /> 
  </head>
<body>
    <div class="mdl-layout mdl-js-layout mdl-layout--fixed-header mdl-layout--fixed-drawer"><header class="mdl-layout__header mdl-layout__header--waterfall ">
    <div class="mdl-layout__header-row">
        
        <!-- Title -->
        <span class="mdl-layout-title">
            <a class="brand" href="../../../index.html">
                <img class="logo" src="../../../_static/trident_logo.png" alt="trident"/>
            </a>
        </span>
        
        <nav class="mdl-navigation breadcrumb">
            <a class="mdl-navigation__link" href="../../index.html">Module code</a><i class="material-icons">navigate_next</i>
            <a class="mdl-navigation__link is-active">trident.optims.pytorch_optimizers</a>
        </nav>
        <div class="mdl-layout-spacer"></div>
        <nav class="mdl-navigation">
        
<form class="form-inline pull-sm-right" action="../../../search.html" method="get">
      <div class="mdl-textfield mdl-js-textfield mdl-textfield--expandable mdl-textfield--floating-label mdl-textfield--align-right">
        <label id="quick-search-icon" class="mdl-button mdl-js-button mdl-button--icon"  for="waterfall-exp">
            <i class="material-icons">search</i>
        </label>
        <div class="mdl-textfield__expandable-holder">
          <input class="mdl-textfield__input" type="text" name="q"  id="waterfall-exp" placeholder="Search" />
          <input type="hidden" name="check_keywords" value="yes" />
          <input type="hidden" name="area" value="default" />
        </div>
      </div>
      <div class="mdl-tooltip" data-mdl-for="quick-search-icon">
      Quick search
      </div>
</form>
        
        </nav>
    </div>
    <div class="mdl-layout__header-row header-links">
      <div class="mdl-layout-spacer"></div>
      <nav class="mdl-navigation">
          <a  class="mdl-navigation__link" href="../../../index.html">
                  <i class="material-icons navigation-link-icon">home</i>
                  Home
              </a>
          
              <a  class="mdl-navigation__link" href="http://example.com">
                  <i class="material-icons navigation-link-icon">launch</i>
                  ExternalLink
              </a>
          
              <a  class="mdl-navigation__link" href="http://example.com">
                  
                  NoIconLink
              </a>
          
              <a  class="mdl-navigation__link" href="https://github.com/AllanYiin/trident">
                  <i class="material-icons navigation-link-icon">link</i>
                  GitHub
              </a>
      </nav>
    </div>
</header><header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../../../index.html">
              <img class="logo" src="../../../_static/trident_logo.png" alt="trident"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
        <!-- Local TOC -->
        <nav class="mdl-navigation"></nav>
        
        </div>
    
</header>
        <main class="mdl-layout__content" tabIndex="0">
<header class="mdl-layout__drawer">
    
          <!-- Title -->
      <span class="mdl-layout-title">
          <a class="title" href="../../../index.html">
              <img class="logo" src="../../../_static/trident_logo.png" alt="trident"/>
          </a>
      </span>
    
    
      <div class="globaltoc">
        <span class="mdl-layout-title toc">Table Of Contents</span>
        
        
        <!-- Local TOC -->
        <nav class="mdl-navigation"></nav>
        
        </div>
    
</header>

    <div class="document">
        <div class="page-content">
        
  <h1>Source code for trident.optims.pytorch_optimizers</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">absolute_import</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">division</span>
<span class="kn">from</span> <span class="nn">__future__</span> <span class="kn">import</span> <span class="n">print_function</span>
<span class="kn">import</span> <span class="nn">weakref</span>
<span class="kn">import</span> <span class="nn">math</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">defaultdict</span>

<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">lbfgs</span><span class="p">,</span> <span class="n">adagrad</span><span class="p">,</span> <span class="n">adadelta</span><span class="p">,</span> <span class="n">rmsprop</span><span class="p">,</span> <span class="n">adam</span>

<span class="kn">from</span> <span class="nn">trident.backend.common</span> <span class="kn">import</span> <span class="n">get_class</span><span class="p">,</span> <span class="n">snake2camel</span>
<span class="kn">from</span> <span class="nn">trident.backend.pytorch_ops</span> <span class="kn">import</span> <span class="o">*</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="s1">&#39;SGD&#39;</span><span class="p">,</span> <span class="s1">&#39;LBFGS&#39;</span><span class="p">,</span> <span class="s1">&#39;Adadelta&#39;</span><span class="p">,</span> <span class="s1">&#39;Adagrad&#39;</span><span class="p">,</span> <span class="s1">&#39;RMSprop&#39;</span><span class="p">,</span> <span class="s1">&#39;RAdam&#39;</span><span class="p">,</span> <span class="s1">&#39;PlainRAdam&#39;</span><span class="p">,</span> <span class="s1">&#39;AdamW&#39;</span><span class="p">,</span> <span class="s1">&#39;Lookahead&#39;</span><span class="p">,</span>
           <span class="s1">&#39;Ranger&#39;</span><span class="p">,</span> <span class="s1">&#39;Ranger21&#39;</span><span class="p">,</span> <span class="s1">&#39;RangerLars&#39;</span><span class="p">,</span> <span class="s1">&#39;AdaBelief&#39;</span><span class="p">,</span> <span class="s1">&#39;RangerAdaBelief&#39;</span><span class="p">,</span> <span class="s1">&#39;DiffGrad&#39;</span><span class="p">,</span> <span class="s1">&#39;Lamb&#39;</span><span class="p">,</span> <span class="s1">&#39;get_optimizer&#39;</span><span class="p">]</span>


<span class="k">def</span> <span class="nf">cheb_steps</span><span class="p">(</span><span class="n">m</span><span class="p">,</span> <span class="n">M</span><span class="p">,</span> <span class="n">T</span><span class="p">):</span>
    <span class="n">C</span><span class="p">,</span> <span class="n">R</span> <span class="o">=</span> <span class="p">(</span><span class="n">M</span> <span class="o">+</span> <span class="n">m</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">,</span> <span class="p">(</span><span class="n">M</span> <span class="o">-</span> <span class="n">m</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span>
    <span class="n">thetas</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">T</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span><span class="p">)</span> <span class="o">/</span> <span class="n">T</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>
    <span class="k">return</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="n">C</span> <span class="o">-</span> <span class="n">R</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">thetas</span><span class="p">))</span>


<span class="k">def</span> <span class="nf">cheb_perm</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">T</span><span class="p">:</span>
        <span class="n">perm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">perm</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">perm</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">perm</span><span class="p">])</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">perm</span>


<span class="k">def</span> <span class="nf">get_chebs</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">):</span>
    <span class="n">num_epochs</span> <span class="o">=</span> <span class="n">num_epochs</span> <span class="o">-</span> <span class="mi">2</span>
    <span class="n">steps</span> <span class="o">=</span> <span class="n">cheb_steps</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_epochs</span><span class="p">)</span>
    <span class="n">perm</span> <span class="o">=</span> <span class="n">cheb_perm</span><span class="p">(</span><span class="n">num_epochs</span><span class="p">)</span>
    <span class="n">cheb_schedule</span> <span class="o">=</span> <span class="n">steps</span><span class="p">[</span><span class="n">perm</span><span class="p">]</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;cheb schedule made with len </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">cheb_schedule</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cheb_schedule</span>


<span class="k">def</span> <span class="nf">normalize_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">use_channels</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;  use stdev to normalize gradients &quot;&quot;&quot;</span>
    <span class="n">size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="c1"># print(f&quot;size = {size}&quot;)</span>

    <span class="k">if</span> <span class="p">(</span><span class="n">size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span> <span class="ow">and</span> <span class="n">use_channels</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">)),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="o">+</span> <span class="n">epsilon</span>
        <span class="c1"># print(f&quot;s = {s}&quot;)</span>
        <span class="n">x</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>  <span class="c1"># , keepdim=True)</span>

    <span class="k">elif</span> <span class="n">torch</span><span class="o">.</span><span class="n">numel</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="n">s</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">+</span> <span class="n">epsilon</span>
        <span class="n">x</span><span class="o">.</span><span class="n">div_</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>  <span class="c1"># , keepdim=True)</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">def</span> <span class="nf">centralize_gradient</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">gc_conv_only</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;credit - https://github.com/Yonghongwei/Gradient-Centralization &quot;&quot;&quot;</span>

    <span class="n">size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span>
    <span class="c1"># print(f&quot;size = {size}&quot;)</span>

    <span class="k">if</span> <span class="n">gc_conv_only</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
            <span class="n">x</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">)),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">size</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">x</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">size</span><span class="p">)),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">x</span>


<span class="k">class</span> <span class="nc">Optimizer</span><span class="p">(</span><span class="n">optimizer</span><span class="o">.</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for all optimizers.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Parameters need to be specified as collections that have a deterministic</span>
<span class="sd">        ordering that is consistent between runs. Examples of objects that don&#39;t</span>
<span class="sd">        satisfy those properties are sets and iterators over values of dictionaries.</span>

<span class="sd">    Args:</span>
<span class="sd">        params (iterable): an iterable of :class:`tf.Variable` s or</span>
<span class="sd">            :class:`dict` s. Specifies what Tensors should be optimized.</span>
<span class="sd">        defaults: (dict): a dict containing default values of optimization</span>
<span class="sd">            options (used when a parameter group doesn&#39;t specify them).</span>


<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="mf">1e-3</span>
        <span class="k">if</span> <span class="s1">&#39;lr&#39;</span> <span class="ow">in</span> <span class="n">defaults</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="n">defaults</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">def</span> <span class="nf">adjust_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            new_lr (float):  new learning rate value</span>
<span class="sd">            verbose (bool): if True, will print the learning rate change information.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">old_lr</span> <span class="o">!=</span> <span class="n">new_lr</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Returns: the weights need to train</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">))]</span>

    <span class="nd">@parameters</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Returns: the weights need to train</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">TypeError</span><span class="p">(</span><span class="s2">&quot;params argument given to the optimizer should be &quot;</span>
                            <span class="s2">&quot;an iterable of Tensors or dicts, but got &quot;</span> <span class="o">+</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">typename</span><span class="p">(</span><span class="n">value</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s1">&#39;param_groups&#39;</span><span class="p">)</span> <span class="ow">or</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="n">param_groups</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">value</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">param_groups</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;optimizer got an empty parameter list&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
                <span class="n">param_groups</span> <span class="o">=</span> <span class="p">[{</span><span class="s1">&#39;params&#39;</span><span class="p">:</span> <span class="n">param_groups</span><span class="p">}]</span>
                <span class="k">for</span> <span class="n">param_group</span> <span class="ow">in</span> <span class="n">param_groups</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">add_param_group</span><span class="p">(</span><span class="n">param_group</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;learning rate&#39; property.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

    <span class="nd">@lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">!=</span> <span class="n">value</span><span class="p">:</span>
            <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
            <span class="n">new_lr</span> <span class="o">=</span> <span class="n">value</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;base learning rate&#39; property (mean the starting learning rate ,</span>
<span class="sd">        excluding warmup ).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span>

    <span class="nd">@base_lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="n">value</span>


<div class="viewcode-block" id="Adam"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Adam">[docs]</a><span class="k">class</span> <span class="nc">Adam</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implements Adam algorithm.</span>

<span class="sd">    It has been proposed in `Adam: A Method for Stochastic Optimization`_.</span>

<span class="sd">    Args:</span>
<span class="sd">        params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="sd">            parameter groups</span>
<span class="sd">        lr (float, optional): learning rate (default: 1e-3)</span>
<span class="sd">        betas (Tuple[float, float], optional): coefficients used for computing</span>
<span class="sd">            running averages of gradient and its square (default: (0.9, 0.999))</span>
<span class="sd">        eps (float, optional): term added to the denominator to improve</span>
<span class="sd">            numerical stability (default: 1e-8)</span>
<span class="sd">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>
<span class="sd">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span>
<span class="sd">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span>
<span class="sd">            (default: False)</span>

<span class="sd">    References</span>
<span class="sd">        .. _Adam\: A Method for Stochastic Optimization:</span>
<span class="sd">            https://arxiv.org/abs/1412.6980</span>
<span class="sd">        .. _On the Convergence of Adam and Beyond:</span>
<span class="sd">            https://openreview.net/forum?id=ryQu7f-RZ</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.95</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid learning rate: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid epsilon value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 0: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 1: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">weight_decay</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid weight_decay value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">weight_decay</span><span class="p">))</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="n">amsgrad</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">Adam</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Adam</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">group</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">&#39;amsgrad&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<div class="viewcode-block" id="Adam.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Adam.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            closure (callable, optional): A closure that reevaluates the model</span>
<span class="sd">                and returns the loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">params_with_grad</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">grads</span> <span class="o">=</span> <span class="p">[]</span>

            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Adam does not support sparse gradients, please consider SparseAdam instead&#39;</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">any_abnormal_number</span><span class="p">(</span><span class="n">grad</span><span class="p">):</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">is_abnormal_number</span><span class="p">(</span><span class="n">grad</span><span class="p">),</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">grad</span><span class="p">),</span> <span class="n">grad</span><span class="p">)</span>
                <span class="n">amsgrad</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;amsgrad&#39;</span><span class="p">]</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># State initialization</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="c1"># Exponential moving average of gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>
                    <span class="c1"># Exponential moving average of squared gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>
                    <span class="c1"># if amsgrad:</span>
                    <span class="c1"># Maintains max of all exp. moving avg. of sq. grad. values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;max_exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>

                <span class="n">max_exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;max_exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gcc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="n">grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">())))),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

                <span class="c1"># Decay the first and second moment running average coefficient</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>

                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                    <span class="c1"># Maintains the maximum of all 2nd moment running avg. till now</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_exp_avg_sq</span><span class="p">,</span> <span class="n">exp_avg_sq</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">max_exp_avg_sq</span><span class="p">)</span>
                    <span class="c1"># Use the max. for normalizing running avg. of gradient</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>

                <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="n">bias_correction1</span>

                <span class="n">G_grad</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="n">denom</span>
                <span class="c1"># if self.gradient_centralization in [&#39;all&#39;, &#39;gc&#39;]:</span>
                <span class="c1">#     if ndim(G_grad) &gt; 1:</span>
                <span class="c1">#         G_grad.add_(-G_grad.mean(axis=list(range(1, ndim(G_grad))), keepdims=True))</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">G_grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="SGD"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.SGD">[docs]</a><span class="k">class</span> <span class="nc">SGD</span><span class="p">(</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements stochastic gradient descent (optionally with momentum).</span>

<span class="sd">    Nesterov momentum is based on the formula from</span>
<span class="sd">    `On the importance of initialization and momentum in deep learning`__.</span>

<span class="sd">    Args:</span>
<span class="sd">        params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="sd">            parameter groups</span>
<span class="sd">        lr (float): learning rate</span>
<span class="sd">        momentum (float, optional): momentum factor (default: 0)</span>
<span class="sd">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>
<span class="sd">        dampening (float, optional): dampening for momentum (default: 0)</span>
<span class="sd">        nesterov (bool, optional): enables Nesterov momentum (default: False)</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; SGD(lr=0.1, momentum=0.9)</span>


<span class="sd">    __ http://www.cs.toronto.edu/%7Ehinton/absps/momentum.pdf</span>

<span class="sd">    .. note::</span>
<span class="sd">        The implementation of SGD with Momentum/Nesterov subtly differs from</span>
<span class="sd">        Sutskever et. al. and implementations in some other frameworks.</span>

<span class="sd">        Considering the specific case of Momentum, the update can be written as</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{aligned}</span>
<span class="sd">                v_{t+1} &amp; = \mu * v_{t} + g_{t+1}, \\</span>
<span class="sd">                p_{t+1} &amp; = p_{t} - \text{lr} * v_{t+1},</span>
<span class="sd">            \end{aligned}</span>

<span class="sd">        where :math:`p`, :math:`g`, :math:`v` and :math:`\mu` denote the</span>
<span class="sd">        parameters, gradient, velocity, and momentum respectively.</span>

<span class="sd">        This is in contrast to Sutskever et. al. and</span>
<span class="sd">        other frameworks which employ an update of the form</span>

<span class="sd">        .. math::</span>
<span class="sd">            \begin{aligned}</span>
<span class="sd">                v_{t+1} &amp; = \mu * v_{t} + \text{lr} * g_{t+1}, \\</span>
<span class="sd">                p_{t+1} &amp; = p_{t} - v_{t+1}.</span>
<span class="sd">            \end{aligned}</span>

<span class="sd">        The Nesterov version is analogously modified.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">dampening</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">dampening</span><span class="o">=</span><span class="n">dampening</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
                         <span class="n">nesterov</span><span class="o">=</span><span class="n">nesterov</span><span class="p">)</span>

<div class="viewcode-block" id="SGD.adjust_learning_rate"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.SGD.adjust_learning_rate">[docs]</a>    <span class="k">def</span> <span class="nf">adjust_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            new_lr (float):  new learning rate value</span>
<span class="sd">            verbose (bool): if True, will print the learning rate change information.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">old_lr</span> <span class="o">!=</span> <span class="n">new_lr</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Returns: the weights need to train</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;learning rate&#39; property.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

    <span class="nd">@lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">!=</span> <span class="n">value</span><span class="p">:</span>
            <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
            <span class="n">new_lr</span> <span class="o">=</span> <span class="n">value</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;base learning rate&#39; property (mean the starting learning rate ,</span>
<span class="sd">        excluding warmup ).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span>

    <span class="nd">@base_lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="n">value</span></div>


<div class="viewcode-block" id="LBFGS"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.LBFGS">[docs]</a><span class="k">class</span> <span class="nc">LBFGS</span><span class="p">(</span><span class="n">lbfgs</span><span class="o">.</span><span class="n">LBFGS</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implements L-BFGS algorithm, heavily inspired by `minFunc</span>
<span class="sd">    &lt;https://www.cs.ubc.ca/~schmidtm/Software/minFunc.html&gt;`.</span>

<span class="sd">    .. warning::</span>
<span class="sd">        This optimizer doesn&#39;t support per-parameter options and parameter</span>
<span class="sd">        groups (there can be only one).</span>

<span class="sd">    .. warning::</span>
<span class="sd">        Right now all parameters have to be on a single device. This will be</span>
<span class="sd">        improved in the future.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This is a very memory intensive optimizer (it requires additional</span>
<span class="sd">        ``param_bytes * (history_size + 1)`` bytes). If it doesn&#39;t fit in memory</span>
<span class="sd">        try reducing the history size, or use a different algorithm.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        lr (float): learning rate (default: 1)</span>
<span class="sd">        max_iter (int): maximal number of iterations per optimization step</span>
<span class="sd">            (default: 20)</span>
<span class="sd">        max_eval (int): maximal number of function evaluations per optimization</span>
<span class="sd">            step (default: max_iter * 1.25).</span>
<span class="sd">        tolerance_grad (float): termination tolerance on first order optimality</span>
<span class="sd">            (default: 1e-5).</span>
<span class="sd">        tolerance_change (float): termination tolerance on function</span>
<span class="sd">            value/parameter changes (default: 1e-9).</span>
<span class="sd">        history_size (int): update history size (default: 100).</span>
<span class="sd">        line_search_fn (str): either &#39;strong_wolfe&#39; or None (default: None).</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span>
                 <span class="n">lr</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
                 <span class="n">max_iter</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
                 <span class="n">max_eval</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">tolerance_grad</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span>
                 <span class="n">tolerance_change</span><span class="o">=</span><span class="mf">1e-9</span><span class="p">,</span>
                 <span class="n">history_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                 <span class="n">line_search_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">max_iter</span><span class="o">=</span><span class="n">max_iter</span><span class="p">,</span> <span class="n">max_eval</span><span class="o">=</span><span class="n">max_eval</span><span class="p">,</span> <span class="n">tolerance_grad</span><span class="o">=</span><span class="n">tolerance_grad</span><span class="p">,</span>
                         <span class="n">tolerance_change</span><span class="o">=</span><span class="n">tolerance_change</span><span class="p">,</span> <span class="n">history_size</span><span class="o">=</span><span class="n">history_size</span><span class="p">,</span>
                         <span class="n">line_search_fn</span><span class="o">=</span><span class="n">line_search_fn</span><span class="p">)</span>

<div class="viewcode-block" id="LBFGS.adjust_learning_rate"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.LBFGS.adjust_learning_rate">[docs]</a>    <span class="k">def</span> <span class="nf">adjust_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            new_lr (float):  new learning rate value</span>
<span class="sd">            verbose (bool): if True, will print the learning rate change information.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">old_lr</span> <span class="o">!=</span> <span class="n">new_lr</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Returns: the weights need to train</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;learning rate&#39; property.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

    <span class="nd">@lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">!=</span> <span class="n">value</span><span class="p">:</span>
            <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
            <span class="n">new_lr</span> <span class="o">=</span> <span class="n">value</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;base learning rate&#39; property (mean the starting learning rate ,</span>
<span class="sd">        excluding warmup ).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span>

    <span class="nd">@base_lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="n">value</span></div>


<div class="viewcode-block" id="Adadelta"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Adadelta">[docs]</a><span class="k">class</span> <span class="nc">Adadelta</span><span class="p">(</span><span class="n">adadelta</span><span class="o">.</span><span class="n">Adadelta</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implements Adadelta algorithm.</span>

<span class="sd">    It has been proposed in `ADADELTA: An Adaptive Learning Rate Method`__.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="sd">            parameter groups</span>
<span class="sd">        rho (float, optional): coefficient used for computing a running average</span>
<span class="sd">            of squared gradients (default: 0.9)</span>
<span class="sd">        eps (float, optional): term added to the denominator to improve</span>
<span class="sd">            numerical stability (default: 1e-6)</span>
<span class="sd">        lr (float, optional): coefficient that scale delta before it is applied</span>
<span class="sd">            to the parameters (default: 1.0)</span>
<span class="sd">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>

<span class="sd">    __ https://arxiv.org/abs/1212.5701</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">rho</span><span class="o">=</span><span class="n">rho</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>

<div class="viewcode-block" id="Adadelta.adjust_learning_rate"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Adadelta.adjust_learning_rate">[docs]</a>    <span class="k">def</span> <span class="nf">adjust_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            new_lr (float):  new learning rate value</span>
<span class="sd">            verbose (bool): if True, will print the learning rate change information.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">old_lr</span> <span class="o">!=</span> <span class="n">new_lr</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Returns: the weights need to train</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;learning rate&#39; property.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

    <span class="nd">@lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">!=</span> <span class="n">value</span><span class="p">:</span>
            <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
            <span class="n">new_lr</span> <span class="o">=</span> <span class="n">value</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;base learning rate&#39; property (mean the starting learning rate ,</span>
<span class="sd">        excluding warmup ).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span>

    <span class="nd">@base_lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="n">value</span></div>


<div class="viewcode-block" id="Adagrad"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Adagrad">[docs]</a><span class="k">class</span> <span class="nc">Adagrad</span><span class="p">(</span><span class="n">adagrad</span><span class="o">.</span><span class="n">Adagrad</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implements Adagrad algorithm.</span>

<span class="sd">     It has been proposed in `Adaptive Subgradient Methods for Online Learning</span>
<span class="sd">     and Stochastic Optimization`_.</span>

<span class="sd">     Arguments:</span>
<span class="sd">         params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="sd">             parameter groups</span>
<span class="sd">         lr (float, optional): learning rate (default: 1e-2)</span>
<span class="sd">         lr_decay (float, optional): learning rate decay (default: 0)</span>
<span class="sd">         weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>
<span class="sd">         eps (float, optional): term added to the denominator to improve</span>
<span class="sd">             numerical stability (default: 1e-10)</span>

<span class="sd">     .. _Adaptive Subgradient Methods for Online Learning and Stochastic</span>
<span class="sd">         Optimization: http://jmlr.org/papers/v12/duchi11a.html</span>
<span class="sd">     &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">lr_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">initial_accumulator_value</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">lr_decay</span><span class="o">=</span><span class="n">lr_decay</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
                         <span class="n">initial_accumulator_value</span><span class="o">=</span><span class="n">initial_accumulator_value</span><span class="p">)</span>

<div class="viewcode-block" id="Adagrad.adjust_learning_rate"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Adagrad.adjust_learning_rate">[docs]</a>    <span class="k">def</span> <span class="nf">adjust_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            new_lr (float):  new learning rate value</span>
<span class="sd">            verbose (bool): if True, will print the learning rate change information.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">old_lr</span> <span class="o">!=</span> <span class="n">new_lr</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Returns: the weights need to train</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;learning rate&#39; property.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

    <span class="nd">@lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">!=</span> <span class="n">value</span><span class="p">:</span>
            <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
            <span class="n">new_lr</span> <span class="o">=</span> <span class="n">value</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;base learning rate&#39; property (mean the starting learning rate ,</span>
<span class="sd">        excluding warmup ).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span>

    <span class="nd">@base_lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="n">value</span></div>


<div class="viewcode-block" id="RMSprop"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.RMSprop">[docs]</a><span class="k">class</span> <span class="nc">RMSprop</span><span class="p">(</span><span class="n">rmsprop</span><span class="o">.</span><span class="n">RMSprop</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements RMSprop algorithm.</span>

<span class="sd">    Proposed by G. Hinton in his</span>
<span class="sd">    `course &lt;http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf&gt;`_.</span>

<span class="sd">    The centered version first appears in `Generating Sequences</span>
<span class="sd">    With Recurrent Neural Networks &lt;https://arxiv.org/pdf/1308.0850v5.pdf&gt;`_.</span>

<span class="sd">    The implementation here takes the square root of the gradient average before</span>
<span class="sd">    adding epsilon (note that TensorFlow interchanges these two operations). The effective</span>
<span class="sd">    learning rate is thus :math:`\alpha/(\sqrt{v} + \epsilon)` where :math:`\alpha`</span>
<span class="sd">    is the scheduled learning rate and :math:`v` is the weighted moving average</span>
<span class="sd">    of the squared gradient.</span>

<span class="sd">    Args:</span>
<span class="sd">        params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="sd">            parameter groups</span>
<span class="sd">        lr (float, optional): learning rate (default: 1e-2)</span>
<span class="sd">        momentum (float, optional): momentum factor (default: 0)</span>
<span class="sd">        alpha (float, optional): smoothing constant (default: 0.99)</span>
<span class="sd">        eps (float, optional): term added to the denominator to improve</span>
<span class="sd">            numerical stability (default: 1e-8)</span>
<span class="sd">        centered (bool, optional) : if ``True``, compute the centered RMSProp,</span>
<span class="sd">            the gradient is normalized by an estimation of its variance</span>
<span class="sd">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.99</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">centered</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
                         <span class="n">centered</span><span class="o">=</span><span class="n">centered</span><span class="p">)</span>

<div class="viewcode-block" id="RMSprop.adjust_learning_rate"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.RMSprop.adjust_learning_rate">[docs]</a>    <span class="k">def</span> <span class="nf">adjust_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Args:</span>
<span class="sd">            new_lr (float):  new learning rate value</span>
<span class="sd">            verbose (bool): if True, will print the learning rate change information.</span>

<span class="sd">        &quot;&quot;&quot;</span>

        <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">old_lr</span> <span class="o">!=</span> <span class="n">new_lr</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span></div>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">parameters</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>

<span class="sd">        Returns: the weights need to train</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;params&#39;</span><span class="p">]</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;learning rate&#39; property.&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>

    <span class="nd">@lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">:</span> <span class="nb">float</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">!=</span> <span class="n">value</span><span class="p">:</span>
            <span class="n">old_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr</span>
            <span class="n">new_lr</span> <span class="o">=</span> <span class="n">value</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">[</span><span class="mi">0</span><span class="p">][</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;learning rate changed! ( form </span><span class="si">{0:.3e}</span><span class="s1"> to </span><span class="si">{1:.3e}</span><span class="s1">)&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">old_lr</span><span class="p">,</span> <span class="n">new_lr</span><span class="p">))</span>

    <span class="nd">@property</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;str: The getter method of the &#39;base learning rate&#39; property (mean the starting learning rate ,</span>
<span class="sd">        excluding warmup ).&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span>

    <span class="nd">@base_lr</span><span class="o">.</span><span class="n">setter</span>
    <span class="k">def</span> <span class="nf">base_lr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">value</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="n">value</span></div>


<div class="viewcode-block" id="RAdam"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.RAdam">[docs]</a><span class="k">class</span> <span class="nc">RAdam</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Variant of the Adam optimizer whose adaptive learning rate is rectified</span>
<span class="sd">        so as to have a consistent variance.</span>
<span class="sd">        It implements the Rectified Adam (a.k.a. RAdam) proposed by</span>
<span class="sd">        Liyuan Liu et al. in [On The Variance Of The Adaptive Learning Rate</span>
<span class="sd">        And Beyond](https://arxiv.org/pdf/1908.03265v1.pdf).</span>

<span class="sd">        Example of usage:</span>
<span class="sd">        ```python</span>
<span class="sd">        opt = tfa.optimizers.RectifiedAdam(lr=1e-3)</span>
<span class="sd">        ```</span>

<span class="sd">        Note: `amsgrad` is not described in the original paper. Use it with</span>
<span class="sd">              caution.</span>
<span class="sd">        RAdam is not a placement of the heuristic warmup, the settings should be</span>
<span class="sd">        kept if warmup has already been employed and tuned in the baseline method.</span>
<span class="sd">        You can enable warmup by setting `total_steps` and `warmup_proportion`:</span>
<span class="sd">        ```python</span>
<span class="sd">        opt = RAdam(lr=1e-3, betas=(0.9,0.999))</span>

<span class="sd">        ```</span>
<span class="sd">        In the above example, the learning rate will increase linearly</span>
<span class="sd">        from 0 to `lr` in 1000 steps, then decrease linearly from `lr` to `min_lr`</span>
<span class="sd">        in 9000 steps.</span>
<span class="sd">        Lookahead, proposed by Michael R. Zhang et.al in the paper</span>
<span class="sd">        [Lookahead Optimizer: k steps forward, 1 step back]</span>
<span class="sd">        (https://arxiv.org/abs/1907.08610v1), can be integrated with RAdam,</span>
<span class="sd">        which is announced by Less Wright and the new combined optimizer can also</span>
<span class="sd">        be called &quot;Ranger&quot;. The mechanism can be enabled by using the lookahead</span>
<span class="sd">        wrapper. For example:</span>

<span class="sd">        ```python</span>

<span class="sd">        radam =RAdam()</span>
<span class="sd">        ranger = Lookahead(radam)</span>

<span class="sd">        ```</span>
<span class="sd">        &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">N_sma_threshhold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">degenerated_to_sgd</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Construct a new RAdam optimizer.</span>
<span class="sd">        Args:</span>
<span class="sd">            params: trainable parameters from model</span>

<span class="sd">            lr (float): The learning rate.</span>
<span class="sd">            betas:  beta1 means the exponential decay rate for the 1st moment estimates.</span>
<span class="sd">                beta_2 means he exponential decay rate for the 2nd moment estimates.</span>
<span class="sd">            eps: A small constant for numerical stability.</span>
<span class="sd">            weight_decay: A floating point value. Weight decay for each param.</span>

<span class="sd">            N_sma_threshhold. A float value.</span>
<span class="sd">                The threshold for simple mean average.</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid learning rate: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid epsilon value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 0: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 1: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">degenerated_to_sgd</span> <span class="o">=</span> <span class="n">degenerated_to_sgd</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span> <span class="o">=</span> <span class="n">N_sma_threshhold</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
                <span class="k">if</span> <span class="s1">&#39;betas&#39;</span> <span class="ow">in</span> <span class="n">param</span> <span class="ow">and</span> <span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="n">param</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;buffer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
                        <span class="n">buffer</span><span class="o">=</span><span class="p">[[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RAdam</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RAdam</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="RAdam.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.RAdam.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Args:</span>
<span class="sd">            closure (callable): call for get loss backward</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;RAdam does not support sparse gradients&#39;</span><span class="p">)</span>

                <span class="n">p_data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">buffered</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;buffer&#39;</span><span class="p">][</span><span class="nb">int</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="mi">10</span><span class="p">)]</span>
                <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">N_sma</span><span class="p">,</span> <span class="n">step_size</span> <span class="o">=</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                    <span class="n">beta2_t</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                    <span class="n">N_sma_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="n">N_sma</span> <span class="o">=</span> <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">beta2_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">N_sma</span>

                    <span class="c1"># more conservative since it&#39;s an approximated value</span>
                    <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span><span class="p">:</span>
                        <span class="n">step_size</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                            <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">N_sma</span> <span class="o">*</span> <span class="n">N_sma_max</span> <span class="o">/</span> <span class="p">(</span>
                                    <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">degenerated_to_sgd</span><span class="p">:</span>
                        <span class="n">step_size</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">step_size</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_size</span>

                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">p_data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
                <span class="c1"># more conservative since it&#39;s an approximated value</span>
                <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span><span class="p">:</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
                    <span class="n">p_data</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>

                <span class="k">elif</span> <span class="n">step_size</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">p_data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">exp_avg</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="PlainRAdam"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.PlainRAdam">[docs]</a><span class="k">class</span> <span class="nc">PlainRAdam</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">degenerated_to_sgd</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid learning rate: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid epsilon value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 0: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 1: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">degenerated_to_sgd</span> <span class="o">=</span> <span class="n">degenerated_to_sgd</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">PlainRAdam</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">PlainRAdam</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="PlainRAdam.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.PlainRAdam.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Args:</span>
<span class="sd">            closure (callable): call for get loss backward</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>

            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;RAdam does not support sparse gradients&#39;</span><span class="p">)</span>

                <span class="n">p_data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">beta2_t</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                <span class="n">N_sma_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                <span class="n">N_sma</span> <span class="o">=</span> <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">beta2_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span>

                <span class="c1"># more conservative since it&#39;s an approximated value</span>
                <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">p_data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
                    <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                        <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">N_sma</span> <span class="o">*</span> <span class="n">N_sma_max</span> <span class="o">/</span> <span class="p">(</span>
                                <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
                    <span class="n">p_data</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">degenerated_to_sgd</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">p_data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
                    <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="n">p_data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="AdamW"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.AdamW">[docs]</a><span class="k">class</span> <span class="nc">AdamW</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Optimizer that implements the Adam algorithm with weight decay.</span>

<span class="sd">    This is an implementation of the AdamW optimizer described in &quot;Decoupled</span>
<span class="sd">    Weight Decay Regularization&quot; by Loshch ilov &amp; Hutter</span>
<span class="sd">    (https://arxiv.org/abs/1711.05101)</span>
<span class="sd">    ([pdf])(https://arxiv.org/pdf/1711.05101.pdf).</span>

<span class="sd">    It computes the update step of `tf.keras.optimizers.Adam` and additionally</span>
<span class="sd">    decays the variable. Note that this is different from adding L2</span>
<span class="sd">    regularization on the variables to the loss: it regularizes variables with</span>
<span class="sd">    large gradients more than L2 regularization would, which was shown to yield</span>
<span class="sd">    better training loss and generalization error in the paper above.</span>
<span class="sd">    For further information see the documentation of the Adam Optimizer.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; AdamW(lr=0.001, betas=(0.9, 0.999))</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                 <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid learning rate: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid epsilon value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 0: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 1: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">warmup</span><span class="o">=</span><span class="n">warmup</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdamW</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdamW</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="AdamW.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.AdamW.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Args:</span>
<span class="sd">            closure (callable): call for get loss backward</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>

            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Adam does not support sparse gradients, please consider SparseAdam instead&#39;</span><span class="p">)</span>

                <span class="n">p_data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gcc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="n">grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">())))),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>

                <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                <span class="n">scheduled_lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>
                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;warmup&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+</span> <span class="mi">20</span><span class="p">:</span>
                    <span class="n">scheduled_lr</span> <span class="o">=</span> <span class="mf">1e-8</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">scheduled_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="c1"># scheduled_lr = group[&#39;lr&#39;]</span>
                <span class="k">elif</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;warmup&#39;</span><span class="p">]</span> <span class="o">&gt;</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]:</span>
                    <span class="n">scheduled_lr</span> <span class="o">=</span> <span class="mf">1e-8</span> <span class="o">*</span> <span class="nb">pow</span><span class="p">(</span><span class="nb">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_lr</span> <span class="o">/</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">20</span><span class="p">),</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;warmup&#39;</span><span class="p">]</span> <span class="o">-</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]))</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">scheduled_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">elif</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;warmup&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">adjust_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                    <span class="n">scheduled_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lr</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">pass</span>  <span class="c1"># scheduled_lr =  group[&#39;lr&#39;]</span>

                <span class="n">step_size</span> <span class="o">=</span> <span class="n">scheduled_lr</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)</span> <span class="o">/</span> <span class="n">bias_correction1</span>

                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">p_data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">scheduled_lr</span><span class="p">)</span>

                <span class="n">G_grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">/</span> <span class="n">denom</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">G_grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">G_grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">G_grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ndim</span><span class="p">(</span><span class="n">G_grad</span><span class="p">))),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

                <span class="n">p_data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">G_grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="Lookahead"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Lookahead">[docs]</a><span class="k">class</span> <span class="nc">Lookahead</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">param_groups</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">state</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fast_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;counter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>

<div class="viewcode-block" id="Lookahead.update"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Lookahead.update">[docs]</a>    <span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">group</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">fast</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
            <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">fast</span><span class="p">]</span>
            <span class="k">if</span> <span class="s2">&quot;slow_param&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_state</span><span class="p">:</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;slow_param&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">fast</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;slow_param&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">fast</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
            <span class="n">slow</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;slow_param&quot;</span><span class="p">]</span>
            <span class="n">slow</span> <span class="o">+=</span> <span class="p">(</span><span class="n">fast</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">slow</span><span class="p">)</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span>
            <span class="n">fast</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">slow</span><span class="p">)</span></div>

<div class="viewcode-block" id="Lookahead.update_lookahead"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Lookahead.update_lookahead">[docs]</a>    <span class="k">def</span> <span class="nf">update_lookahead</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">group</span><span class="p">)</span></div>

<div class="viewcode-block" id="Lookahead.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Lookahead.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Args:</span>
<span class="sd">            closure (callable): call for get loss backward</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;counter&quot;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">group</span><span class="p">)</span>
            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;counter&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
            <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;counter&quot;</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">:</span>
                <span class="n">group</span><span class="p">[</span><span class="s2">&quot;counter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">return</span> <span class="n">loss</span></div>

<div class="viewcode-block" id="Lookahead.state_dict"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Lookahead.state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">fast_state_dict</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state_dict</span><span class="p">()</span>
        <span class="n">slow_state</span> <span class="o">=</span> <span class="p">{(</span><span class="nb">id</span><span class="p">(</span><span class="n">k</span><span class="p">)</span> <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="k">else</span> <span class="n">k</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="n">fast_state</span> <span class="o">=</span> <span class="n">fast_state_dict</span><span class="p">[</span><span class="s2">&quot;state&quot;</span><span class="p">]</span>
        <span class="n">param_groups</span> <span class="o">=</span> <span class="n">fast_state_dict</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">]</span>
        <span class="k">return</span> <span class="p">{</span><span class="s2">&quot;fast_state&quot;</span><span class="p">:</span> <span class="n">fast_state</span><span class="p">,</span> <span class="s2">&quot;slow_state&quot;</span><span class="p">:</span> <span class="n">slow_state</span><span class="p">,</span> <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="n">param_groups</span><span class="p">,</span> <span class="p">}</span></div>

<div class="viewcode-block" id="Lookahead.load_state_dict"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Lookahead.load_state_dict">[docs]</a>    <span class="k">def</span> <span class="nf">load_state_dict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state_dict</span><span class="p">):</span>
        <span class="n">slow_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;state&quot;</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;slow_state&quot;</span><span class="p">],</span> <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">],</span> <span class="p">}</span>
        <span class="n">fast_state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;state&quot;</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;fast_state&quot;</span><span class="p">],</span> <span class="s2">&quot;param_groups&quot;</span><span class="p">:</span> <span class="n">state_dict</span><span class="p">[</span><span class="s2">&quot;param_groups&quot;</span><span class="p">],</span> <span class="p">}</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Lookahead</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">slow_state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">fast_state_dict</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fast_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">state</span></div>

<div class="viewcode-block" id="Lookahead.add_param_group"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Lookahead.add_param_group">[docs]</a>    <span class="k">def</span> <span class="nf">add_param_group</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_group</span><span class="p">):</span>
        <span class="n">param_group</span><span class="p">[</span><span class="s2">&quot;counter&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="o">.</span><span class="n">add_param_group</span><span class="p">(</span><span class="n">param_group</span><span class="p">)</span></div></div>


<div class="viewcode-block" id="Ranger"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Ranger">[docs]</a><span class="k">class</span> <span class="nc">Ranger</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">N_sma_threshhold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">.95</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span>
                 <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>
        <span class="c1"># parameter checks</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid slow update rate: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">k</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid lookahead steps: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">lr</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid Learning Rate: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">eps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid eps: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>

        <span class="c1"># parameter comments:</span>
        <span class="c1"># beta1 (momentum) of .95 seems to work better than .90...</span>
        <span class="c1"># N_sma_threshold of 5 seems better in testing than 4.</span>
        <span class="c1"># In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to</span>
        <span class="c1"># make sure which works best for you.</span>

        <span class="c1"># prep defaults and init torch.optim base</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">step_counter</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
                        <span class="n">N_sma_threshhold</span><span class="o">=</span><span class="n">N_sma_threshhold</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">gradient_centralization</span><span class="o">=</span><span class="n">gradient_centralization</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="c1"># adjustable threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span> <span class="o">=</span> <span class="n">N_sma_threshhold</span>

        <span class="c1"># now we can get to work...</span>
        <span class="c1"># removed as we now use step from RAdam...no need for</span>
        <span class="c1"># duplicate step counting</span>
        <span class="c1"># for group in self.param_groups:</span>
        <span class="c1">#    group[&quot;step_counter&quot;] = 0</span>
        <span class="c1"># print(&quot;group step counter init&quot;)</span>

        <span class="c1"># look ahead params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

        <span class="c1"># radam buffer for state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">radam_buffer</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>

        <span class="c1"># self.first_run_check=0</span>

        <span class="c1"># lookahead weights</span>
        <span class="c1"># 9/2/19 - lookahead param tensors have been moved to state storage.</span>
        <span class="c1"># This should resolve issues with load/save where weights were left in</span>
        <span class="c1"># GPU memory from first load, slowing down future runs.</span>

        <span class="c1"># self.slow_weights = [[p.clone().detach() for p in group[&#39;params&#39;]]</span>
        <span class="c1">#                     for group in self.param_groups]</span>

        <span class="c1"># don&#39;t use grad for lookahead weights</span>
        <span class="c1"># for w in it.chain(*self.slow_weights):</span>
        <span class="c1">#    w.requires_grad = False</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Ranger</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="Ranger.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Ranger.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Args:</span>
<span class="sd">            closure (callable): call for get loss backward</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="c1"># cast data type</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>

                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Ranger optimizer does not support sparse gradients&#39;</span><span class="p">)</span>

                <span class="c1"># p_data = p.data</span>
                <span class="n">p_data_fp32</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>  <span class="c1"># get state dict for this param</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># if first time to run...init dictionary with our desired entries</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>

                    <span class="c1"># look ahead weight storage now in state dict</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>

                <span class="c1"># begin computations</span>
                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                <span class="c1"># if self.gradient_centralization in [&#39;all&#39;, &#39;gcc&#39;]:</span>
                <span class="c1">#     if ndim(grad) &gt; 3:</span>
                <span class="c1">#         grad.add_(-grad.mean(axis=tuple(range(1, ndim(grad) )), keepdims=True))</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mf">1.0</span>

                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>

                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

                <span class="n">buffered</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">radam_buffer</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="mi">10</span><span class="p">)]</span>

                <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">N_sma</span><span class="p">,</span> <span class="n">step_size</span> <span class="o">=</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                    <span class="n">beta2_t</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                    <span class="n">N_sma_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="n">N_sma</span> <span class="o">=</span> <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">beta2_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">N_sma</span>
                    <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span><span class="p">:</span>
                        <span class="n">step_size</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                                <span class="n">N_sma</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">N_sma</span> <span class="o">*</span> <span class="n">N_sma_max</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">step_size</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_size</span>

                <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span><span class="p">:</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
                    <span class="n">G_grad</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="n">denom</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">G_grad</span> <span class="o">=</span> <span class="n">exp_avg</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gc&#39;</span><span class="p">,</span> <span class="s1">&#39;gcc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">G_grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">G_grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">G_grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ndim</span><span class="p">(</span><span class="n">G_grad</span><span class="p">))),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

                <span class="n">p_data_fp32</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">G_grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">any_abnormal_number</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">):</span>
                    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
                        <span class="s1">&#39;</span><span class="si">{0}</span><span class="s1"> p_data has abnormal value,trident automatically replace these abnormal value to zero.</span><span class="se">\n\r</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
                    <span class="n">p_data_fp32</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">is_abnormal_number</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">p_data_fp32</span><span class="p">)</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;k&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># get access to slow param tensor</span>
                    <span class="n">slow_p</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span>
                    <span class="c1"># (fast weights - slow weights) * alpha</span>
                    <span class="n">slow_p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">slow_p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
                    <span class="c1"># copy interpolated weights to RAdam param tensor</span>
                    <span class="k">if</span> <span class="n">any_abnormal_number</span><span class="p">(</span><span class="n">slow_p</span><span class="p">):</span>
                        <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
                            <span class="s1">&#39;</span><span class="si">{0}</span><span class="s1"> p_data has abnormal value,trident automatically replace these abnormal value to zero.</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
                        <span class="n">slow_p</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">is_abnormal_number</span><span class="p">(</span><span class="n">slow_p</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">slow_p</span><span class="p">)</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">slow_p</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="Ranger21"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Ranger21">[docs]</a><span class="k">class</span> <span class="nc">Ranger21</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">N_sma_threshhold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">,</span>
                 <span class="n">momentum_type</span><span class="o">=</span><span class="s2">&quot;pnm&quot;</span><span class="p">,</span> <span class="n">pnm_momentum_factor</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
                 <span class="n">weight_decay</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">gradient_centralization</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">,</span>
                 <span class="n">using_normgc</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">use_madgrad</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">use_adabelief</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">softplus</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">beta_softplus</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
                 <span class="n">use_adaptive_gradient_clipping</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">agc_clipping_value</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
                 <span class="n">agc_eps</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>

                 <span class="n">normloss_active</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">normloss_factor</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">lookahead_active</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">lookahead_mergetime</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                 <span class="n">lookahead_blending_alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span>
                 <span class="n">lookahead_load_at_validation</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">use_cheb</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">use_warmup</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">num_warmup_iterations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
                 <span class="n">warmdown_active</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">warmdown_start_pct</span><span class="o">=</span><span class="mf">0.72</span><span class="p">,</span>
                 <span class="n">warmdown_min_lr</span><span class="o">=</span><span class="mf">3e-5</span><span class="p">,</span>
                 <span class="n">decay_type</span><span class="o">=</span><span class="s2">&quot;stable&quot;</span><span class="p">,</span>
                 <span class="n">warmup_type</span><span class="o">=</span><span class="s2">&quot;linear&quot;</span><span class="p">,</span>
                 <span class="n">warmup_pct_default</span><span class="o">=</span><span class="mf">0.22</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="c1"># momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum_pnm</span> <span class="o">=</span> <span class="n">momentum_type</span> <span class="o">==</span> <span class="s2">&quot;pnm&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pnm_momentum</span> <span class="o">=</span> <span class="n">pnm_momentum_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>

        <span class="c1"># decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">decay_type</span> <span class="o">=</span> <span class="n">decay_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">param_size</span> <span class="o">=</span> <span class="mi">0</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">use_madgrad</span> <span class="o">=</span> <span class="n">use_madgrad</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_madgrad</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">core_engine</span> <span class="o">=</span> <span class="s2">&quot;AdamW&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">core_engine</span> <span class="o">=</span> <span class="s2">&quot;madgrad&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_adabelief</span> <span class="o">=</span> <span class="n">use_adabelief</span>
        <span class="c1"># eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eps</span> <span class="o">=</span> <span class="n">eps</span>

        <span class="c1"># softplus for denom</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span> <span class="o">=</span> <span class="n">softplus</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">beta_softplus</span> <span class="o">=</span> <span class="n">beta_softplus</span>
        <span class="c1"># norm loss</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normloss_active</span> <span class="o">=</span> <span class="n">normloss_active</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">normloss_factor</span> <span class="o">=</span> <span class="n">normloss_factor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_gc</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">gradient_centralization</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_gcnorm</span> <span class="o">=</span> <span class="n">using_normgc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gc_conv_only</span> <span class="o">=</span> <span class="kc">True</span> <span class="k">if</span> <span class="n">gradient_centralization</span> <span class="o">==</span> <span class="s1">&#39;gcc&#39;</span> <span class="k">else</span> <span class="kc">False</span>

        <span class="c1"># lookahead</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_active</span> <span class="o">=</span> <span class="n">lookahead_active</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_mergetime</span> <span class="o">=</span> <span class="n">lookahead_mergetime</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_step</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_alpha</span> <span class="o">=</span> <span class="n">lookahead_blending_alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_validation_load</span> <span class="o">=</span> <span class="n">lookahead_load_at_validation</span>

        <span class="c1"># agc</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agc_active</span> <span class="o">=</span> <span class="n">use_adaptive_gradient_clipping</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agc_clip_val</span> <span class="o">=</span> <span class="n">agc_clipping_value</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">agc_eps</span> <span class="o">=</span> <span class="n">agc_eps</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">total_iterations</span> <span class="o">=</span> <span class="mi">5000</span>
        <span class="c1"># chebs</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_cheb</span> <span class="o">=</span> <span class="n">use_cheb</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">cheb_schedule</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cheb</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iterations</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;can&#39;t produce chebs without num epochs info being passed in&quot;</span>
                <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">cheb_schedule</span> <span class="o">=</span> <span class="n">get_chebs</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">total_iterations</span><span class="o">//</span><span class="mi">100</span><span class="p">)</span>

        <span class="c1"># self.total_iterations = num_epochs * num_batches_per_epoch</span>
        <span class="c1"># if not self.total_iterations:</span>
        <span class="c1">#     raise ValueError(</span>
        <span class="c1">#         &quot;missing total iterations, which is calced from num epochs and num iters per epoch param&quot;</span>
        <span class="c1">#     )</span>

        <span class="c1"># lr</span>

        <span class="c1"># warmup - we&#39;ll use default recommended in Ma/Yarats unless user specifies num iterations</span>
        <span class="c1"># -=-=-=-=-=-=-=-=-=-=-=-=-=--=-=--=-=-</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_warmup</span> <span class="o">=</span> <span class="n">use_warmup</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_complete</span> <span class="o">=</span> <span class="kc">False</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_type</span> <span class="o">=</span> <span class="n">warmup_type</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmup_pct_default</span> <span class="o">=</span> <span class="n">warmup_pct_default</span>


        <span class="k">if</span> <span class="n">use_warmup</span><span class="o">==</span><span class="kc">True</span> <span class="ow">and</span> <span class="n">num_warmup_iterations</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">beta_warmup_iters</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span>
                <span class="p">(</span><span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
            <span class="p">)</span>  <span class="c1"># default untuned linear warmup</span>

            <span class="n">beta_pct</span> <span class="o">=</span> <span class="n">beta_warmup_iters</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iterations</span>
            <span class="c1"># print(f&quot;beta_warmup_pct = {beta_pct}&quot;)</span>

            <span class="c1"># this can be unreasonable for short runs...so let&#39;s compare vs warmup pct % of total epochs</span>
            <span class="k">if</span> <span class="n">beta_pct</span> <span class="o">&gt;</span> <span class="mf">0.45</span><span class="p">:</span>
                <span class="n">warmup_auto_pct</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_pct_default</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iterations</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_warmup_iters</span> <span class="o">=</span> <span class="n">warmup_auto_pct</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">num_warmup_iters</span> <span class="o">=</span> <span class="n">beta_warmup_iters</span>

        <span class="k">else</span><span class="p">:</span>  <span class="c1"># user passed in specific num</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_warmup_iters</span> <span class="o">=</span> <span class="n">num_warmup_iterations</span> <span class="k">if</span> <span class="n">num_warmup_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">0</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">use_warmup</span><span class="o">=</span><span class="kc">True</span>  <span class="k">if</span> <span class="n">num_warmup_iterations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="kc">False</span>

        <span class="c1"># warm down</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span> <span class="o">=</span> <span class="n">warmdown_min_lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_lr_delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_base_lr</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_active</span> <span class="o">=</span> <span class="n">warmdown_active</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_active</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">warm_down_start_pct</span> <span class="o">=</span> <span class="n">warmdown_start_pct</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">start_warm_down</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">warm_down_start_pct</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">total_iterations</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_total_iterations</span> <span class="o">=</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">total_iterations</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_warm_down</span>
            <span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_displayed</span> <span class="o">=</span> <span class="kc">False</span>  <span class="c1"># print when warmdown begins...</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">warmup_curr_pct</span> <span class="o">=</span> <span class="mf">0.01</span>  <span class="c1"># used to verify warmup reaches full set point.</span>

            <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">            print(f&quot;debug warmdown:\n&quot;)</span>
<span class="sd">            print(f&quot;warm_down_start_pct = {self.warm_down_start_pct}&quot;)</span>
<span class="sd">            print(f&quot;num_epochs = {self.num_epochs}, num_batches per epoch = {self.num_batches_per_epoch}&quot;)</span>
<span class="sd">            print(f&quot; start warmdown at {self.start_warm_down}&quot;)</span>
<span class="sd">            print(f&quot; total iterations of warmdown = {self.warmdown_total_iterations}&quot;)</span>
<span class="sd">            print(f&quot; total lr delta = {self.warmdown_lr_delta}&quot;)</span>
<span class="sd">            &quot;&quot;&quot;</span>

        <span class="c1"># parameter checks</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid slow update rate: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">alpha</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">k</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid lookahead steps: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">k</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">lr</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid Learning Rate: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">eps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid eps: </span><span class="si">{}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>

        <span class="c1"># parameter comments:</span>
        <span class="c1"># beta1 (momentum) of .95 seems to work better than .90...</span>
        <span class="c1"># N_sma_threshold of 5 seems better in testing than 4.</span>
        <span class="c1"># In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to</span>
        <span class="c1"># make sure which works best for you.</span>

        <span class="c1"># prep defaults and init torch.optim base</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">gradient_centralization</span><span class="o">=</span><span class="n">gradient_centralization</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="c1"># adjustable threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span> <span class="o">=</span> <span class="n">N_sma_threshhold</span>

        <span class="c1"># now we can get to work...</span>
        <span class="c1"># removed as we now use step from RAdam...no need for</span>
        <span class="c1"># duplicate step counting</span>
        <span class="c1"># for group in self.param_groups:</span>
        <span class="c1">#    group[&quot;step_counter&quot;] = 0</span>
        <span class="c1"># print(&quot;group step counter init&quot;)</span>

        <span class="c1"># look ahead params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

        <span class="c1"># radam buffer for state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">radam_buffer</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>

        <span class="c1"># self.first_run_check=0</span>

        <span class="c1"># lookahead weights</span>
        <span class="c1"># 9/2/19 - lookahead param tensors have been moved to state storage.</span>
        <span class="c1"># This should resolve issues with load/save where weights were left in</span>
        <span class="c1"># GPU memory from first load, slowing down future runs.</span>

        <span class="c1"># self.slow_weights = [[p.clone().detach() for p in group[&#39;params&#39;]]</span>
        <span class="c1">#                     for group in self.param_groups]</span>

        <span class="c1"># don&#39;t use grad for lookahead weights</span>
        <span class="c1"># for w in it.chain(*self.slow_weights):</span>
        <span class="c1">#    w.requires_grad = False</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Ranger21</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="Ranger21.unit_norm"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Ranger21.unit_norm">[docs]</a>    <span class="k">def</span> <span class="nf">unit_norm</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; axis-based Euclidean norm&quot;&quot;&quot;</span>
        <span class="c1"># verify shape</span>
        <span class="n">keepdim</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="n">dim</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">xlen</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

            <span class="c1"># print(f&quot;xlen = {xlen}&quot;)</span>

            <span class="k">if</span> <span class="n">xlen</span> <span class="o">&lt;=</span> <span class="mi">1</span><span class="p">:</span>
                <span class="n">keepdim</span> <span class="o">=</span> <span class="kc">False</span>
            <span class="k">elif</span> <span class="n">xlen</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">):</span>  <span class="c1"># linear layers</span>
                <span class="n">dim</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="k">elif</span> <span class="n">xlen</span> <span class="o">==</span> <span class="mi">4</span><span class="p">:</span>  <span class="c1"># conv kernels</span>
                <span class="n">dim</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">dim</span> <span class="o">=</span> <span class="nb">tuple</span><span class="p">(</span>
                    <span class="p">[</span><span class="n">x</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">xlen</span><span class="p">)]</span>
                <span class="p">)</span>  <span class="c1"># create 1,..., xlen-1 tuple, while avoiding last dim ...</span>

            <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="n">dim</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="n">keepdim</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">2.0</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="nb">type</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="p">)</span>
            <span class="k">return</span> <span class="n">x</span></div>

<div class="viewcode-block" id="Ranger21.agc"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Ranger21.agc">[docs]</a>    <span class="k">def</span> <span class="nf">agc</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;clip gradient values in excess of the unitwise norm.</span>
<span class="sd">        the hardcoded 1e-6 is simple stop from div by zero and no relation to standard optimizer eps</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="c1"># params = [p for p in parameters if p.grad is not None]</span>
        <span class="c1"># if not params:</span>
        <span class="c1">#    return</span>

        <span class="c1"># for p in params:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">p</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">:</span>
            <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>
            <span class="n">p_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_norm</span><span class="p">(</span><span class="n">p</span><span class="p">)</span><span class="o">.</span><span class="n">clamp_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">agc_eps</span><span class="p">)</span>
            <span class="n">g_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_norm</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

            <span class="n">max_norm</span> <span class="o">=</span> <span class="n">p_norm</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">agc_clip_val</span>

            <span class="n">clipped_grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">*</span> <span class="p">(</span><span class="n">max_norm</span> <span class="o">/</span> <span class="n">g_norm</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">))</span>
            <span class="k">if</span> <span class="n">clipped_grad</span><span class="o">.</span><span class="n">device</span> <span class="o">!=</span> <span class="n">grad</span><span class="o">.</span><span class="n">device</span><span class="p">:</span>
                <span class="n">grad</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">g_norm</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
                <span class="n">clipped_grad</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">g_norm</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
            <span class="n">new_grads</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">g_norm</span> <span class="o">&gt;</span> <span class="n">max_norm</span><span class="p">,</span> <span class="n">clipped_grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">)</span>
            <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">new_grads</span><span class="p">)</span></div>

<div class="viewcode-block" id="Ranger21.warmup_dampening"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Ranger21.warmup_dampening">[docs]</a>    <span class="k">def</span> <span class="nf">warmup_dampening</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">step</span><span class="p">):</span>

        <span class="n">style</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_type</span>
        <span class="n">warmup</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_warmup_iters</span>

        <span class="k">if</span> <span class="n">style</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">style</span> <span class="o">=</span> <span class="s2">&quot;linear&quot;</span>

        <span class="k">if</span> <span class="n">step</span> <span class="o">&gt;</span> <span class="n">warmup</span><span class="p">:</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_complete</span><span class="p">:</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_curr_pct</span> <span class="o">==</span> <span class="mf">1.0</span><span class="p">:</span>
                    <span class="nb">print</span><span class="p">(</span>
                        <span class="sa">f</span><span class="s2">&quot;Error - lr did not achieve full set point from warmup, currently </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">warmup_curr_pct</span><span class="si">}</span><span class="s2">&quot;</span>
                    <span class="p">)</span>

                <span class="bp">self</span><span class="o">.</span><span class="n">warmup_complete</span> <span class="o">=</span> <span class="kc">True</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adjust_learning_rate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">** Ranger21 update = Warmup complete - lr set to </span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lr</span>

        <span class="k">if</span> <span class="n">style</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="n">warmup</span> <span class="o">//</span> <span class="mi">2</span><span class="p">:</span>
                <span class="k">return</span> <span class="mf">1e-8</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">warmup_curr_pct</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="p">(</span><span class="n">step</span> <span class="o">/</span> <span class="n">warmup</span><span class="p">))</span>
                <span class="n">new_lr</span> <span class="o">=</span> <span class="mf">1e-8</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">base_lr</span> <span class="o">/</span> <span class="mf">1e-8</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="p">(</span><span class="n">warmup</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)),</span> <span class="n">step</span> <span class="o">-</span> <span class="n">warmup</span> <span class="o">//</span> <span class="mi">2</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">new_lr</span>

        <span class="c1"># elif style == &quot;exponential&quot;:</span>
        <span class="c1"># return lr * (1.0 - math.exp(-step / warmup))</span>

        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;warmup type </span><span class="si">{</span><span class="n">style</span><span class="si">}</span><span class="s2"> not implemented.&quot;</span><span class="p">)</span></div>

<div class="viewcode-block" id="Ranger21.get_warm_down"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Ranger21.get_warm_down">[docs]</a>    <span class="k">def</span> <span class="nf">get_warm_down</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lr</span><span class="p">,</span> <span class="n">iteration</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot; linear style warmdown &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">iteration</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_warm_down</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">lr</span>

        <span class="k">if</span> <span class="n">iteration</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_warm_down</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># print when starting</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_displayed</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2">** Ranger21 update: Warmdown starting now.  Current iteration = </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2">....</span><span class="se">\n</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_displayed</span> <span class="o">=</span> <span class="kc">True</span>

            <span class="n">warmdown_iteration</span> <span class="o">=</span> <span class="p">(</span>
                                         <span class="n">iteration</span> <span class="o">+</span> <span class="mi">1</span>
                                 <span class="p">)</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">start_warm_down</span>  <span class="c1"># to force the first iteration to be 1 instead of 0</span>

            <span class="k">if</span> <span class="n">warmdown_iteration</span> <span class="o">&lt;</span> <span class="mi">1</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span>
                    <span class="sa">f</span><span class="s2">&quot; warning - iteration started at </span><span class="si">{</span><span class="n">iteration</span><span class="si">}</span><span class="s2"> and </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">start_warm_down</span><span class="si">}</span><span class="s2"> with value </span><span class="si">{</span><span class="n">warmdown_iteration</span><span class="si">}</span><span class="s2">&quot;</span>
                <span class="p">)</span>
                <span class="n">warmdown_iteration</span> <span class="o">=</span> <span class="mi">1</span>
            <span class="c1"># print(f&quot;warmdown iteration = {warmdown_iteration}&quot;)</span>
            <span class="c1"># linear start 3672  5650 total iterations 1972 iterations</span>

            <span class="n">warmdown_pct</span> <span class="o">=</span> <span class="n">warmdown_iteration</span> <span class="o">/</span> <span class="p">(</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_total_iterations</span> <span class="o">+</span> <span class="mi">1</span>
            <span class="p">)</span>  <span class="c1"># +1 to offset that we have to include first as an iteration to support 1 index instead of 0 based.</span>
            <span class="k">if</span> <span class="n">warmdown_pct</span> <span class="o">&gt;</span> <span class="mf">1.00</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;error in warmdown pct calc.  new pct = </span><span class="si">{</span><span class="n">warmdown_pct</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;auto handled but please report issue&quot;</span><span class="p">)</span>
                <span class="n">warmdown_pct</span> <span class="o">=</span> <span class="mf">1.00</span>

            <span class="c1"># .5</span>
            <span class="n">lr_range</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_lr_delta</span>

            <span class="n">reduction</span> <span class="o">=</span> <span class="n">lr_range</span> <span class="o">*</span> <span class="n">warmdown_pct</span>
            <span class="c1"># print(f&quot;lr reduction = {reduction} for {warmdown_pct} with iter {warmdown_iteration} and total iter {iteration}&quot;)</span>
            <span class="n">new_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_lr</span> <span class="o">-</span> <span class="n">reduction</span>
            <span class="k">if</span> <span class="n">new_lr</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span><span class="p">:</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;error in warmdown - lr below min lr. current lr = </span><span class="si">{</span><span class="n">new_lr</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;auto handling but please report issue!&quot;</span><span class="p">)</span>
                <span class="n">new_lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_lr</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">new_lr</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">new_lr</span><span class="o">=</span><span class="n">new_lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">new_lr</span></div>

            <span class="c1"># new_lr = (</span>
            <span class="c1">#    self.min_lr</span>
            <span class="c1">#    + self.starting_lr</span>
            <span class="c1">#    * (1 + math.cos(math.pi * warmdown_iteration / self.warmdown_total_iterations))</span>
            <span class="c1">#    / 2</span>
            <span class="c1"># )</span>
            <span class="c1"># self.current_lr = new_lr</span>
            <span class="c1"># return new_lr</span>

    <span class="c1">#   Lookahead merge process</span>
<div class="viewcode-block" id="Ranger21.lookahead_process_step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Ranger21.lookahead_process_step">[docs]</a>    <span class="k">def</span> <span class="nf">lookahead_process_step</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;handles blending of params for lookahead step&quot;&quot;&quot;</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_active</span><span class="p">:</span>
            <span class="k">return</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_step</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_step</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_mergetime</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_step</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="c1"># merge lookahead cached params and save current ones</span>
            <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
                <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                        <span class="k">continue</span>

                    <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">lookahead_alpha</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span>
                        <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;lookahead_params&quot;</span><span class="p">],</span>
                        <span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_alpha</span><span class="p">,</span>
                    <span class="p">)</span>
                    <span class="c1"># save for next merge</span>
                    <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;lookahead_params&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span></div>

    <span class="c1"># def new_epoch_handler(self, iteration):</span>

    <span class="c1">#    self.epoch_count +=1</span>

<div class="viewcode-block" id="Ranger21.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Ranger21.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Args:</span>
<span class="sd">            closure (callable): call for get loss backward</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">global</span> <span class="n">variance_ma_belief</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="n">param_size</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">variance_ma_sum</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">group</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]):</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="c1"># cast data type</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="c1"># apply agc if enabled</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">agc_active</span><span class="p">:</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">agc</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>

                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Ranger optimizer does not support sparse gradients&#39;</span><span class="p">)</span>

                <span class="c1"># p_data = p.data</span>
                <span class="n">p_data_fp32</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>  <span class="c1"># get state dict for this param</span>
                <span class="n">momentum</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;momentum&quot;</span><span class="p">]</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># if first time to run...init dictionary with our desired entries</span>

                    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

                    <span class="c1"># Exponential moving average of gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;grad_ma&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>

                    <span class="c1"># Exponential moving average of squared gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s2">&quot;variance_ma&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_active</span><span class="p">:</span>
                        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;lookahead_params&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;lookahead_params&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_adabelief</span><span class="p">:</span>
                        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;variance_ma_belief&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum_pnm</span><span class="p">:</span>
                        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;neg_grad_ma&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>

                        <span class="c1"># Maintains max of all exp. moving avg. of sq. grad. values</span>
                        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_variance_ma&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>

                <span class="c1"># centralize gradients</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gc</span><span class="p">:</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">centralize_gradient</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">gc_conv_only</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gc_conv_only</span><span class="p">,</span> <span class="p">)</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gcnorm</span><span class="p">:</span>
                    <span class="n">grad</span> <span class="o">=</span> <span class="n">normalize_gradient</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>
                <span class="c1"># else:</span>
                <span class="c1">#    grad = uncentralized_grad</span>

                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="n">step</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>

                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">]</span>
                <span class="n">grad_ma</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;grad_ma&quot;</span><span class="p">]</span>

                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>
                <span class="c1"># print(f&quot;bias2 = {bias_correction2}&quot;)</span>

                <span class="n">variance_ma</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;variance_ma&quot;</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_adabelief</span><span class="p">:</span>
                    <span class="n">variance_ma_belief</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;variance_ma_belief&quot;</span><span class="p">]</span>

                <span class="c1"># print(f&quot;variance_ma, upper loop = {variance_ma}&quot;)</span>

                <span class="c1"># update the exp averages</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_adabelief</span><span class="p">:</span>
                    <span class="n">grad_ma</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>
                    <span class="n">grad_residual</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">-</span> <span class="n">grad_ma</span>
                    <span class="n">variance_ma_belief</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul</span><span class="p">(</span><span class="n">grad_residual</span><span class="p">,</span> <span class="n">grad_residual</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

                <span class="c1"># print(f&quot;upper loop grad = {grad.shape}&quot;)</span>
                <span class="n">variance_ma</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
                <span class="c1"># print(f&quot;variance_ma, grad adjusted&quot;)</span>
                <span class="n">variance_ma_debiased</span> <span class="o">=</span> <span class="n">variance_ma</span> <span class="o">/</span> <span class="n">bias_correction2</span>

                <span class="n">variance_ma_sum</span> <span class="o">+=</span> <span class="n">variance_ma_debiased</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
                <span class="c1"># print(f&quot;variance_ma_sum = {variance_ma_sum}&quot;)</span>
                <span class="c1"># else: #madgrad</span>

        <span class="c1"># if not self.param_size:</span>
        <span class="c1">#     self.param_size = param_size</span>
        <span class="c1">#     print(f&quot;params size saved&quot;)</span>
        <span class="c1">#     print(f&quot;total param groups = {i + 1}&quot;)</span>
        <span class="c1">#     print(f&quot;total params in groups = {j + 1}&quot;)</span>
        <span class="c1">#</span>
        <span class="c1"># if not self.param_size:</span>
        <span class="c1">#     raise ValueError(&quot;failed to set param size&quot;)</span>

        <span class="c1"># stable weight decay</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_madgrad</span><span class="p">:</span>
            <span class="n">variance_normalized</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">variance_ma_sum</span> <span class="o">/</span> <span class="n">param_size</span><span class="p">,</span> <span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">variance_normalized</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">variance_ma_sum</span> <span class="o">/</span> <span class="n">param_size</span><span class="p">)</span>
            <span class="c1"># variance_mean = variance_ma_sum / param_size</span>
        <span class="k">if</span> <span class="n">math</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">variance_normalized</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s2">&quot;hit nan for variance_normalized&quot;</span><span class="p">)</span>

        <span class="c1"># print(f&quot;variance_mean = {variance_mean}&quot;)</span>
        <span class="c1"># print(f&quot;variance_normalized = {variance_normalized}&quot;)</span>
        <span class="c1"># else:</span>
        <span class="c1">#    variance_normalized = math.pow((variance_ma / self.param_size), .3333)</span>

        <span class="c1"># print(f&quot;variance mean sqrt = {variance_normalized}&quot;)</span>

        <span class="c1"># phase 2 - apply weight decay and step</span>
        <span class="c1"># ===========================================</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="c1"># print(f&quot;In second phase loop&quot;)</span>
            <span class="n">step</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span>

            <span class="c1"># Perform stable weight decay</span>
            <span class="n">decay</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">]</span>
            <span class="n">eps</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;eps&quot;</span><span class="p">]</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>
            <span class="n">momentum</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;momentum&quot;</span><span class="p">]</span>

            <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">]</span>

            <span class="c1"># warmup</span>
            <span class="c1"># ======================</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_warmup</span> <span class="ow">and</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_complete</span><span class="p">:</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_dampening</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">new_lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="c1"># print(f&quot;lr = {lr}&quot;)</span>

            <span class="c1"># chebyshev</span>
            <span class="c1"># ===================</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_cheb</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmup_complete</span><span class="p">:</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_cheb_lr</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">new_lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>

            <span class="c1"># warmdown</span>
            <span class="c1"># ==========</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warmdown_active</span><span class="p">:</span>
                <span class="n">orig_lr</span> <span class="o">=</span> <span class="n">lr</span>
                <span class="n">lr</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_warm_down</span><span class="p">(</span><span class="n">lr</span><span class="p">,</span> <span class="n">step</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">adjust_learning_rate</span><span class="p">(</span><span class="n">new_lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
                <span class="k">assert</span> <span class="n">lr</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;lr went negative&quot;</span>

            <span class="c1"># madgrad outer</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_madgrad</span><span class="p">:</span>
                <span class="n">ck</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">momentum</span>
                <span class="n">lamb</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>

            <span class="c1"># stable decay and / or norm loss</span>
            <span class="c1"># ==================================</span>
            <span class="k">if</span> <span class="n">decay</span><span class="p">:</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_madgrad</span><span class="p">:</span>
                    <span class="c1"># stable weight decay</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay</span> <span class="o">*</span> <span class="n">lr</span> <span class="o">/</span> <span class="n">variance_normalized</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">decay</span> <span class="o">*</span> <span class="n">lamb</span> <span class="o">/</span> <span class="n">variance_normalized</span><span class="p">)</span>

            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">normloss_active</span><span class="p">:</span>
                <span class="c1"># apply norm loss</span>
                <span class="n">unorm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unit_norm</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                <span class="n">correction</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">normloss_factor</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">torch</span><span class="o">.</span><span class="n">div</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">unorm</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)))</span>
                <span class="n">p</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">correction</span><span class="p">)</span>

            <span class="c1"># innner loop, params</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
                <span class="n">inner_grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_madgrad</span><span class="p">:</span>
                    <span class="c1"># ================== madgrad ============================</span>
                    <span class="k">if</span> <span class="s2">&quot;grad_sum_sq&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">state</span><span class="p">:</span>
                        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;grad_sum_sq&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                        <span class="n">state</span><span class="p">[</span><span class="s2">&quot;s&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>
                        <span class="k">if</span> <span class="n">momentum</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                            <span class="n">state</span><span class="p">[</span><span class="s2">&quot;x0&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clone</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span>

                    <span class="k">if</span> <span class="n">momentum</span> <span class="o">!=</span> <span class="mf">0.0</span> <span class="ow">and</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                        <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                            <span class="s2">&quot;momentum != 0 is not compatible with sparse gradients&quot;</span>
                        <span class="p">)</span>

                    <span class="c1"># centralize gradients</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gc</span><span class="p">:</span>
                        <span class="n">inner_grad</span> <span class="o">=</span> <span class="n">centralize_gradient</span><span class="p">(</span>
                            <span class="n">inner_grad</span><span class="p">,</span>
                            <span class="n">gc_conv_only</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gc_conv_only</span><span class="p">,</span>
                        <span class="p">)</span>

                    <span class="n">grad_sum_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;grad_sum_sq&quot;</span><span class="p">]</span>
                    <span class="n">s</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;s&quot;</span><span class="p">]</span>
                    <span class="k">if</span> <span class="n">momentum</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="c1"># Compute x_0 from other known quantities</span>
                        <span class="n">rms</span> <span class="o">=</span> <span class="n">grad_sum_sq</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
                        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">:</span>
                            <span class="n">rms</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">rms</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_softplus</span><span class="p">)</span>
                        <span class="n">x0</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">addcdiv</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">rms</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">x0</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;x0&quot;</span><span class="p">]</span>

                    <span class="c1"># Accumulate second moments</span>

                    <span class="c1"># print(f&quot; grad = {grad}&quot;)</span>
                    <span class="c1"># print(f&quot;lamb = {lamb}&quot;)</span>
                    <span class="c1"># print(f&quot;gsumsq = {grad_sum_sq}&quot;)</span>

                    <span class="n">grad_sum_sq</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">inner_grad</span><span class="p">,</span> <span class="n">inner_grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="n">lamb</span><span class="p">)</span>
                    <span class="n">rms</span> <span class="o">=</span> <span class="n">grad_sum_sq</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="mi">3</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">:</span>
                        <span class="n">rms</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">rms</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_softplus</span><span class="p">)</span>

                    <span class="c1"># Update s</span>
                    <span class="n">s</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">inner_grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">lamb</span><span class="p">)</span>

                    <span class="c1"># Step</span>
                    <span class="k">if</span> <span class="n">momentum</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">x0</span><span class="o">.</span><span class="n">addcdiv</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">rms</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mi">1</span><span class="p">))</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">z</span> <span class="o">=</span> <span class="n">x0</span><span class="o">.</span><span class="n">addcdiv</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="n">rms</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

                        <span class="c1"># p is a moving average of z</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">ck</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">z</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">ck</span><span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>  <span class="c1"># adam with pnm core</span>
                    <span class="c1"># ============= adamW with pnm option ========================</span>

                    <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span>

                    <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;betas&quot;</span><span class="p">]</span>

                    <span class="n">grad_ma</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;grad_ma&quot;</span><span class="p">]</span>
                    <span class="n">variance_ma</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;variance_ma&quot;</span><span class="p">]</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_adabelief</span><span class="p">:</span>
                        <span class="n">variance_ma_belief</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;variance_ma_belief&quot;</span><span class="p">]</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum_pnm</span><span class="p">:</span>
                        <span class="n">max_variance_ma</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;max_variance_ma&quot;</span><span class="p">]</span>

                        <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s2">&quot;step&quot;</span><span class="p">]</span> <span class="o">%</span> <span class="mi">2</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
                            <span class="n">grad_ma</span><span class="p">,</span> <span class="n">neg_grad_ma</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;grad_ma&quot;</span><span class="p">],</span>
                                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;neg_grad_ma&quot;</span><span class="p">],</span>
                            <span class="p">)</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">grad_ma</span><span class="p">,</span> <span class="n">neg_grad_ma</span> <span class="o">=</span> <span class="p">(</span>
                                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;neg_grad_ma&quot;</span><span class="p">],</span>
                                <span class="n">state</span><span class="p">[</span><span class="s2">&quot;grad_ma&quot;</span><span class="p">],</span>
                            <span class="p">)</span>

                    <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">step</span>
                    <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">step</span>

                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum_pnm</span><span class="p">:</span>
                        <span class="c1"># Maintains the maximum of all 2nd moment running avg. till now</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_variance_ma</span><span class="p">,</span> <span class="n">variance_ma</span><span class="p">,</span> <span class="n">out</span><span class="o">=</span><span class="n">variance_ma</span><span class="p">)</span>
                        <span class="c1"># Use the max. for normalizing running avg. of gradient</span>
                        <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">variance_ma</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span>
                            <span class="n">group</span><span class="p">[</span><span class="s2">&quot;eps&quot;</span><span class="p">]</span>
                        <span class="p">)</span>

                    <span class="c1"># centralize gradients</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gc</span><span class="p">:</span> <span class="n">grad</span> <span class="o">=</span> <span class="n">centralize_gradient</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">gc_conv_only</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">gc_conv_only</span><span class="p">,</span> <span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_gcnorm</span><span class="p">:</span>
                        <span class="n">grad</span> <span class="o">=</span> <span class="n">normalize_gradient</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_adabelief</span><span class="p">:</span>
                        <span class="n">grad_ma</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

                    <span class="n">noise_norm</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">1</span> <span class="o">+</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">+</span> <span class="n">beta2</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

                    <span class="n">step_size</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">/</span> <span class="n">bias_correction1</span>

                    <span class="c1"># softplus the denom</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">softplus</span><span class="p">:</span>
                        <span class="n">denom</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softplus</span><span class="p">(</span><span class="n">denom</span><span class="p">,</span> <span class="n">beta</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">beta_softplus</span><span class="p">)</span>

                    <span class="n">pnmomentum</span> <span class="o">=</span> <span class="p">(</span>
                        <span class="n">grad_ma</span><span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">1</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">momentum_pnm</span><span class="p">)</span>
                        <span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">neg_grad_ma</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum_pnm</span><span class="p">)</span>
                        <span class="o">.</span><span class="n">mul</span><span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">noise_norm</span><span class="p">)</span>
                    <span class="p">)</span>

                    <span class="n">p</span><span class="o">.</span><span class="n">addcdiv_</span><span class="p">(</span><span class="n">pnmomentum</span><span class="p">,</span> <span class="n">denom</span><span class="p">,</span> <span class="n">value</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>

                    <span class="c1"># denom = variance_biased_ma.sqrt().add(eps)</span>

                    <span class="c1"># step_size = lr / bias_correction1</span>

                    <span class="c1"># update weights</span>
                    <span class="c1"># p.data.add_(weight_mod, alpha=-step_size)</span>
                    <span class="c1"># p.addcdiv_(grad_ma, denom, value=-step_size)</span>
        <span class="c1"># print(f&quot;\n End optimizer step\n&quot;)</span>

        <span class="c1"># end of step processes....</span>

        <span class="c1"># lookahead</span>
        <span class="c1"># ---------------------</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_active</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">lookahead_process_step</span><span class="p">()</span>

        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="RangerLars"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.RangerLars">[docs]</a><span class="k">class</span> <span class="nc">RangerLars</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    https://github.com/lessw2020/Ranger-Deep-Learning-Optimizer/blob/master/ranger/ranger.py</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">N_sma_threshhold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span>
                 <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="c1"># parameter checks</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid slow update rate: </span><span class="si">{alpha}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">k</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid lookahead steps: </span><span class="si">{k}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">lr</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid Learning Rate: </span><span class="si">{lr}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">eps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid eps: </span><span class="si">{eps}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span> <span class="o">=</span> <span class="n">N_sma_threshhold</span>
        <span class="c1"># parameter comments:</span>
        <span class="c1"># beta1 (momentum) of .95 seems to work better than .90...</span>
        <span class="c1"># N_sma_threshold of 5 seems better in testing than 4.</span>
        <span class="c1"># In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.</span>

        <span class="c1"># prep defaults and init torch.optim base</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>

        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
        <span class="c1"># radam buffer for state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">radam_buffer</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

        <span class="c1"># self.first_run_check=0</span>

        <span class="c1"># lookahead weights  # 9/2/19 - lookahead param tensors have been moved to state storage.  # This should   #</span>
        <span class="c1"># resolve issues with load/save where weights were left in GPU memory from first load, slowing down future runs.</span>

        <span class="c1"># self.slow_weights = [[p.clone().detach() for p in group[&#39;params&#39;]]  #                     for group in</span>
        <span class="c1"># self.param_groups]</span>

        <span class="c1"># don&#39;t use grad for lookahead weights  # for w in it.chain(*self.slow_weights):  #    w.requires_grad = False</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;set state called&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RangerLars</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="RangerLars.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.RangerLars.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Args:</span>
<span class="sd">            closure (callable): call for get loss backward</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="c1"># cast data type</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Ranger optimizer does not support sparse gradients&#39;</span><span class="p">)</span>

                <span class="n">p_data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>  <span class="c1"># get state dict for this param</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># if first time to run...init dictionary with our desired entries</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span>

                    <span class="c1"># look ahead weight storage now in state dict</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>

                <span class="c1"># begin computations</span>
                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gcc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="n">grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dim</span><span class="p">())),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>

                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

                <span class="c1"># grad=_filter_grads(grad,self.gradient_centralization)</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">buffered</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">radam_buffer</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="mi">10</span><span class="p">)]</span>
                <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">N_sma</span><span class="p">,</span> <span class="n">step_size</span> <span class="o">=</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                    <span class="n">beta2_t</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                    <span class="n">N_sma_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="n">N_sma</span> <span class="o">=</span> <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">beta2_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">N_sma</span>

                    <span class="c1"># more conservative since it&#39;s an approximated value</span>
                    <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span><span class="p">:</span>
                        <span class="n">step_size</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                            <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">N_sma</span> <span class="o">*</span> <span class="n">N_sma_max</span> <span class="o">/</span> <span class="p">(</span>
                                    <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">step_size</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_size</span>

                <span class="n">update</span> <span class="o">=</span> <span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>

                    <span class="n">G_grad</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg</span> <span class="o">/</span> <span class="n">denom</span><span class="p">)</span> <span class="o">*</span> <span class="n">step_size</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">G_grad</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">*</span> <span class="n">step_size</span>

                <span class="c1"># if group[&#39;weight_decay&#39;] != 0:</span>
                <span class="c1">#     update.add_(group[&#39;weight_decay&#39;], p_data)</span>
                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">G_grad</span> <span class="o">=</span> <span class="n">G_grad</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">G_grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">G_grad</span> <span class="o">=</span> <span class="n">G_grad</span> <span class="o">-</span> <span class="n">G_grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ndim</span><span class="p">(</span><span class="n">G_grad</span><span class="p">))),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="n">radam_norm</span> <span class="o">=</span> <span class="n">update</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
                <span class="n">weight_norm</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mf">2.0</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">weight_norm</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">radam_norm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="mf">1.0</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="n">weight_norm</span> <span class="o">/</span> <span class="n">radam_norm</span>

                <span class="n">trust_ratio</span> <span class="o">=</span> <span class="n">clip</span><span class="p">(</span><span class="n">to_tensor</span><span class="p">(</span><span class="n">trust_ratio</span><span class="p">),</span> <span class="mf">0.0</span><span class="p">,</span> <span class="mf">10.0</span><span class="p">)</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;weight_norm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_norm</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;adam_norm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">radam_norm</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;trust_ratio&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trust_ratio</span>

                <span class="n">p_data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">update</span> <span class="o">*</span> <span class="n">trust_ratio</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>

                <span class="k">if</span> <span class="n">any_abnormal_number</span><span class="p">(</span><span class="n">p_data</span><span class="p">):</span>
                    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
                        <span class="s1">&#39;</span><span class="si">{0}</span><span class="s1"> p_data has abnormal value,trident automatically replace these abnormal value to zero.</span><span class="se">\n\r</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
                <span class="n">p_data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">where</span><span class="p">(</span><span class="n">is_abnormal_number</span><span class="p">(</span><span class="n">p_data</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">p_data</span><span class="p">))</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p_data</span><span class="p">)</span>

                <span class="c1"># integrated look ahead...</span>
                <span class="c1"># we do it at the param level instead of group level</span>
                <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;k&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">slow_p</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span>  <span class="c1"># get access to slow param tensor</span>
                    <span class="n">slow_p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">slow_p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>  <span class="c1"># (fast weights - slow weights) * alpha</span>
                    <span class="k">if</span> <span class="n">any_abnormal_number</span><span class="p">(</span><span class="n">slow_p</span><span class="p">):</span>
                        <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
                            <span class="s1">&#39;</span><span class="si">{0}</span><span class="s1"> p_data has abnormal value,trident automatically replace these abnormal value to zero.</span><span class="se">\n</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                                <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
                    <span class="n">slow_p</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">is_abnormal_number</span><span class="p">(</span><span class="n">slow_p</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">slow_p</span><span class="p">)</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">slow_p</span><span class="p">)</span>  <span class="c1"># copy interpolated weights to RAdam param tensor</span>

                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<span class="k">class</span> <span class="nc">LARS</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Layer-wise Adaptive Rate Scaling for large batch training.</span>
<span class="sd">    Introduced by &quot;Large Batch Training of Convolutional Networks&quot; by Y. You,</span>
<span class="sd">    I. Gitman, and B. Ginsburg. (https://arxiv.org/abs/1708.03888)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
            <span class="bp">self</span><span class="p">,</span>
            <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span>
            <span class="n">momentum</span><span class="o">=</span><span class="mf">0.9</span><span class="p">,</span>
            <span class="n">use_nesterov</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span>
            <span class="n">exclude_from_weight_decay</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">exclude_from_layer_adaptation</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
            <span class="n">classic_momentum</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">eeta</span><span class="o">=</span><span class="mf">0.001</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Constructs a LARSOptimizer.</span>
<span class="sd">        Args:</span>
<span class="sd">        lr: A `float` for learning rate.</span>
<span class="sd">        momentum: A `float` for momentum.</span>
<span class="sd">        use_nesterov: A &#39;Boolean&#39; for whether to use nesterov momentum.</span>
<span class="sd">        weight_decay: A `float` for weight decay.</span>
<span class="sd">        exclude_from_weight_decay: A list of `string` for variable screening, if</span>
<span class="sd">            any of the string appears in a variable&#39;s name, the variable will be</span>
<span class="sd">            excluded for computing weight decay. For example, one could specify</span>
<span class="sd">            the list like [&#39;batch_normalization&#39;, &#39;bias&#39;] to exclude BN and bias</span>
<span class="sd">            from weight decay.</span>
<span class="sd">        exclude_from_layer_adaptation: Similar to exclude_from_weight_decay, but</span>
<span class="sd">            for layer adaptation. If it is None, it will be defaulted the same as</span>
<span class="sd">            exclude_from_weight_decay.</span>
<span class="sd">        classic_momentum: A `boolean` for whether to use classic (or popular)</span>
<span class="sd">            momentum. The learning rate is applied during momeuntum update in</span>
<span class="sd">            classic momentum, but after momentum for popular momentum.</span>
<span class="sd">        eeta: A `float` for scaling of learning rate when computing trust ratio.</span>
<span class="sd">        name: The name for the scope.</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span>
            <span class="n">momentum</span><span class="o">=</span><span class="n">momentum</span><span class="p">,</span>
            <span class="n">use_nesterov</span><span class="o">=</span><span class="n">use_nesterov</span><span class="p">,</span>
            <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
            <span class="n">exclude_from_weight_decay</span><span class="o">=</span><span class="n">exclude_from_weight_decay</span><span class="p">,</span>
            <span class="n">exclude_from_layer_adaptation</span><span class="o">=</span><span class="n">exclude_from_layer_adaptation</span><span class="p">,</span>
            <span class="n">classic_momentum</span><span class="o">=</span><span class="n">classic_momentum</span><span class="p">,</span>
            <span class="n">eeta</span><span class="o">=</span><span class="n">eeta</span><span class="p">,</span>
        <span class="p">)</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">LARS</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">lr</span> <span class="o">=</span> <span class="n">lr</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">=</span> <span class="n">momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">use_nesterov</span> <span class="o">=</span> <span class="n">use_nesterov</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classic_momentum</span> <span class="o">=</span> <span class="n">classic_momentum</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eeta</span> <span class="o">=</span> <span class="n">eeta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">exclude_from_weight_decay</span> <span class="o">=</span> <span class="n">exclude_from_weight_decay</span>
        <span class="c1"># exclude_from_layer_adaptation is set to exclude_from_weight_decay if the</span>
        <span class="c1"># arg is None.</span>
        <span class="k">if</span> <span class="n">exclude_from_layer_adaptation</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">exclude_from_layer_adaptation</span> <span class="o">=</span> <span class="n">exclude_from_layer_adaptation</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">exclude_from_layer_adaptation</span> <span class="o">=</span> <span class="n">exclude_from_weight_decay</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;set state called&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">LARS</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">epoch</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Args:</span>
<span class="sd">            epoch (int): current epoch</span>
<span class="sd">            closure (callable): call for get loss backward</span>

<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>
        <span class="k">if</span> <span class="n">epoch</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">epoch</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epoch</span> <span class="o">+=</span> <span class="mi">1</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">]</span>
            <span class="n">momentum</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;momentum&quot;</span><span class="p">]</span>
            <span class="n">eeta</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;eeta&quot;</span><span class="p">]</span>
            <span class="n">lr</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span>

            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s2">&quot;params&quot;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>

                <span class="c1"># cast data type</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">param</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>

                <span class="n">param_state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># TODO: get param names</span>
                <span class="c1"># if self._use_weight_decay(param_name):</span>
                <span class="n">grad</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span> <span class="o">*</span> <span class="n">param</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">classic_momentum</span><span class="p">:</span>
                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="mf">1.0</span>

                    <span class="c1"># TODO: get param names</span>
                    <span class="c1"># if self._do_layer_adaptation(param_name):</span>
                    <span class="n">w_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">param</span><span class="p">)</span>
                    <span class="n">g_norm</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span>

                    <span class="n">device</span> <span class="o">=</span> <span class="n">g_norm</span><span class="o">.</span><span class="n">get_device</span><span class="p">()</span>
                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                        <span class="n">w_norm</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">where</span><span class="p">(</span>
                            <span class="n">g_norm</span><span class="o">.</span><span class="n">ge</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
                            <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">eeta</span> <span class="o">*</span> <span class="n">w_norm</span> <span class="o">/</span> <span class="n">g_norm</span><span class="p">),</span>
                            <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                        <span class="p">),</span>
                        <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.0</span><span class="p">])</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span>
                    <span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>

                    <span class="n">scaled_lr</span> <span class="o">=</span> <span class="n">lr</span> <span class="o">*</span> <span class="n">trust_ratio</span>
                    <span class="k">if</span> <span class="s2">&quot;momentum_buffer&quot;</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">param_state</span><span class="p">:</span>
                        <span class="n">next_v</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;momentum_buffer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">data</span>
                        <span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">next_v</span> <span class="o">=</span> <span class="n">param_state</span><span class="p">[</span><span class="s2">&quot;momentum_buffer&quot;</span><span class="p">]</span>

                    <span class="n">next_v</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">momentum</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">scaled_lr</span><span class="p">)</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_nesterov</span><span class="p">:</span>
                        <span class="n">update</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">momentum</span> <span class="o">*</span> <span class="n">next_v</span><span class="p">)</span> <span class="o">+</span> <span class="p">(</span><span class="n">scaled_lr</span> <span class="o">*</span> <span class="n">grad</span><span class="p">)</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">update</span> <span class="o">=</span> <span class="n">next_v</span>

                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">update</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">NotImplementedError</span>

        <span class="k">return</span> <span class="n">loss</span>

    <span class="k">def</span> <span class="nf">_use_weight_decay</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Whether to use L2 weight decay for `param_name`.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decay</span><span class="p">:</span>
            <span class="k">return</span> <span class="kc">False</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">exclude_from_weight_decay</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">exclude_from_weight_decay</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">_do_layer_adaptation</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">param_name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Whether to do layer-wise learning rate adaptation for `param_name`.&quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">exclude_from_layer_adaptation</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">r</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">exclude_from_layer_adaptation</span><span class="p">:</span>
                <span class="k">if</span> <span class="n">re</span><span class="o">.</span><span class="n">search</span><span class="p">(</span><span class="n">r</span><span class="p">,</span> <span class="n">param_name</span><span class="p">)</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
                    <span class="k">return</span> <span class="kc">False</span>
        <span class="k">return</span> <span class="kc">True</span>


<div class="viewcode-block" id="AdaBelief"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.AdaBelief">[docs]</a><span class="k">class</span> <span class="nc">AdaBelief</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Implements AdaBelief algorithm. Modified from Adam in PyTorch</span>
<span class="sd">    Arguments:</span>
<span class="sd">        params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="sd">            parameter groups</span>
<span class="sd">        lr (float, optional): learning rate (default: 1e-3)</span>
<span class="sd">        betas (Tuple[float, float], optional): coefficients used for computing</span>
<span class="sd">            running averages of gradient and its square (default: (0.9, 0.999))</span>
<span class="sd">        eps (float, optional): term added to the denominator to improve</span>
<span class="sd">            numerical stability (default: 1e-16)</span>
<span class="sd">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>
<span class="sd">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span>
<span class="sd">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span>
<span class="sd">            (default: False)</span>
<span class="sd">        weight_decouple (boolean, optional): ( default: True) If set as True, then</span>
<span class="sd">            the optimizer uses decoupled weight decay as in AdamW</span>
<span class="sd">        fixed_decay (boolean, optional): (default: False) This is used when weight_decouple</span>
<span class="sd">            is set as True.</span>
<span class="sd">            When fixed_decay == True, the weight decay is performed as</span>
<span class="sd">            $W_{new} = W_{old} - W_{old} \times decay$.</span>
<span class="sd">            When fixed_decay == False, the weight decay is performed as</span>
<span class="sd">            $W_{new} = W_{old} - W_{old} \times decay \times lr$. Note that in this case, the</span>
<span class="sd">            weight decay ratio decreases with learning rate (lr).</span>
<span class="sd">        rectify (boolean, optional): (default: True) If set as True, then perform the rectified</span>
<span class="sd">            update similar to RAdam</span>
<span class="sd">        degenerated_to_sgd (boolean, optional) (default:True) If set as True, then perform SGD update</span>
<span class="sd">            when variance of gradient is high</span>

<span class="sd">    reference: AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients, NeurIPS 2020</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span>
                 <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">weight_decouple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fixed_decay</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">rectify</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                 <span class="n">degenerated_to_sgd</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid learning rate: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid epsilon value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 0: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 1: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="p">(</span><span class="nb">list</span><span class="p">,</span> <span class="nb">tuple</span><span class="p">))</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">params</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">params</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">dict</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">param</span> <span class="ow">in</span> <span class="n">params</span><span class="p">:</span>
                <span class="k">if</span> <span class="s1">&#39;betas&#39;</span> <span class="ow">in</span> <span class="n">param</span> <span class="ow">and</span> <span class="p">(</span><span class="n">param</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="o">!=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="ow">or</span> <span class="n">param</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">][</span><span class="mi">1</span><span class="p">]</span> <span class="o">!=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">param</span><span class="p">[</span><span class="s1">&#39;buffer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>

        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span>
                        <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span> <span class="n">amsgrad</span><span class="o">=</span><span class="n">amsgrad</span><span class="p">,</span> <span class="n">buffer</span><span class="o">=</span><span class="p">[[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)])</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBelief</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">degenerated_to_sgd</span> <span class="o">=</span> <span class="n">degenerated_to_sgd</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decouple</span> <span class="o">=</span> <span class="n">weight_decouple</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">rectify</span> <span class="o">=</span> <span class="n">rectify</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fixed_decay</span> <span class="o">=</span> <span class="n">fixed_decay</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">AdaBelief</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="n">group</span><span class="o">.</span><span class="n">setdefault</span><span class="p">(</span><span class="s1">&#39;amsgrad&#39;</span><span class="p">,</span> <span class="kc">False</span><span class="p">)</span>

<div class="viewcode-block" id="AdaBelief.reset"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.AdaBelief.reset">[docs]</a>    <span class="k">def</span> <span class="nf">reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
                <span class="n">amsgrad</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;amsgrad&#39;</span><span class="p">]</span>
                <span class="n">version_higher</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&gt;=</span> <span class="s2">&quot;1.5.0&quot;</span><span class="p">)</span>
                <span class="c1"># State initialization</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                <span class="c1"># Exponential moving average of gradient values</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span> \
                    <span class="k">if</span> <span class="n">version_higher</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="c1"># Exponential moving average of squared gradient values</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span> \
                    <span class="k">if</span> <span class="n">version_higher</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                    <span class="c1"># Maintains max of all exp. moving avg. of sq. grad. values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;max_exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span> \
                        <span class="k">if</span> <span class="n">version_higher</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span></div>

<div class="viewcode-block" id="AdaBelief.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.AdaBelief.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            closure (callable, optional): A closure that reevaluates the model</span>
<span class="sd">                and returns the loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">version_higher</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&gt;=</span> <span class="s2">&quot;1.5.0&quot;</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="c1"># cast data type</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s1">&#39;AdaBelief does not support sparse gradients, please consider SparseAdam instead&#39;</span><span class="p">)</span>
                <span class="n">p_data_fp32</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="n">amsgrad</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;amsgrad&#39;</span><span class="p">]</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                <span class="c1"># State initialization</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="c1"># Exponential moving average of gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                        <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span> <span class="k">if</span> <span class="n">version_higher</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="c1"># Exponential moving average of squared gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                           <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span> <span class="k">if</span> <span class="n">version_higher</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                        <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                        <span class="c1"># Maintains max of all exp. moving avg. of sq. grad. values</span>
                        <span class="n">state</span><span class="p">[</span><span class="s1">&#39;max_exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span>
                                                                   <span class="n">memory_format</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">preserve_format</span><span class="p">)</span> <span class="k">if</span> <span class="n">version_higher</span> <span class="k">else</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span>
                            <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="c1"># perform weight decay, check if decoupled weight decay</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decouple</span><span class="p">:</span>
                    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">fixed_decay</span><span class="p">:</span>
                        <span class="n">p_data_fp32</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">p_data_fp32</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">-</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                        <span class="n">grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                <span class="c1"># get current state variable</span>
                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>

                <span class="c1"># if self.gradient_centralization in [&#39;all&#39;, &#39;gcc&#39;]:</span>
                <span class="c1">#     if len(list(grad.size())) &gt; 3:</span>
                <span class="c1">#         grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))</span>
                <span class="c1">#</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gcc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">grad</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="n">grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="o">-</span><span class="n">grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="nb">tuple</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">grad</span><span class="o">.</span><span class="n">dim</span><span class="p">())),</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">))</span>

                <span class="c1"># Update first and second moment running average</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>
                <span class="n">grad_residual</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">-</span> <span class="n">exp_avg</span>
                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad_residual</span><span class="p">,</span> <span class="n">grad_residual</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">amsgrad</span><span class="p">:</span>
                    <span class="n">max_exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;max_exp_avg_sq&#39;</span><span class="p">]</span>
                    <span class="c1"># Maintains the maximum of all 2nd moment running avg. till now</span>
                    <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">max_exp_avg_sq</span><span class="p">,</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">]),</span> <span class="n">out</span><span class="o">=</span><span class="n">max_exp_avg_sq</span><span class="p">)</span>

                    <span class="c1"># Use the max. for normalizing running avg. of gradient</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">denom</span> <span class="o">=</span> <span class="p">(</span><span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">))</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>

                <span class="c1"># update</span>
                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">rectify</span><span class="p">:</span>
                    <span class="c1"># Default update</span>
                    <span class="n">step_size</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">bias_correction1</span>
                    <span class="n">G_grad</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="n">denom</span>


                <span class="k">else</span><span class="p">:</span>  <span class="c1"># Rectified update, forked from RAdam</span>
                    <span class="n">buffered</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;buffer&#39;</span><span class="p">][</span><span class="nb">int</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="mi">10</span><span class="p">)]</span>
                    <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                        <span class="n">N_sma</span><span class="p">,</span> <span class="n">step_size</span> <span class="o">=</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                        <span class="n">beta2_t</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                        <span class="n">N_sma_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                        <span class="n">N_sma</span> <span class="o">=</span> <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">beta2_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span>
                        <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">N_sma</span>

                        <span class="c1"># more conservative since it&#39;s an approximated value</span>
                        <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;=</span> <span class="mi">5</span><span class="p">:</span>
                            <span class="n">step_size</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span>
                                <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">N_sma</span> <span class="o">*</span> <span class="n">N_sma_max</span> <span class="o">/</span> <span class="p">(</span>
                                        <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">degenerated_to_sgd</span><span class="p">:</span>
                            <span class="n">step_size</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                        <span class="k">else</span><span class="p">:</span>
                            <span class="n">step_size</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span>
                        <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_size</span>

                    <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;</span> <span class="mi">5</span><span class="p">:</span>
                        <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
                        <span class="n">G_grad</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="n">denom</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">G_grad</span> <span class="o">=</span> <span class="n">exp_avg</span>

                <span class="c1">#</span>
                <span class="c1"># if self.gradient_centralization is not None:</span>
                <span class="c1">#     if ndim(G_grad) &gt; 1:</span>
                <span class="c1">#         G_grad = G_grad - G_grad.mean(axis=list(range(1, ndim(G_grad))), keepdims=True)</span>

                <span class="n">p_data_fp32</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">G_grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">any_abnormal_number</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">):</span>
                    <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="o">.</span><span class="n">write</span><span class="p">(</span>
                        <span class="s1">&#39;</span><span class="si">{0}</span><span class="s1"> p_data has abnormal value,trident automatically replace these abnormal value to zero.</span><span class="se">\n\r</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                            <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">))</span>
                    <span class="n">p_data_fp32</span> <span class="o">=</span> <span class="n">where</span><span class="p">(</span><span class="n">is_abnormal_number</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">),</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">(),</span> <span class="n">p_data_fp32</span><span class="p">)</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="RangerAdaBelief"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.RangerAdaBelief">[docs]</a><span class="k">class</span> <span class="nc">RangerAdaBelief</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements AdaBelief algorithm. Modified from Adam in PyTorch</span>

<span class="sd">    https://github.com/juntang-zhuang/Adabelief-Optimizer</span>
<span class="sd">    Arguments:</span>
<span class="sd">        params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="sd">            parameter groups</span>
<span class="sd">        lr (float, optional): learning rate (default: 1e-3)</span>
<span class="sd">        betas (Tuple[float, float], optional): coefficients used for computing</span>
<span class="sd">            running averages of gradient and its square (default: (0.9, 0.999))</span>
<span class="sd">        eps (float, optional): term added to the denominator to improve</span>
<span class="sd">            numerical stability (default: 1e-16)</span>
<span class="sd">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>
<span class="sd">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span>
<span class="sd">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span>
<span class="sd">            (default: False)</span>
<span class="sd">        weight_decouple (boolean, optional): ( default: True) If set as True, then</span>
<span class="sd">            the optimizer uses decoupled weight decay as in AdamW</span>
<span class="sd">        fixed_decay (boolean, optional): (default: False) This is used when weight_decouple</span>
<span class="sd">            is set as True.</span>
<span class="sd">            When fixed_decay == True, the weight decay is performed as</span>
<span class="sd">            $W_{new} = W_{old} - W_{old} \times decay$.</span>
<span class="sd">            When fixed_decay == False, the weight decay is performed as</span>
<span class="sd">            $W_{new} = W_{old} - W_{old} \times decay \times lr$. Note that in this case, the</span>
<span class="sd">            weight decay ratio decreases with learning rate (lr).</span>
<span class="sd">        rectify (boolean, optional): (default: True) If set as True, then perform the rectified</span>
<span class="sd">            update similar to RAdam</span>
<span class="sd">        degenerated_to_sgd (boolean, optional) (default:True) If set as True, then perform SGD update</span>
<span class="sd">            when variance of gradient is high</span>
<span class="sd">        print_change_log (boolean, optional) (default: True) If set as True, print the modifcation to</span>
<span class="sd">            default hyper-parameters</span>
<span class="sd">    reference: AdaBelief Optimizer, adapting stepsizes by the belief in observed gradients, NeurIPS 2020</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>  <span class="c1"># lr</span>
                 <span class="n">alpha</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">N_sma_threshhold</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>  <span class="c1"># Ranger options</span>
                 <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">.95</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>  <span class="c1"># Adam options</span>
                 <span class="c1"># Gradient centralization on or off, applied to conv layers only or conv + fc layers</span>
                 <span class="n">adabelief</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">weight_decouple</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">alpha</span> <span class="o">&lt;=</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Invalid slow update rate: </span><span class="si">{</span><span class="n">alpha</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mi">1</span> <span class="o">&lt;=</span> <span class="n">k</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Invalid lookahead steps: </span><span class="si">{</span><span class="n">k</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">lr</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Invalid Learning Rate: </span><span class="si">{</span><span class="n">lr</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">eps</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Invalid eps: </span><span class="si">{</span><span class="n">eps</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="c1"># parameter comments:</span>
        <span class="c1"># beta1 (momentum) of .95 seems to work better than .90...</span>
        <span class="c1"># N_sma_threshold of 5 seems better in testing than 4.</span>
        <span class="c1"># In both cases, worth testing on your dataset (.90 vs .95, 4 vs 5) to make sure which works best for you.</span>

        <span class="c1"># prep defaults and init torch.optim base</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">alpha</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">k</span><span class="p">,</span> <span class="n">step_counter</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span>
                        <span class="n">N_sma_threshhold</span><span class="o">=</span><span class="n">N_sma_threshhold</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

        <span class="c1"># adjustable threshold</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span> <span class="o">=</span> <span class="n">N_sma_threshhold</span>

        <span class="c1"># look ahead params</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">alpha</span> <span class="o">=</span> <span class="n">alpha</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>

        <span class="c1"># radam buffer for state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">radam_buffer</span> <span class="o">=</span> <span class="p">[[</span><span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="k">for</span> <span class="n">ind</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">)]</span>

        <span class="c1"># gc on or off</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>
        <span class="c1"># level of gradient centralization</span>
        <span class="c1"># self.gc_gradient_threshold = 3 if gc_conv_only else 1</span>

        <span class="c1"># Turn on AdaBelief or Not</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adabelief</span> <span class="o">=</span> <span class="n">adabelief</span>

        <span class="c1"># Turn on decoupled weight decay or not</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">weight_decouple</span> <span class="o">=</span> <span class="n">weight_decouple</span>

        <span class="nb">print</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Ranger optimizer loaded. </span><span class="se">\n</span><span class="s2">Gradient Centralization usage = </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">use_gc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_gc</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">gc_conv_only</span> <span class="o">==</span> <span class="kc">False</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GC applied to both conv and fc layers&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">use_gc</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">gc_conv_only</span> <span class="o">==</span> <span class="kc">True</span><span class="p">):</span>
            <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;GC applied to conv layers only&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;set state called&quot;</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">RangerAdaBelief</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="RangerAdaBelief.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.RangerAdaBelief.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            closure (callable, optional): A closure that reevaluates the model</span>
<span class="sd">                and returns the loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">version_higher</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">__version__</span> <span class="o">&gt;=</span> <span class="s2">&quot;1.5.0&quot;</span><span class="p">)</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="c1"># cast data type</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decouple</span><span class="p">:</span>  <span class="c1"># if not decoupled weight decay, add weight decay to grad</span>
                    <span class="n">grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span>
                        <span class="s1">&#39;Ranger optimizer does not support sparse gradients&#39;</span><span class="p">)</span>

                <span class="n">p_data_fp32</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>  <span class="c1"># get state dict for this param</span>

                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>  <span class="c1"># if first time to run...init dictionary with our desired entries</span>
                    <span class="c1"># if self.first_run_check==0:</span>
                    <span class="c1"># self.first_run_check=1</span>
                    <span class="c1"># print(&quot;Initializing slow buffer...should not see this at load from saved model!&quot;)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>

                    <span class="c1"># look ahead weight storage now in state dict</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="k">else</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">type_as</span><span class="p">(</span>
                        <span class="n">p_data_fp32</span><span class="p">)</span>

                <span class="c1"># begin computations</span>
                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                <span class="c1"># GC operation for Conv layers and FC layers</span>
                <span class="c1"># if grad.dim() &gt; self.gc_gradient_threshold:</span>
                <span class="c1">#    grad.add_(-grad.mean(dim=tuple(range(1, grad.dim())), keepdim=True))</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gcc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">-</span> <span class="n">grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ndim</span><span class="p">(</span><span class="n">grad</span><span class="p">))),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># compute mean moving avg</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>

                <span class="c1"># compute variance mov avg</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adabelief</span><span class="p">:</span>
                    <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span> <span class="o">-</span> <span class="n">exp_avg</span><span class="p">,</span> <span class="n">grad</span> <span class="o">-</span> <span class="n">exp_avg</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

                <span class="n">buffered</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">radam_buffer</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="mi">10</span><span class="p">)]</span>

                <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span>
                    <span class="n">N_sma</span><span class="p">,</span> <span class="n">step_size</span> <span class="o">=</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                    <span class="n">beta2_t</span> <span class="o">=</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                    <span class="n">N_sma_max</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span>
                    <span class="n">N_sma</span> <span class="o">=</span> <span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span> <span class="o">*</span> \
                            <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">beta2_t</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">N_sma</span>
                    <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span><span class="p">:</span>
                        <span class="n">step_size</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2_t</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">N_sma</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">4</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span>
                                <span class="n">N_sma</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span> <span class="o">/</span> <span class="n">N_sma</span> <span class="o">*</span> <span class="n">N_sma_max</span> <span class="o">/</span> <span class="p">(</span><span class="n">N_sma_max</span> <span class="o">-</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">step_size</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">])</span>
                    <span class="n">buffered</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">step_size</span>

                <span class="c1"># if group[&#39;weight_decay&#39;] != 0:</span>
                <span class="c1">#    p_data_fp32.add_(-group[&#39;weight_decay&#39;]</span>
                <span class="c1">#                     * group[&#39;lr&#39;], p_data_fp32)</span>

                <span class="c1"># apply lr</span>
                <span class="k">if</span> <span class="n">N_sma</span> <span class="o">&gt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">N_sma_threshhold</span><span class="p">:</span>
                    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adabelief</span><span class="p">:</span>
                        <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
                    <span class="k">else</span><span class="p">:</span>
                        <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>

                    <span class="n">G_grad</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="n">denom</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">G_grad</span> <span class="o">=</span> <span class="n">exp_avg</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight_decouple</span> <span class="ow">and</span> <span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">):</span>  <span class="c1"># decoupled weight decay</span>
                    <span class="n">G_grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                <span class="c1"># GC operation</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">G_grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="n">G_grad</span> <span class="o">=</span> <span class="n">G_grad</span> <span class="o">-</span> <span class="n">G_grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ndim</span><span class="p">(</span><span class="n">G_grad</span><span class="p">))),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="n">p_data_fp32</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">G_grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">])</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>

                <span class="c1"># integrated look ahead...</span>
                <span class="c1"># we do it at the param level instead of group level</span>
                <span class="k">if</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">%</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;k&#39;</span><span class="p">]</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="c1"># get access to slow param tensor</span>
                    <span class="n">slow_p</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;slow_buffer&#39;</span><span class="p">]</span>
                    <span class="c1"># (fast weights - slow weights) * alpha</span>
                    <span class="n">slow_p</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">-</span> <span class="n">slow_p</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">alpha</span><span class="p">)</span>
                    <span class="c1"># copy interpolated weights to RAdam param tensor</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">copy_</span><span class="p">(</span><span class="n">slow_p</span><span class="p">)</span>
                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="DiffGrad"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.DiffGrad">[docs]</a><span class="k">class</span> <span class="nc">DiffGrad</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements diffGrad algorithm. It is modified from the pytorch implementation of Adam.</span>

<span class="sd">    It has been proposed in `diffGrad: An Optimization Method for Convolutional Neural Networks`_.</span>

<span class="sd">    Arguments:</span>
<span class="sd">        params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="sd">            parameter groups</span>
<span class="sd">        lr (float, optional): learning rate (default: 1e-3)</span>
<span class="sd">        betas (Tuple[float, float], optional): coefficients used for computing</span>
<span class="sd">            running averages of gradient and its square (default: (0.9, 0.999))</span>
<span class="sd">        eps (float, optional): term added to the denominator to improve</span>
<span class="sd">            numerical stability (default: 1e-8)</span>
<span class="sd">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>
<span class="sd">        amsgrad (boolean, optional): whether to use the AMSGrad variant of this</span>
<span class="sd">            algorithm from the paper `On the Convergence of Adam and Beyond`_</span>
<span class="sd">            (default: False)</span>

<span class="sd">    .. _diffGrad: An Optimization Method for Convolutional Neural Networks:</span>
<span class="sd">        https://arxiv.org/abs/1909.11015</span>
<span class="sd">    .. _Adam\: A Method for Stochastic Optimization:</span>
<span class="sd">        https://arxiv.org/abs/1412.6980</span>
<span class="sd">    .. _On the Convergence of Adam and Beyond:</span>
<span class="sd">        https://openreview.net/forum?id=ryQu7f-RZ</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-7</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid learning rate: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid epsilon value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 0: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 1: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
                        <span class="n">gradient_centralization</span><span class="o">=</span><span class="n">gradient_centralization</span><span class="p">)</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DiffGrad</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">__setstate__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">DiffGrad</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__setstate__</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>

<div class="viewcode-block" id="DiffGrad.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.DiffGrad.step">[docs]</a>    <span class="nd">@torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>

<span class="sd">        Arguments:</span>
<span class="sd">            closure (callable, optional): A closure that reevaluates the model</span>
<span class="sd">                and returns the loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="c1"># cast data type</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="n">p_data_fp32</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;diffGrad does not support sparse gradients, please consider SparseAdam instead&#39;</span><span class="p">)</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># State initialization</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="c1"># Exponential moving average of gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>
                    <span class="c1"># Exponential moving average of squared gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>
                    <span class="c1"># Previous gradient</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;previous_grad&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">)</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span><span class="p">,</span> <span class="n">previous_grad</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;previous_grad&#39;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>

                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">grad</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p_data_fp32</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gcc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">-</span> <span class="n">grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ndim</span><span class="p">(</span><span class="n">grad</span><span class="p">))),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Decay the first and second moment running average coefficient</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>
                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

                <span class="n">denom</span> <span class="o">=</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>

                <span class="n">bias_correction1</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>
                <span class="n">bias_correction2</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span> <span class="o">**</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span>

                <span class="c1"># compute diffgrad coefficient (dfc)</span>
                <span class="n">diff</span> <span class="o">=</span> <span class="nb">abs</span><span class="p">(</span><span class="n">previous_grad</span> <span class="o">-</span> <span class="n">grad</span><span class="p">)</span>
                <span class="n">dfc</span> <span class="o">=</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">diff</span><span class="p">))</span>
                <span class="c1"># state[&#39;previous_grad&#39;] = grad %used in paper but has the bug that previous grad is overwritten with grad and diff becomes always zero. Fixed in the next line.</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;previous_grad&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">grad</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

                <span class="c1"># update momentum with dfc</span>
                <span class="n">exp_avg1</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">*</span> <span class="n">dfc</span>
                <span class="n">G_grad</span> <span class="o">=</span> <span class="n">true_divide</span><span class="p">(</span><span class="n">exp_avg1</span><span class="p">,</span> <span class="n">denom</span><span class="p">)</span>

                <span class="c1"># if self.gradient_centralization is not None:</span>
                <span class="c1">#     if ndim(G_grad) &gt; 1:</span>
                <span class="c1">#         G_grad = G_grad - G_grad.mean(axis=list(range(1, ndim(G_grad))), keepdims=True)</span>

                <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span> <span class="o">*</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">bias_correction2</span><span class="p">)</span> <span class="o">/</span> <span class="n">bias_correction1</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">G_grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="Lamb"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Lamb">[docs]</a><span class="k">class</span> <span class="nc">Lamb</span><span class="p">(</span><span class="n">Optimizer</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Implements Lamb algorithm.</span>
<span class="sd">    It has been proposed in `Large Batch Optimization for Deep Learning: Training BERT in 76 minutes`_.</span>
<span class="sd">    Arguments:</span>
<span class="sd">        params (iterable): iterable of parameters to optimize or dicts defining</span>
<span class="sd">            parameter groups</span>
<span class="sd">        lr (float, optional): learning rate (default: 1e-3)</span>
<span class="sd">        betas (Tuple[float, float], optional): coefficients used for computing</span>
<span class="sd">            running averages of gradient and its square (default: (0.9, 0.999))</span>
<span class="sd">        eps (float, optional): term added to the denominator to improve</span>
<span class="sd">            numerical stability (default: 1e-8)</span>
<span class="sd">        weight_decay (float, optional): weight decay (L2 penalty) (default: 0)</span>
<span class="sd">        adam (bool, optional): always use trust ratio = 1, which turns this into</span>
<span class="sd">            Adam. Useful for comparison purposes.</span>
<span class="sd">    .. _Large Batch Optimization for Deep Learning: Training BERT in 76 minutes:</span>
<span class="sd">        https://arxiv.org/abs/1904.00962</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="p">(</span><span class="mf">0.9</span><span class="p">,</span> <span class="mf">0.999</span><span class="p">),</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">adam</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                 <span class="n">gradient_centralization</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">lr</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid learning rate: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">lr</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">eps</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid epsilon value: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">eps</span><span class="p">))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 0: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="mf">0.0</span> <span class="o">&lt;=</span> <span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">&lt;</span> <span class="mf">1.0</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Invalid beta parameter at index 1: </span><span class="si">{}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">betas</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="o">=</span> <span class="n">gradient_centralization</span>
        <span class="n">defaults</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="n">lr</span><span class="p">,</span> <span class="n">betas</span><span class="o">=</span><span class="n">betas</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="n">eps</span><span class="p">,</span> <span class="n">weight_decay</span><span class="o">=</span><span class="n">weight_decay</span><span class="p">,</span>
                        <span class="n">gradient_centralization</span><span class="o">=</span><span class="n">gradient_centralization</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">adam</span> <span class="o">=</span> <span class="n">adam</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Lamb</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">params</span><span class="p">,</span> <span class="n">defaults</span><span class="p">)</span>

<div class="viewcode-block" id="Lamb.step"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.Lamb.step">[docs]</a>    <span class="k">def</span> <span class="nf">step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">closure</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Performs a single optimization step.</span>
<span class="sd">        Arguments:</span>
<span class="sd">            closure (callable, optional): A closure that reevaluates the model</span>
<span class="sd">                and returns the loss.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">loss</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">if</span> <span class="n">closure</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">enable_grad</span><span class="p">():</span>
                <span class="n">loss</span> <span class="o">=</span> <span class="n">closure</span><span class="p">()</span>

        <span class="k">for</span> <span class="n">group</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">param_groups</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;params&#39;</span><span class="p">]:</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">p</span><span class="o">.</span><span class="n">requires_grad</span><span class="p">:</span>
                    <span class="k">continue</span>
                <span class="c1"># cast data type</span>
                <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">False</span>
                <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="o">.</span><span class="n">float16</span><span class="p">:</span>
                    <span class="n">half_precision</span> <span class="o">=</span> <span class="kc">True</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

                <span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">data</span>
                <span class="k">if</span> <span class="n">grad</span><span class="o">.</span><span class="n">is_sparse</span><span class="p">:</span>
                    <span class="k">raise</span> <span class="ne">RuntimeError</span><span class="p">(</span><span class="s1">&#39;Lamb does not support sparse gradients, consider SparseAdam instad.&#39;</span><span class="p">)</span>

                <span class="n">state</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">state</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>

                <span class="c1"># State initialization</span>
                <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">state</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
                    <span class="c1"># Exponential moving average of gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
                    <span class="c1"># Exponential moving average of squared gradient values</span>
                    <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>

                <span class="n">exp_avg</span><span class="p">,</span> <span class="n">exp_avg_sq</span> <span class="o">=</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg&#39;</span><span class="p">],</span> <span class="n">state</span><span class="p">[</span><span class="s1">&#39;exp_avg_sq&#39;</span><span class="p">]</span>
                <span class="n">beta1</span><span class="p">,</span> <span class="n">beta2</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;betas&#39;</span><span class="p">]</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">gradient_centralization</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;all&#39;</span><span class="p">,</span> <span class="s1">&#39;gcc&#39;</span><span class="p">]:</span>
                    <span class="k">if</span> <span class="n">ndim</span><span class="p">(</span><span class="n">grad</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">3</span><span class="p">:</span>
                        <span class="n">grad</span> <span class="o">=</span> <span class="n">grad</span> <span class="o">-</span> <span class="n">grad</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">ndim</span><span class="p">(</span><span class="n">grad</span><span class="p">))),</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;step&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>

                <span class="c1"># Decay the first and second moment running average coefficient</span>
                <span class="c1"># m_t</span>
                <span class="n">exp_avg</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta1</span><span class="p">)</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta1</span><span class="p">)</span>
                <span class="c1"># v_t</span>
                <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">mul_</span><span class="p">(</span><span class="n">beta2</span><span class="p">)</span><span class="o">.</span><span class="n">addcmul_</span><span class="p">(</span><span class="n">grad</span><span class="p">,</span> <span class="n">grad</span><span class="p">,</span> <span class="n">value</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="n">beta2</span><span class="p">)</span>

                <span class="c1"># Paper v3 does not use debiasing.</span>
                <span class="c1"># bias_correction1 = 1 - beta1 ** state[&#39;step&#39;]</span>
                <span class="c1"># bias_correction2 = 1 - beta2 ** state[&#39;step&#39;]</span>
                <span class="c1"># Apply bias to lr to avoid broadcast.</span>
                <span class="n">step_size</span> <span class="o">=</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;lr&#39;</span><span class="p">]</span>  <span class="c1"># * math.sqrt(bias_correction2) / bias_correction1</span>

                <span class="n">weight_norm</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>

                <span class="n">adam_step</span> <span class="o">=</span> <span class="n">exp_avg</span> <span class="o">/</span> <span class="n">exp_avg_sq</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;eps&#39;</span><span class="p">])</span>
                <span class="k">if</span> <span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">]</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">adam_step</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="n">group</span><span class="p">[</span><span class="s1">&#39;weight_decay&#39;</span><span class="p">])</span>

                <span class="n">adam_norm</span> <span class="o">=</span> <span class="n">adam_step</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">sqrt</span><span class="p">()</span>
                <span class="k">if</span> <span class="n">weight_norm</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">or</span> <span class="n">adam_norm</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="mi">1</span>
                <span class="k">else</span><span class="p">:</span>
                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="n">weight_norm</span> <span class="o">/</span> <span class="n">adam_norm</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;weight_norm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">weight_norm</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;adam_norm&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">adam_norm</span>
                <span class="n">state</span><span class="p">[</span><span class="s1">&#39;trust_ratio&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">trust_ratio</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">adam</span><span class="p">:</span>
                    <span class="n">trust_ratio</span> <span class="o">=</span> <span class="mi">1</span>

                <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">add_</span><span class="p">(</span><span class="n">adam_step</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=-</span><span class="n">step_size</span> <span class="o">*</span> <span class="n">trust_ratio</span><span class="p">)</span>

                <span class="k">if</span> <span class="n">half_precision</span><span class="p">:</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">data</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
                    <span class="n">p</span><span class="o">.</span><span class="n">grad</span> <span class="o">=</span> <span class="n">p</span><span class="o">.</span><span class="n">grad</span><span class="o">.</span><span class="n">half</span><span class="p">()</span>
        <span class="k">return</span> <span class="n">loss</span></div></div>


<div class="viewcode-block" id="get_optimizer"><a class="viewcode-back" href="../../../trident.optims.html#trident.optims.pytorch_optimizers.get_optimizer">[docs]</a><span class="k">def</span> <span class="nf">get_optimizer</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">optimizer_name</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">None</span>
    <span class="n">optimizer_modules</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;trident.optims.pytorch_optimizers&#39;</span><span class="p">,</span> <span class="s1">&#39;torch.optim&#39;</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">optimizer_name</span> <span class="ow">in</span> <span class="n">__all__</span><span class="p">:</span>
        <span class="n">optimizer_class</span> <span class="o">=</span> <span class="n">get_class</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">,</span> <span class="n">optimizer_modules</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizer_class</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">optimizer_class</span> <span class="o">=</span> <span class="n">get_class</span><span class="p">(</span><span class="n">snake2camel</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">),</span> <span class="n">optimizer_modules</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">optimizer_class</span>
        <span class="k">except</span> <span class="ne">Exception</span><span class="p">:</span>
            <span class="n">optimizer_class</span> <span class="o">=</span> <span class="n">get_class</span><span class="p">(</span><span class="n">optimizer_name</span><span class="p">,</span> <span class="n">optimizer_modules</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">optimizer_class</span></div>
</pre></div>

        </div>
        <div class="side-doc-outline">
            <div class="side-doc-outline--content"> 
            </div>
        </div>

      <div class="clearer"></div>
    </div><div class="pagenation">
  </div>
        <footer class="mdl-mini-footer">
    <div class="mdl-mini-footer__left-section">
      <div class="mdl-logo">trident</div>
      <div>
        
        
      </div>
    </div>

    <div class="mdl-mini-footer__right-section">
        <div>&copy; Copyright 2022, AllanYiin.</div>
      <div>Generated by <a href="http://sphinx.pocoo.org/">Sphinx</a> 5.0.2 using <a href="https://github.com/myyasuda/sphinx_materialdesign_theme">sphinx_materialdesign_theme</a>.</div>
    </div>
</footer>
        </main>
    </div>
  </body>
</html>